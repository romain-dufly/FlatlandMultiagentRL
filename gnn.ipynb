{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN approach\n",
    "\n",
    "## Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import math\n",
    "from datetime import datetime\n",
    "from tabulate import tabulate\n",
    "from ast import literal_eval\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from enum import IntEnum\n",
    "import time\n",
    "import random\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from collections import namedtuple, deque\n",
    "from typing import List, Optional, Tuple, Union, Callable, Dict, Sequence, NamedTuple\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "from importlib_resources import path\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as gnn\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch.nn.modules.container import ParameterList\n",
    "import torch.optim as optim\n",
    "\n",
    "import networkx as nx\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "# Base flatland environment\n",
    "from flatland.envs.line_generators import SparseLineGen\n",
    "from flatland.envs.line_generators import sparse_line_generator\n",
    "from flatland.envs.rail_generators import sparse_rail_generator, rail_from_file\n",
    "from flatland.envs.malfunction_generators import (\n",
    "    MalfunctionParameters,\n",
    "    ParamMalfunctionGen,\n",
    ")\n",
    "\n",
    "from flatland.envs.observations import GlobalObsForRailEnv\n",
    "from flatland.envs.observations import TreeObsForRailEnv\n",
    "from flatland.envs.distance_map import DistanceMap\n",
    "import flatland.envs.rail_env_shortest_paths as sp\n",
    "from flatland.envs.rail_env import RailEnv, RailEnvActions\n",
    "\n",
    "from flatland.envs.step_utils.states import TrainState\n",
    "from flatland.envs.predictions import ShortestPathPredictorForRailEnv\n",
    "from flatland.envs.agent_utils import EnvAgent\n",
    "from flatland.envs.persistence import RailEnvPersister\n",
    "from flatland.core.env import Environment\n",
    "from flatland.core.env_observation_builder import ObservationBuilder\n",
    "\n",
    "from flatland.core.grid.grid4_utils import get_new_position, direction_to_point, MOVEMENT_ARRAY\n",
    "from flatland.core.grid.grid4 import Grid4TransitionsEnum\n",
    "from flatland.core.grid.rail_env_grid import RailEnvTransitions\n",
    "\n",
    "from flatland.core.env_prediction_builder import PredictionBuilder\n",
    "from flatland.utils.ordered_set import OrderedSet\n",
    "\n",
    "import sys\n",
    "sys.path.append('src/')\n",
    "\n",
    "from src import test_utils, training, rewards\n",
    "from src.observation_utils import normalize_observation\n",
    "from src.models import *\n",
    "from src.deep_model_policy import DeepPolicy, PolicyParameters\n",
    "from training import train_agent\n",
    "\n",
    "# Visualization\n",
    "from flatland.utils.rendertools import RenderTool, AgentRenderVariant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utils\n",
    "\n",
    "def get_linear(input_size, output_size, hidden_sizes, nonlinearity=\"tanh\"):\n",
    "    '''\n",
    "    Returns a PyTorch Sequential object containing FC layers with\n",
    "    non-linear activation functions, by following the given input/hidden/output sizes\n",
    "    '''\n",
    "    fc = []\n",
    "    nl = nn.ReLU(inplace=True) if nonlinearity == \"relu\" else nn.Tanh()\n",
    "    sizes = [input_size] + hidden_sizes + [output_size]\n",
    "    for i in range(1, len(sizes)):\n",
    "        fc.extend([nn.Linear(sizes[i - 1], sizes[i]), nl])\n",
    "    return nn.Sequential(*fc)\n",
    "\n",
    "\n",
    "def conv_bn_act(input_channels, output_channels, kernel_size=3,\n",
    "                stride=1, padding=0, nonlinearity=\"relu\"):\n",
    "    '''\n",
    "    Returns a block composed by a convolutional layer and a batch norm one,\n",
    "    followed by a non-linearity (e.g. ReLU or Tanh)\n",
    "    '''\n",
    "    return [\n",
    "        nn.Conv2d(\n",
    "            input_channels, output_channels,\n",
    "            kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        ),\n",
    "        nn.BatchNorm2d(output_channels),\n",
    "        nn.ReLU(inplace=True) if nonlinearity == \"relu\" else nn.Tanh()\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_conv(input_channels, output_channels, hidden_channels,\n",
    "             conv_params, pool_params, nonlinearity=\"relu\"):\n",
    "    '''\n",
    "    Returns a PyTorch Sequential object containing `conv_bn_act` blocks\n",
    "    interleaved with max pooling layers, following the given \n",
    "    input/hidden/output number of channels\n",
    "\n",
    "    Note: the `conv_params` and `pool_params` arguments should be tuples\n",
    "    containing (kernel_size, stride, padding) to use with the respective layer\n",
    "    '''\n",
    "    assert len(hidden_channels) >= 1\n",
    "\n",
    "    convs = []\n",
    "    channels = [input_channels] + hidden_channels + [output_channels]\n",
    "    conv_kernel_size, conv_stride, conv_padding = conv_params\n",
    "    pool_kernel_size, pool_stride, pool_padding = pool_params\n",
    "    for i in range(1, len(channels)):\n",
    "        block = conv_bn_act(\n",
    "            channels[i - 1], channels[i],\n",
    "            kernel_size=conv_kernel_size, stride=conv_stride,\n",
    "            padding=conv_padding, nonlinearity=nonlinearity\n",
    "        )\n",
    "        # Add pooling once every two layers\n",
    "        if i % 2 == 0:\n",
    "            block += [\n",
    "                nn.MaxPool2d(\n",
    "                    kernel_size=pool_kernel_size,\n",
    "                    stride=pool_stride,\n",
    "                    padding=pool_padding\n",
    "                )\n",
    "            ]\n",
    "        convs.extend(block)\n",
    "\n",
    "    return nn.Sequential(*convs)\n",
    "\n",
    "\n",
    "def conv_block_output_size(modules, input_width, input_height):\n",
    "    '''\n",
    "    Given a sequence of PyTorch modules (e.g. Python list, PyTorch Sequential/ModuleList)\n",
    "    containing convolution related layers (currently only Conv2d and MaxPool2d are supported),\n",
    "    returns the output size of the input tensor, after it passes through all the given layers\n",
    "    '''\n",
    "    output_width, output_height = input_width, input_height\n",
    "    for module in modules:\n",
    "        if type(module) in (nn.Conv2d, nn.MaxPool2d):\n",
    "            if type(module) == nn.Conv2d:\n",
    "                kernel_size, stride, padding, dilation = get_conv2d_params(\n",
    "                    module\n",
    "                )\n",
    "            elif type(module) == nn.MaxPool2d:\n",
    "                kernel_size, stride, padding, dilation = get_maxpool2d_params(\n",
    "                    module\n",
    "                )\n",
    "            kernel_size_h, kernel_size_w = kernel_size\n",
    "            stride_h, stride_w = stride\n",
    "            padding_h, padding_w = padding\n",
    "            dilation_h, dilation_w = dilation\n",
    "            output_width = np.floor((\n",
    "                output_width + 2 * padding_w -\n",
    "                dilation_w * (kernel_size_w - 1) - 1\n",
    "            ) / stride_w + 1)\n",
    "            output_height = np.floor((\n",
    "                output_height + 2 * padding_h -\n",
    "                dilation_h * (kernel_size_h - 1) - 1\n",
    "            ) / stride_h + 1)\n",
    "    return int(output_width), int(output_height)\n",
    "\n",
    "\n",
    "def get_conv2d_params(conv):\n",
    "    '''\n",
    "    Return kernel size, stride, padding and dilation for a Conv2d layer\n",
    "    '''\n",
    "    return (\n",
    "        conv.kernel_size,\n",
    "        conv.stride,\n",
    "        conv.padding,\n",
    "        conv.dilation\n",
    "    )\n",
    "\n",
    "\n",
    "def get_maxpool2d_params(pool):\n",
    "    '''\n",
    "    Return kernel size, stride, padding and dilation for a MaxPool2d layer\n",
    "    '''\n",
    "    return (\n",
    "        (pool.kernel_size, pool.kernel_size),\n",
    "        (pool.stride, pool.stride),\n",
    "        (pool.padding, pool.padding),\n",
    "        (pool.dilation, pool.dilation)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DQN\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, params, device=\"cpu\"):\n",
    "        super(DQN, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.params = params\n",
    "        self.device = device\n",
    "        self.fc = get_linear(\n",
    "            state_size, action_size, self.params.hidden_sizes,\n",
    "            nonlinearity=self.params.nonlinearity\n",
    "        )\n",
    "\n",
    "    def forward(self, states, mask=None):\n",
    "        states = torch.flatten(states, start_dim=1)\n",
    "        assert len(states.shape) == 2 and states.shape[1] == self.state_size\n",
    "        if mask is None:\n",
    "            mask = torch.ones(\n",
    "                (states.shape[0],), dtype=torch.bool,\n",
    "                device=self.device\n",
    "            )\n",
    "        mask = torch.flatten(mask)\n",
    "        mask_q = torch.zeros(\n",
    "            (states.shape[0], self.action_size), device=self.device)\n",
    "        q_values = self.fc(states[mask, :])\n",
    "        mask_q[mask, :] = q_values\n",
    "        return mask_q\n",
    "\n",
    "\n",
    "## Dueling DQN \n",
    "\n",
    "class DuelingDQN(DQN):\n",
    "   \n",
    "\n",
    "    def __init__(self, state_size, action_size, params, device=\"cpu\"):\n",
    "        super(DuelingDQN, self).__init__(\n",
    "            state_size, action_size, params, device=device\n",
    "        )\n",
    "        self.aggregation = self.params.dueling.aggregation.get_true_key()\n",
    "        self.fc_val = get_linear(\n",
    "            state_size, 1, self.params.hidden_sizes,\n",
    "            nonlinearity=self.params.nonlinearity\n",
    "        )\n",
    "\n",
    "    def forward(self, states, mask=None):\n",
    "        states = torch.flatten(states, start_dim=1)\n",
    "        assert len(states.shape) == 2 and states.shape[1] == self.state_size\n",
    "        if mask is None:\n",
    "            mask = torch.ones(\n",
    "                (states.shape[0],), dtype=torch.bool,\n",
    "                device=self.device\n",
    "            )\n",
    "        mask = torch.flatten(mask)\n",
    "        mask_val = torch.zeros(\n",
    "            (states.shape[0], self.action_size),\n",
    "            device=self.device\n",
    "        )\n",
    "        val = self.fc(states[mask, :])\n",
    "        mask_val[mask, :] = val\n",
    "        mask_adv = super().forward(states, mask=mask)\n",
    "        mask_agg = torch.zeros((states.shape[0], 1), device=self.device)\n",
    "        agg = (\n",
    "            mask_adv[mask, :].mean(dim=1, keepdim=True) if self.aggregation == \"mean\"\n",
    "            else mask_adv[mask, :].max(dim=1, keepdim=True)\n",
    "        )\n",
    "        mask_agg[mask, :] = agg\n",
    "        return mask_val + mask_adv - mask_agg\n",
    "\n",
    "\n",
    "## Entire graph GNN\n",
    "\n",
    "class EntireGNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, depth, params, device=\"cpu\"):\n",
    "        super(EntireGNN, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.depth = depth\n",
    "        self.params = params\n",
    "        self.device = device\n",
    "\n",
    "        self.embedding_size = self.params.embedding_size\n",
    "        self.hidden_size = self.params.hidden_size\n",
    "        self.pos_size = self.params.pos_size\n",
    "        self.dropout = self.params.dropout\n",
    "        self.nonlinearity = self.params.nonlinearity.get_true_key()\n",
    "\n",
    "        self.nl = (\n",
    "            nn.ReLU(inplace=True) if self.nonlinearity == \"relu\" else nn.Tanh()\n",
    "        )\n",
    "        self.gnn_conv = nn.ModuleList()\n",
    "        sizes = (\n",
    "            [state_size] +\n",
    "            [self.hidden_size] * (self.depth - 2) +\n",
    "            [self.embedding_size]\n",
    "        )\n",
    "        for i in range(1, len(sizes)):\n",
    "            self.gnn_conv.append(\n",
    "                gnn.GCNConv(sizes[i - 1], sizes[i])\n",
    "            )\n",
    "\n",
    "    def forward(self, states, **kwargs):\n",
    "        graphs = states.to_data_list()\n",
    "        embs = torch.zeros(\n",
    "            size=(\n",
    "                len(graphs),\n",
    "                self.pos_size * self.embedding_size\n",
    "            ), dtype=torch.float,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        # For each graph in the batch\n",
    "        for i, graph in enumerate(graphs):\n",
    "            x, edge_index, edge_weight, pos = (\n",
    "                graph.x, graph.edge_index, graph.edge_weight, graph.pos\n",
    "            )\n",
    "\n",
    "            # Perform a number of graph convolutions specified by\n",
    "            # the given depth\n",
    "            for d in range(self.depth):\n",
    "                x = self.gnn_conv[d](x, edge_index, edge_weight=edge_weight)\n",
    "                emb = x\n",
    "                x = self.nl(x)\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "            # Extract useful embeddings\n",
    "            tmp_embs = torch.full(\n",
    "                (self.pos_size, self.embedding_size),\n",
    "                [-self.depth] * self.embedding_size,\n",
    "                dtype=torch.float,\n",
    "                device=self.device\n",
    "            )\n",
    "            for j, p in enumerate(pos):\n",
    "                if p != -1:\n",
    "                    tmp_embs[j] = emb[p.item()]\n",
    "            embs[i] = torch.flatten(tmp_embs)\n",
    "\n",
    "        return embs\n",
    "\n",
    "\n",
    "## Multi agent GNN\n",
    "\n",
    "class MultiGNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_width, input_height, input_channels, params, device=\"cpu\"):\n",
    "        super(MultiGNN, self).__init__()\n",
    "        self.input_width = input_width\n",
    "        self.input_height = input_height\n",
    "        self.input_channels = input_channels\n",
    "        self.params = params\n",
    "        self.device = device\n",
    "\n",
    "        self.output_channels = self.params.cnn_encoder.output_channels\n",
    "        self.hidden_channels = self.params.cnn_encoder.hidden_channels\n",
    "        self.mlp_output_size = self.params.mlp_compression.output_size\n",
    "        self.mlp_hidden_sizes = self.params.mlp_compression.hidden_sizes\n",
    "        self.gnn_hidden_sizes = self.params.gnn_communication.hidden_sizes\n",
    "        self.embedding_size = self.params.gnn_communication.embedding_size\n",
    "        self.dropout = self.params.gnn_communication.dropout\n",
    "        self.nonlinearity = self.params.nonlinearity.get_true_key()\n",
    "        self.nl = (\n",
    "            nn.ReLU(inplace=True) if self.nonlinearity == \"relu\" else nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # Encoder\n",
    "        conv_settings, pool_settings = self.params.cnn_encoder.conv, self.params.cnn_encoder.pool\n",
    "        conv_params = conv_settings.kernel_size, conv_settings.stride, conv_settings.padding\n",
    "        pool_params = pool_settings.kernel_size, pool_settings.stride, pool_settings.padding\n",
    "        self.convs = get_conv(\n",
    "            self.input_channels, self.output_channels, self.hidden_channels,\n",
    "            conv_params, pool_params, nonlinearity=self.nonlinearity\n",
    "        )\n",
    "\n",
    "        # MLP\n",
    "        output_width, output_height = conv_block_output_size(\n",
    "            self.convs, self.input_width, self.input_height\n",
    "        )\n",
    "        assert output_width > 0 and output_height > 0\n",
    "        self.mlp = get_linear(\n",
    "            output_width * output_height * self.output_channels,\n",
    "            self.mlp_output_size, self.mlp_hidden_sizes, nonlinearity=self.nonlinearity\n",
    "        )\n",
    "\n",
    "        # GNN\n",
    "        self.gnn_conv = nn.ModuleList()\n",
    "        sizes = (\n",
    "            [self.mlp_output_size] +\n",
    "            self.gnn_hidden_sizes +\n",
    "            [self.embedding_size]\n",
    "        )\n",
    "        for i in range(1, len(sizes)):\n",
    "            self.gnn_conv.append(\n",
    "                gnn.GATConv(\n",
    "                    sizes[i - 1], sizes[i], add_self_loops=False,\n",
    "                    heads=2, concat=False\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, states, **kwargs):\n",
    "        \n",
    "        encoded = self.convs(states.states) # Encode the FOV observation of each agent with the convolutional encoder\n",
    "        flattened = torch.flatten(encoded, start_dim=1) # Use an MLP from the encoded values to have a consistent number of features\n",
    "        features = self.mlp(flattened)\n",
    "        embeddings = None\n",
    "        for conv in self.gnn_conv:\n",
    "            features = conv(features, states.edge_index)\n",
    "            embeddings = features\n",
    "            features = self.nl(features)\n",
    "            features = F.dropout(\n",
    "                features, p=self.dropout, training=self.training\n",
    "            )\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prediction = namedtuple('Prediction', ['lenght', 'path', 'edges', 'positions'])\n",
    "\n",
    "\n",
    "def _empty_prediction():\n",
    "    '''\n",
    "    Return an empty Prediction namedtuple\n",
    "    '''\n",
    "    return Prediction(\n",
    "        lenght=np.inf, path=[], edges=[], positions=[]\n",
    "    )\n",
    "\n",
    "\n",
    "class NullPredictor(PredictionBuilder):\n",
    "\n",
    "    def __init__(self, max_depth=None):\n",
    "        super().__init__(max_depth)\n",
    "\n",
    "    def set_env(self, env):\n",
    "        super().set_env(env)\n",
    "\n",
    "    def get_many(self):\n",
    "        '''\n",
    "        Build the prediction for every agent\n",
    "        '''\n",
    "        return {agent.handle: None for agent in self.env.agents}\n",
    "\n",
    "    def get(self, handle):\n",
    "        '''\n",
    "        Build the prediction for the given agent\n",
    "        '''\n",
    "        return None\n",
    "\n",
    "\n",
    "class ShortestDeviationPathPredictor(PredictionBuilder):\n",
    "\n",
    "    def __init__(self, max_depth, max_deviations):\n",
    "        super().__init__(max_depth)\n",
    "        self.max_deviations = max_deviations\n",
    "\n",
    "    def set_env(self, env):\n",
    "        super().set_env(env)\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "        Initialize shortest paths for each agent\n",
    "        '''\n",
    "        self._shortest_paths = dict()\n",
    "        for agent in self.env.agents:\n",
    "            self._shortest_paths[agent.handle] = self.env.railway_encoding.shortest_paths(\n",
    "                agent.handle\n",
    "            )\n",
    "\n",
    "    def get_shortest_path(self, handle):\n",
    "        '''\n",
    "        Keep a list of shortest paths for the given agent.\n",
    "        At each time step, update the already compute paths and delete the ones\n",
    "        which cannot be followed anymore.\n",
    "        The returned shortest paths have the agent's position as the first element.\n",
    "        '''\n",
    "        position = self.env.railway_encoding.get_agent_cell(handle)\n",
    "        node, _ = self.env.railway_encoding.next_node(position)\n",
    "        chosen_path = None\n",
    "        paths_to_delete = []\n",
    "        for i, shortest_path in enumerate(self._shortest_paths[handle]):\n",
    "            lenght, path = shortest_path\n",
    "            # Delete divergent path\n",
    "            if node != path[0] and node != path[1]:\n",
    "                paths_to_delete = [i] + paths_to_delete\n",
    "                continue\n",
    "\n",
    "            # Update agent position\n",
    "            if path[0] != position:\n",
    "                lenght -= 1\n",
    "            path[0] = position\n",
    "\n",
    "            # If the agent is on a packed graph node, drop it\n",
    "            if path[0] == path[1]:\n",
    "                path = path[1:]\n",
    "\n",
    "            # Agent arrived to target\n",
    "            if lenght == 0:\n",
    "                chosen_path = lenght, path\n",
    "                break\n",
    "\n",
    "            # Select this path if no other path has been previously selected\n",
    "            if chosen_path is None:\n",
    "                chosen_path = lenght, path\n",
    "\n",
    "            # Update shortest path\n",
    "            self._shortest_paths[handle][i] = lenght, path\n",
    "\n",
    "        # Delete divergent paths\n",
    "        for i in paths_to_delete:\n",
    "            del self._shortest_paths[handle][i]\n",
    "\n",
    "        # Compute shortest paths, if no path is already available\n",
    "        if chosen_path is None:\n",
    "            self._shortest_paths[handle] = self.env.railway_encoding.shortest_paths(\n",
    "                handle\n",
    "            )\n",
    "            if not self._shortest_paths[handle]:\n",
    "                if position == node:\n",
    "                    node = self.env.railway_encoding.get_successors(node)[0]\n",
    "                return np.inf, [position, node]\n",
    "\n",
    "            chosen_path = self._shortest_paths[handle][0]\n",
    "\n",
    "        return chosen_path\n",
    "\n",
    "    def get_deviation_paths(self, handle, lenght, path):\n",
    "        '''\n",
    "        Return one deviation path for at most `max_deviations` nodes in the given path\n",
    "        and limit the computed path lenghts by `max_depth`\n",
    "        '''\n",
    "        start = 0\n",
    "        depth = min(self.max_deviations, len(path) - 1)\n",
    "        deviation_paths = []\n",
    "        padding = self.max_deviations\n",
    "        if lenght < np.inf:\n",
    "            padding -= len(path)\n",
    "            source, _ = self.env.railway_encoding.next_node(path[0])\n",
    "            if source != path[0]:\n",
    "                start = 1\n",
    "                deviation_paths.append(_empty_prediction())\n",
    "            for i in range(start, depth):\n",
    "                paths = self.env.railway_encoding.deviation_paths(\n",
    "                    handle, path[i], path[i + 1]\n",
    "                )\n",
    "                deviation_path = []\n",
    "                deviation_lenght = 0\n",
    "                if len(paths) > 0:\n",
    "                    deviation_path = paths[0][1]\n",
    "                    deviation_lenght = paths[0][0]\n",
    "                    edges = self.env.railway_encoding.edges_from_path(\n",
    "                        deviation_path[:self.max_depth]\n",
    "                    )\n",
    "                    pos = self.env.railway_encoding.positions_from_path(\n",
    "                        deviation_path[:self.max_depth]\n",
    "                    )\n",
    "                    deviation_paths.append(\n",
    "                        Prediction(\n",
    "                            lenght=deviation_lenght,\n",
    "                            path=deviation_path[:self.max_depth],\n",
    "                            edges=edges,\n",
    "                            positions=pos\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    deviation_paths.append(_empty_prediction())\n",
    "\n",
    "        deviation_paths.extend(\n",
    "            [_empty_prediction()] * (padding)\n",
    "        )\n",
    "        return deviation_paths\n",
    "\n",
    "    def get_many(self):\n",
    "        '''\n",
    "        Build the prediction for every agent\n",
    "        '''\n",
    "        prediction_dict = {}\n",
    "        for agent in self.env.agents:\n",
    "            prediction_dict[agent.handle] = None\n",
    "            if agent.malfunction_data[\"malfunction\"] == 0:\n",
    "                prediction_dict[agent.handle] = self.get(agent.handle)\n",
    "        return prediction_dict\n",
    "\n",
    "    def get(self, handle):\n",
    "        '''\n",
    "        Build the prediction for the given agent\n",
    "        '''\n",
    "        agent = self.env.agents[handle]\n",
    "        if agent.status == TrainState.DONE:\n",
    "            return None\n",
    "\n",
    "        # Build predictions\n",
    "        lenght, path = self.get_shortest_path(handle)\n",
    "        edges = self.env.railway_encoding.edges_from_path(\n",
    "            path[:self.max_depth]\n",
    "        )\n",
    "        pos = self.env.railway_encoding.positions_from_path(\n",
    "            path[:self.max_depth]\n",
    "        )\n",
    "        shortest_path_prediction = Prediction(\n",
    "            lenght=lenght, path=path[:self.max_depth], edges=edges, positions=pos\n",
    "        )\n",
    "        deviation_paths_prediction = self.get_deviation_paths(\n",
    "            handle, lenght, path\n",
    "        )\n",
    "\n",
    "        # Update GUI\n",
    "        visited = OrderedSet()\n",
    "        visited.update(shortest_path_prediction.positions)\n",
    "        self.env.dev_pred_dict[handle] = visited\n",
    "\n",
    "        return (shortest_path_prediction, deviation_paths_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer\n",
    "\n",
    "def get_index(arr, elem):\n",
    "    '''\n",
    "    Return the index of the first occurrence of `elem` in `arr`,\n",
    "    if `elem` is present in `arr`, otherwise return None\n",
    "    '''\n",
    "    return arr.index(elem) if elem in arr else None\n",
    "\n",
    "\n",
    "def is_close(a, b, rtol=1e-03):\n",
    "    '''\n",
    "    Return if a is relatively close to the value of b\n",
    "    '''\n",
    "    return abs(a - b) <= rtol\n",
    "\n",
    "\n",
    "def reciprocal_sum(a, b):\n",
    "    '''\n",
    "    Compute the reciprocal sum of the given inputs\n",
    "    '''\n",
    "    return (1 / a) + (1 / b)\n",
    "\n",
    "\n",
    "def min_max_scaling(values, lower, upper, under, over, known_min=None, known_max=None):\n",
    "    '''\n",
    "    Perform min-max scaling over the given array\n",
    "    (`under` is substituted for -np.inf and `over` for np.inf)\n",
    "    '''\n",
    "    finite_values = values[np.isfinite(values)]\n",
    "    min_value, max_value = known_min, known_max\n",
    "    try:\n",
    "        if min_value is None:\n",
    "            min_value = finite_values.min()\n",
    "        if max_value is None:\n",
    "            max_value = finite_values.max()\n",
    "        if min_value != max_value:\n",
    "            values = lower + (\n",
    "                ((values - min_value) * (upper - lower)) /\n",
    "                (max_value - min_value)\n",
    "            )\n",
    "        elif min_value != 0:\n",
    "            values = values / min_value\n",
    "        else:\n",
    "            values[:] = under\n",
    "    except:\n",
    "        pass\n",
    "    values[values == -np.inf] = under\n",
    "    values[values == np.inf] = over\n",
    "    return values\n",
    "\n",
    "\n",
    "def extract_fov(matrix, center_index, window_size, pad=0):\n",
    "    '''\n",
    "    Extract a patch of size window_size from the given matrix centered around\n",
    "    the specified position and pad external values with the given fill value\n",
    "    '''\n",
    "    # Window is entirely contained in the given matrix\n",
    "    m, n = matrix.shape\n",
    "    offset = window_size // 2\n",
    "    yl, yu = center_index[0] - offset, center_index[0] + offset\n",
    "    xl, xu = center_index[1] - offset, center_index[1] + offset\n",
    "    if xl >= 0 and xu < n and yl >= 0 and yu < m:\n",
    "        return np.array(matrix[yl: yu + 1, xl:xu + 1], dtype=matrix.dtype)\n",
    "\n",
    "    # Window has to be padded\n",
    "    window = np.full((window_size, window_size), pad, dtype=matrix.dtype)\n",
    "    c_yl, c_yu = np.clip(yl, 0, m), np.clip(yu, 0, m)\n",
    "    c_xl, c_xu = np.clip(xl, 0, n), np.clip(xu, 0, n)\n",
    "    sub = matrix[c_yl: c_yu + 1, c_xl:c_xu + 1]\n",
    "    w_yl = 0 if yl >= 0 else abs(yl)\n",
    "    w_yu = window_size if yu < m else window_size - (yu - m) - 1\n",
    "    w_xl = 0 if xl >= 0 else abs(xl)\n",
    "    w_xu = window_size if xu < n else window_size - (xu - n) - 1\n",
    "    window[w_yl:w_yu, w_xl:w_xu] = sub\n",
    "    return window\n",
    "\n",
    "\n",
    "def fix_random(seed):\n",
    "    '''\n",
    "    Fix all the possible sources of randomness\n",
    "    '''\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def set_num_threads(num_threads):\n",
    "    '''\n",
    "    Set the maximum number of threads PyTorch can use\n",
    "    '''\n",
    "    torch.set_num_threads(num_threads)\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = str(num_threads)\n",
    "    os.environ[\"MKL_NUM_THREADS\"] = str(num_threads)\n",
    "\n",
    "\n",
    "class Timer():\n",
    "    '''\n",
    "    Utility to measure times\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.total_time = 0.0\n",
    "        self.start_time = 0.0\n",
    "        self.end_time = 0.0\n",
    "\n",
    "    def start(self):\n",
    "        self.start_time = default_timer()\n",
    "\n",
    "    def end(self):\n",
    "        self.total_time += default_timer() - self.start_time\n",
    "\n",
    "    def get(self):\n",
    "        return self.total_time\n",
    "\n",
    "    def get_current(self):\n",
    "        return default_timer() - self.start_time\n",
    "\n",
    "    def reset(self):\n",
    "        self.__init__()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.get()\n",
    "\n",
    "\n",
    "class Struct:\n",
    "    '''\n",
    "    Struct class, s.t. a nested dictionary is transformed\n",
    "    into a nested object\n",
    "    '''\n",
    "\n",
    "    def __init__(self, **entries):\n",
    "        for k, v in entries.items():\n",
    "            if isinstance(v, dict):\n",
    "                self.__dict__.update({k: Struct(**v)})\n",
    "            else:\n",
    "                self.__dict__.update({k: v})\n",
    "\n",
    "    def get_true_key(self):\n",
    "        '''\n",
    "        Return the only key in the Struct s.t. its value is True\n",
    "        '''\n",
    "        true_types = [k for k, v in self.__dict__.items() if v == True]\n",
    "        assert len(true_types) == 1\n",
    "        return true_types[0]\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.__dict__)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deadlock detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeadlocksDetector:\n",
    "    '''\n",
    "    Class containing code to track deadlocks during an episode,\n",
    "    based on https://github.com/AlessandroLombardi/FlatlandChallenge\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.deadlocks = dict()\n",
    "        self.deadlock_turns = dict()\n",
    "\n",
    "    def reset(self, num_agents):\n",
    "        '''\n",
    "        Reset deadlock counters\n",
    "        '''\n",
    "        self.deadlocks = {a: False for a in range(num_agents)}\n",
    "        self.deadlock_turns = {a: None for a in range(num_agents)}\n",
    "\n",
    "    def step(self, env):\n",
    "        '''\n",
    "        Check for new deadlocks, updates counter and returns it\n",
    "        '''\n",
    "        agents = []\n",
    "        for a in range(env.get_num_agents()):\n",
    "            if env.agents[a].status == TrainState.ACTIVE:\n",
    "                agents.append(a)\n",
    "                if not self.deadlocks[a]:\n",
    "                    self.deadlocks[a] = self._check_deadlocks(\n",
    "                        agents, self.deadlocks, env\n",
    "                    )\n",
    "                if not self.deadlocks[a]:\n",
    "                    del agents[-1]\n",
    "                elif self.deadlock_turns[a] is None:\n",
    "                    self.deadlock_turns[a] = env._elapsed_steps - 1\n",
    "            else:\n",
    "                self.deadlocks[a] = False\n",
    "\n",
    "        return self.deadlocks, self.deadlock_turns\n",
    "\n",
    "    def _check_feasible_transitions(self, pos, env):\n",
    "        '''\n",
    "        Function used to collect chains of blocked agents\n",
    "        '''\n",
    "        transitions = env.rail.get_transitions(*pos)\n",
    "        n_transitions = 0\n",
    "        occupied = 0\n",
    "        agent_in_path = None\n",
    "        for direction, values in enumerate(MOVEMENT_ARRAY):\n",
    "            if transitions[direction] == 1:\n",
    "                n_transitions += 1\n",
    "                new_position = get_new_position(pos, direction)\n",
    "                for agent in range(env.get_num_agents()):\n",
    "                    if env.agents[agent].position == new_position:\n",
    "                        occupied += 1\n",
    "                        agent_in_path = agent\n",
    "        if n_transitions > occupied:\n",
    "            return None\n",
    "        return agent_in_path\n",
    "\n",
    "    def _check_next_pos(self, agent, env):\n",
    "        '''\n",
    "        Check the next pos and the possible transitions of an agent to find deadlocks\n",
    "        '''\n",
    "        pos = (*env.agents[agent].position, env.agents[agent].direction)\n",
    "        return self._check_feasible_transitions(pos, env)\n",
    "\n",
    "    def _check_deadlocks(self, agents, deadlocks, env):\n",
    "        '''\n",
    "        Recursive procedure to find out whether agents in `agents` are in a deadlock\n",
    "        '''\n",
    "        other_agent = self._check_next_pos(agents[-1], env)\n",
    "\n",
    "        # No agents in front\n",
    "        if other_agent is None:\n",
    "            return False\n",
    "\n",
    "        # Deadlocked agent in front or loop chain found\n",
    "        if deadlocks[other_agent] or other_agent in agents:\n",
    "            return True\n",
    "\n",
    "        # Investigate further\n",
    "        agents.append(other_agent)\n",
    "        deadlocks[other_agent] = self._check_deadlocks(agents, deadlocks, env)\n",
    "\n",
    "        # If the agent `other_agent` is in deadlock\n",
    "        # also the last one in `agents` is\n",
    "        if deadlocks[other_agent]:\n",
    "            return True\n",
    "\n",
    "        # Back to previous recursive call\n",
    "        del agents[-1]\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RailEnvWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EnvWrapper, encoding of the map in a graph and Observators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_actions():\n",
    "    '''\n",
    "    Return the number of possible RailEnvActions\n",
    "    '''\n",
    "    return len([\n",
    "        action_type for _, action_type in RailEnvActions.__members__.items()\n",
    "    ])\n",
    "    \n",
    "class RailEnvWrapper(RailEnv):\n",
    "    '''\n",
    "    Railway environment wrapper, to handle custom logic\n",
    "    '''\n",
    "\n",
    "    def __init__(self, params, *args, normalize=True, **kwargs):\n",
    "        super(RailEnvWrapper, self).__init__(*args, **kwargs)\n",
    "        self.params = params\n",
    "        self.railway_encoding = None\n",
    "        self.normalize = normalize\n",
    "        self.state_size = self._get_state_size()\n",
    "        self.deadlocks_detector = DeadlocksDetector()\n",
    "        self.partial_rewards = dict()\n",
    "        self.arrived_turns = dict()\n",
    "        self.stop_actions = dict()\n",
    "        self.current_info = dict()\n",
    "        self.num_actions = get_num_actions()\n",
    "\n",
    "    def _get_state_size(self):\n",
    "        '''\n",
    "        Compute the state size based on observation type\n",
    "        '''\n",
    "        n_features_per_node = self.obs_builder.observation_dim\n",
    "        n_nodes = 1\n",
    "        if isinstance(self.obs_builder, TreeObsForRailEnv):\n",
    "            n_nodes = sum(\n",
    "                4 ** i for i in range(self.obs_builder.max_depth + 1)\n",
    "            )\n",
    "        elif isinstance(self.obs_builder, BinaryTreeObservator):\n",
    "            n_nodes = sum(2 ** i for i in range(self.obs_builder.max_depth))\n",
    "        return n_features_per_node * n_nodes\n",
    "\n",
    "    def get_agents_same_start(self):\n",
    "        '''\n",
    "        Return a dictionary indexed by agents starting positions,\n",
    "        and having a list of handles as values, s.t. agents with\n",
    "        the same starting position are ordered by decreasing speed\n",
    "        '''\n",
    "        agents_with_same_start = dict()\n",
    "        for handle_one, agent_one in enumerate(self.agents):\n",
    "            for handle_two, agent_two in enumerate(self.agents):\n",
    "                if handle_one != handle_two and agent_one.initial_position == agent_two.initial_position:\n",
    "                    agents_with_same_start.setdefault(\n",
    "                        agent_one.initial_position, set()\n",
    "                    ).update({handle_one, handle_two})\n",
    "\n",
    "        for position in agents_with_same_start:\n",
    "            agents_with_same_start[position] = sorted(\n",
    "                list(agents_with_same_start[position]), reverse=True,\n",
    "                key=lambda x: self.agents[x].speed_data['speed']\n",
    "            )\n",
    "        return agents_with_same_start\n",
    "\n",
    "    def check_if_all_blocked(self, deadlocks):\n",
    "        '''\n",
    "        Checks whether all the agents are blocked (full deadlock situation)\n",
    "        '''\n",
    "        remaining_agents = self.railway_encoding.remaining_agents_handles()\n",
    "        num_deadlocks = sum(\n",
    "            int(v) for k, v in deadlocks.items()\n",
    "            if k in remaining_agents\n",
    "        )\n",
    "        return num_deadlocks == len(remaining_agents) and len(remaining_agents) > 0\n",
    "\n",
    "    def save(self, path):\n",
    "        '''\n",
    "        Save the given RailEnv environment as pickle\n",
    "        '''\n",
    "        filename = os.path.join(\n",
    "            path, f\"{self.width}x{self.height}-{self.random_seed}.pkl\"\n",
    "        )\n",
    "        RailEnvPersister.save(self, filename)\n",
    "\n",
    "    def get_renderer(self):\n",
    "        '''\n",
    "        Return a renderer for the current environment\n",
    "        '''\n",
    "        return RenderTool(\n",
    "            self,\n",
    "            agent_render_variant=AgentRenderVariant.AGENT_SHOWS_OPTIONS_AND_BOX,\n",
    "            show_debug=True,\n",
    "            screen_height=1080,\n",
    "            screen_width=1920\n",
    "        )\n",
    "\n",
    "    def reset(self, regenerate_rail=True, regenerate_schedule=True,\n",
    "              activate_agents=False, random_seed=None):\n",
    "        '''\n",
    "        Reset the environment\n",
    "        '''\n",
    "        # Get a random seed\n",
    "        if random_seed:\n",
    "            self._seed(random_seed)\n",
    "\n",
    "        # Regenerate the rail, if necessary\n",
    "        optionals = {}\n",
    "        if regenerate_rail or self.rail is None:\n",
    "            rail, optionals = self._generate_rail()\n",
    "            self.rail = rail\n",
    "            self.height, self.width = self.rail.grid.shape\n",
    "            self.obs_builder.set_env(self)\n",
    "\n",
    "        # Set the distance map\n",
    "        if optionals and 'distance_map' in optionals:\n",
    "            self.distance_map.set(optionals['distance_map'])\n",
    "\n",
    "        # Reset agents positions\n",
    "        self.agent_positions = np.full(\n",
    "            (self.height, self.width), -1, dtype=int\n",
    "        )\n",
    "        self.reset_agents()\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            if activate_agents:\n",
    "                self.set_agent_active(agent)\n",
    "            self._break_agent(agent)\n",
    "            if agent.malfunction_data[\"malfunction\"] > 0:\n",
    "                agent.speed_data['transition_action_on_cellexit'] = RailEnvActions.DO_NOTHING\n",
    "            self._fix_agent_after_malfunction(agent)\n",
    "\n",
    "            # Reset partial rewards\n",
    "            self.partial_rewards[i] = 0.0\n",
    "\n",
    "        # Reset common variables\n",
    "        self.num_resets += 1\n",
    "        self._elapsed_steps = 0\n",
    "        self.dones = dict.fromkeys(\n",
    "            list(range(self.get_num_agents())) + [\"__all__\"], False\n",
    "        )\n",
    "        self.arrived_turns = [None] * self.get_num_agents()\n",
    "        self.stop_actions = [0] * self.get_num_agents()\n",
    "\n",
    "        # Build the cell orientation graph\n",
    "        self.railway_encoding = CellOrientationGraph(\n",
    "            grid=self.rail.grid, agents=self.agents\n",
    "        )\n",
    "\n",
    "        # Reset the state of the observation builder with the new environment\n",
    "        self.obs_builder.reset()\n",
    "        self.distance_map.reset(self.agents, self.rail)\n",
    "\n",
    "        # Empty the episode store of agent positions\n",
    "        self.cur_episode = []\n",
    "\n",
    "        # Compute deadlocks\n",
    "        self.deadlocks_detector.reset(self.get_num_agents())\n",
    "\n",
    "        # Build the info dict\n",
    "        self.current_info = {\n",
    "            'action_required': {}, 'malfunction': {}, 'speed': {},\n",
    "            'status': {}, 'deadlocks': {}, 'deadlock_turns': {}, 'finished': {},\n",
    "            'first_time_deadlock': {}, 'first_time_finished': {}\n",
    "        }\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            self.current_info['action_required'][i] = self.action_required(\n",
    "                agent\n",
    "            )\n",
    "            self.current_info['malfunction'][i] = agent.malfunction_data['malfunction']\n",
    "            self.current_info['speed'][i] = agent.speed_data['speed']\n",
    "            self.current_info['status'][i] = agent.status\n",
    "            self.current_info[\"deadlocks\"][i] = self.deadlocks_detector.deadlocks[i]\n",
    "            self.current_info[\"deadlock_turns\"][i] = self.deadlocks_detector.deadlock_turns[i]\n",
    "            self.current_info[\"finished\"][i] = self.dones[i] or self.deadlocks_detector.deadlocks[i]\n",
    "            self.current_info[\"first_time_deadlock\"][i] = (\n",
    "                self.deadlocks_detector.deadlocks[i] and\n",
    "                0 == self.deadlocks_detector.deadlock_turns[i]\n",
    "            )\n",
    "            self.current_info[\"first_time_finished\"][i] = (\n",
    "                self.dones[i] and\n",
    "                0 == self.arrived_turns[i]\n",
    "            )\n",
    "\n",
    "        # Return the new observation vectors for each agent\n",
    "        observation_dict = self._get_observations()\n",
    "        return (self._normalize_obs(observation_dict), self.current_info)\n",
    "\n",
    "    def _generate_rail(self):\n",
    "        '''\n",
    "        Regenerate the rail, if necessary\n",
    "        '''\n",
    "        if \"__call__\" in dir(self.rail_generator):\n",
    "            return self.rail_generator(\n",
    "                self.width, self.height, self.number_of_agents, self.num_resets, self.np_random\n",
    "            )\n",
    "        elif \"generate\" in dir(self.rail_generator):\n",
    "            return self.rail_generator.generate(\n",
    "                self.width, self.height, self.number_of_agents, self.num_resets, self.np_random\n",
    "            )\n",
    "        raise ValueError(\n",
    "            \"Could not invoke __call__ or generate on rail_generator\"\n",
    "        )\n",
    "\n",
    "    def step(self, action_dict_):\n",
    "        '''\n",
    "        Perform a step in the environment\n",
    "        '''\n",
    "        current_step = self._elapsed_steps\n",
    "        agents_in_decision_cells = self.agents_in_decision_cells()\n",
    "        obs, rewards, dones, info = super().step(action_dict_)\n",
    "        info[\"deadlocks\"], info[\"deadlock_turns\"] = self.deadlocks_detector.step(\n",
    "            self\n",
    "        )\n",
    "\n",
    "        # Patch dones dict, update arrived agents turns and stop actions\n",
    "        # and store other data in info dict\n",
    "        finished, first_time_deadlock, first_time_finished = dict(), dict(), dict()\n",
    "        remove_all = False\n",
    "        for agent in range(self.get_num_agents()):\n",
    "            # Update dones\n",
    "            if dones[agent] and not self.railway_encoding.is_done(agent):\n",
    "                dones[agent] = False\n",
    "                remove_all = True\n",
    "\n",
    "            # Update arrived agents and first time dones\n",
    "            if dones[agent] and self.arrived_turns[agent] is None:\n",
    "                self.arrived_turns[agent] = current_step\n",
    "                first_time_finished[agent] = True\n",
    "            else:\n",
    "                first_time_finished[agent] = False\n",
    "\n",
    "            # Store the number of consequent stop actions\n",
    "            if action_dict_[agent] == RailEnvActions.STOP_MOVING.value:\n",
    "                self.stop_actions[agent] += 1\n",
    "            else:\n",
    "                self.stop_actions[agent] = 0\n",
    "\n",
    "            # Update done or deadlocked agents and first time deadlocked agents\n",
    "            finished[agent] = dones[agent] or info['deadlocks'][agent]\n",
    "            first_time_deadlock[agent] = (\n",
    "                info['deadlocks'][agent] and\n",
    "                current_step == info['deadlock_turns'][agent]\n",
    "            )\n",
    "\n",
    "        # Patch info dict\n",
    "        info['finished'] = finished\n",
    "        info['first_time_finished'] = first_time_finished\n",
    "        info['first_time_deadlock'] = first_time_deadlock\n",
    "\n",
    "        # If at least one agent is not at target, then\n",
    "        # the __all__ flag of dones should be False\n",
    "        if remove_all:\n",
    "            dones[\"__all__\"] = False\n",
    "\n",
    "        # Compute custom rewards\n",
    "        custom_rewards = self._reward_shaping(\n",
    "            action_dict_, rewards, dones, info, agents_in_decision_cells\n",
    "        )\n",
    "\n",
    "        # Update last info dict\n",
    "        self.current_info = info\n",
    "\n",
    "        return (self._normalize_obs(obs), rewards, custom_rewards, dones, info)\n",
    "\n",
    "    def _reward_shaping(self, action_dict_, rewards, dones, info, agents_in_decision_cells):\n",
    "        '''\n",
    "        Apply custom reward functions\n",
    "        '''\n",
    "        # The step for which we are evaluating the rewards\n",
    "        # is the previous one\n",
    "        step = self._elapsed_steps - 1\n",
    "\n",
    "        custom_rewards = copy.deepcopy(rewards)\n",
    "        for agent in range(self.get_num_agents()):\n",
    "            # Return a positive reward equal to the maximum number of steps\n",
    "            # minus the current number of steps if the agent has arrived\n",
    "            if dones[agent] and self.arrived_turns[agent] == step:\n",
    "                done_reward = self._max_episode_steps - step\n",
    "                custom_rewards[agent] = done_reward\n",
    "                self.partial_rewards[agent] = done_reward\n",
    "            # Return minus the maximum number of steps if the agent is in deadlock\n",
    "            elif info[\"deadlocks\"][agent] and info[\"deadlock_turns\"][agent] == step:\n",
    "                deadlock_penalty = -self._max_episode_steps\n",
    "                custom_rewards[agent] = deadlock_penalty\n",
    "                self.partial_rewards[agent] = deadlock_penalty\n",
    "            # Accumulate rewards for choices if the agent is not in a decision cell\n",
    "            # and add other penalties, such as the stop moving one\n",
    "            else:\n",
    "                self.partial_rewards[agent] += custom_rewards[agent]\n",
    "                # If an agent performed a STOP action, give the agent a reward\n",
    "                # which is worse than the the reward it could have received\n",
    "                # by choosing any other action\n",
    "                if action_dict_[agent] == RailEnvActions.STOP_MOVING.value:\n",
    "                    weight = self.railway_encoding.stop_moving_worst_alternative_weight(\n",
    "                        agent\n",
    "                    )\n",
    "                    weight *= (1 / self.agents[agent].speed_data['speed'])\n",
    "                    weight *= (\n",
    "                        self.stop_actions[agent] *\n",
    "                        self.params.env.rewards.stop_penalty\n",
    "                    )\n",
    "                    # Clip stop reward to be at maximum equal to the deadlock penalty\n",
    "                    self.partial_rewards[agent] += np.clip(\n",
    "                        -weight, -self._max_episode_steps, 0\n",
    "                    )\n",
    "                custom_rewards[agent] = 0.0\n",
    "\n",
    "            # Reset the partial rewards counter when an agent is\n",
    "            # in a decision cell or in the last step\n",
    "            if agents_in_decision_cells[agent] or self._elapsed_steps == self._max_episode_steps:\n",
    "                custom_rewards[agent] = self.partial_rewards[agent]\n",
    "                self.partial_rewards[agent] = 0.0\n",
    "\n",
    "            # Normalize rewards\n",
    "            custom_rewards[agent] /= self._max_episode_steps\n",
    "\n",
    "        return custom_rewards\n",
    "\n",
    "    def agents_in_decision_cells(self):\n",
    "        '''\n",
    "        Return the agents that are on a decision cell\n",
    "        '''\n",
    "        return [\n",
    "            self.railway_encoding.is_real_decision(h)\n",
    "            for h in range(self.get_num_agents())\n",
    "        ]\n",
    "\n",
    "    def agents_adjacency_matrix(self, radius=None):\n",
    "        '''\n",
    "        Return the adjacency matrix containing pairwise distances between agents\n",
    "        '''\n",
    "        adj = np.zeros((self.get_num_agents(), self.get_num_agents()))\n",
    "        for i in range(adj.shape[0]):\n",
    "            for j in range(adj.shape[1]):\n",
    "                if i != j:\n",
    "                    distance = self.railway_encoding.get_agents_distance(i, j)\n",
    "                    if (distance is not None and\n",
    "                            (radius is None or (radius is not None and distance <= radius))):\n",
    "                        adj[i, j] = distance\n",
    "        return adj\n",
    "\n",
    "    def pre_act(self):\n",
    "        '''\n",
    "        Return the list of legal actions and choices for each agent and a list\n",
    "        representing which agent needs to make a choice\n",
    "        '''\n",
    "        legal_choices = np.full(\n",
    "            (self.get_num_agents(), RailEnvChoices.choice_size()),\n",
    "            RailEnvChoices.default_choices()\n",
    "        )\n",
    "        legal_actions = np.full(\n",
    "            (self.get_num_agents(), self.num_actions), False\n",
    "        )\n",
    "        moving_agents = np.full((self.get_num_agents(),), False)\n",
    "\n",
    "        # Compute which agents need to make a choice\n",
    "        for agent in self.get_agent_handles():\n",
    "            legal_actions[\n",
    "                agent, self.railway_encoding.get_agent_actions(agent)\n",
    "            ] = True\n",
    "            if (self.current_info['action_required'][agent] and\n",
    "                    self.railway_encoding.is_real_decision(agent)):\n",
    "                legal_choices[agent] = self.railway_encoding.get_legal_choices(\n",
    "                    agent, legal_actions[agent]\n",
    "                )\n",
    "                moving_agents[agent] = True\n",
    "\n",
    "        return legal_actions, legal_choices, moving_agents\n",
    "\n",
    "    def post_act(self, choices, is_best, legal_actions, moving_agents):\n",
    "        '''\n",
    "        Return the action dictionary (to be given to the environment step)\n",
    "        and a dictionary of training metadatas\n",
    "        '''\n",
    "        action_dict, choice_dict = dict(), dict()\n",
    "        choices_count = np.zeros((RailEnvChoices.choice_size(),))\n",
    "        num_exploration_choices = np.zeros_like(choices_count)\n",
    "\n",
    "        # Assign an action to each agent\n",
    "        for agent in self.get_agent_handles():\n",
    "            action = RailEnvActions.DO_NOTHING.value\n",
    "            if moving_agents[agent]:\n",
    "                action = self.railway_encoding.map_choice_to_action(\n",
    "                    choices[agent], legal_actions[agent]\n",
    "                )\n",
    "                assert action != RailEnvActions.DO_NOTHING.value, (\n",
    "                    choices[agent], legal_actions[agent]\n",
    "                )\n",
    "                choices_count[choices[agent]] += 1\n",
    "                num_exploration_choices[choices[agent]] += int(\n",
    "                    not(is_best[agent])\n",
    "                )\n",
    "                choice_dict[agent] = choices[agent]\n",
    "            elif (not self.dones[agent] and\n",
    "                  self.agents[agent].speed_data['position_fraction'] == 0):\n",
    "                actions = np.flatnonzero(legal_actions[agent])\n",
    "                assert actions.shape[0] > 0, actions\n",
    "                action = actions[0]\n",
    "                if actions.shape[0] > 1:\n",
    "                    action = RailEnvActions.DO_NOTHING.value\n",
    "            action_dict[agent] = action\n",
    "\n",
    "        # Build the metadata dict\n",
    "        metadata = {\n",
    "            'choices_count': choices_count,\n",
    "            'num_exploration_choices': num_exploration_choices,\n",
    "            'choice_dict': choice_dict\n",
    "        }\n",
    "\n",
    "        return action_dict, metadata\n",
    "\n",
    "    def pre_step(self, experience):\n",
    "        '''\n",
    "        To be called before the policy step function, \n",
    "        it returns a list of experiences to be passed to the policy step\n",
    "        '''\n",
    "        (\n",
    "            prev_obs, prev_choices, custom_rewards,\n",
    "            obs, legal_choices, update_values\n",
    "        ) = experience\n",
    "        finished = np.array(list(self.current_info['finished'].values()))\n",
    "        experiences = []\n",
    "\n",
    "        # Gather valuable experiences\n",
    "        for agent in self.get_agent_handles():\n",
    "            if (update_values[agent] or\n",
    "                    self.current_info['first_time_finished'][agent] or\n",
    "                    self.current_info['first_time_deadlock'][agent]):\n",
    "                # Check for policy type\n",
    "                if self.params.policy.type.decentralized_fov:\n",
    "                    exp = (\n",
    "                        prev_obs[agent],\n",
    "                        list(prev_choices[agent].values()),\n",
    "                        np.array(list(custom_rewards.values())),\n",
    "                        obs[agent],\n",
    "                        legal_choices,\n",
    "                        finished,\n",
    "                        update_values\n",
    "                    )\n",
    "                else:\n",
    "                    exp = (\n",
    "                        prev_obs[agent],\n",
    "                        prev_choices[agent],\n",
    "                        custom_rewards[agent],\n",
    "                        obs[agent],\n",
    "                        legal_choices[agent],\n",
    "                        finished[agent],\n",
    "                        update_values[agent]\n",
    "                    )\n",
    "                experiences.append(exp)\n",
    "\n",
    "        return experiences\n",
    "\n",
    "    def post_step(self, obs, choice_dict, next_obs, update_values, rewards, custom_rewards):\n",
    "        '''\n",
    "        To be called after the policy step function, \n",
    "        it returns a dictionary of training metadatas\n",
    "        '''\n",
    "        prev_obs, prev_choices = dict(), dict()\n",
    "        score, custom_score = 0.0, 0.0\n",
    "\n",
    "        for agent in self.get_agent_handles():\n",
    "            # Update previous observations and choices\n",
    "            if (update_values[agent] or\n",
    "                    self.current_info['first_time_finished'][agent] or\n",
    "                    self.current_info['first_time_deadlock'][agent]):\n",
    "                prev_obs[agent] = copy_obs(obs[agent])\n",
    "                if self.params.policy.type.decentralized_fov:\n",
    "                    prev_choices[agent] = dict(choice_dict)\n",
    "                else:\n",
    "                    prev_choices[agent] = choice_dict[agent]\n",
    "\n",
    "            # Update observation and score\n",
    "            score += rewards[agent]\n",
    "            custom_score += custom_rewards[agent]\n",
    "            if next_obs[agent] is not None:\n",
    "                if self.params.policy.type.decentralized_fov:\n",
    "                    obs[agent] = next_obs[agent]\n",
    "                else:\n",
    "                    obs[agent] = copy_obs(next_obs[agent])\n",
    "\n",
    "        # Build and return the metadata dict\n",
    "        return {\n",
    "            'obs': obs,\n",
    "            'prev_obs': prev_obs,\n",
    "            'prev_choices': prev_choices,\n",
    "            'score': score,\n",
    "            'custom_score': custom_score\n",
    "        }\n",
    "\n",
    "    def _normalize_obs(self, obs):\n",
    "        '''\n",
    "        Normalize observations\n",
    "        '''\n",
    "        if not self.normalize:\n",
    "            return obs\n",
    "\n",
    "        for handle in obs:\n",
    "            if obs[handle] is not None:\n",
    "                # Normalize tree observation\n",
    "                if isinstance(self.obs_builder, TreeObsForRailEnv):\n",
    "                    obs[handle] = normalize_tree_obs(\n",
    "                        obs[handle], self.obs_builder.max_depth,\n",
    "                        self.params.observator.tree.radius\n",
    "                    )\n",
    "\n",
    "        return obs\n",
    "\n",
    "## Encoding of the railway map in a cell orientation graph\n",
    "TRANS = [\n",
    "    Grid4TransitionsEnum.NORTH,\n",
    "    Grid4TransitionsEnum.EAST,\n",
    "    Grid4TransitionsEnum.SOUTH,\n",
    "    Grid4TransitionsEnum.WEST\n",
    "]\n",
    "\n",
    "def agent_action(original_dir, final_dir):\n",
    "    '''\n",
    "    Return the action performed by an agent, by analyzing\n",
    "    the starting direction and the final direction of the movement\n",
    "    '''\n",
    "    value = (final_dir.value - original_dir.value) % 4\n",
    "    if value in (1, -3):\n",
    "        return RailEnvActions.MOVE_RIGHT\n",
    "    elif value in (-1, 3):\n",
    "        return RailEnvActions.MOVE_LEFT\n",
    "    return RailEnvActions.MOVE_FORWARD\n",
    "\n",
    "class CellOrientationGraph():\n",
    "\n",
    "    _BITMAP_TO_TRANS = [(t1, t2) for t1 in TRANS for t2 in TRANS]\n",
    "\n",
    "    def __init__(self, grid, agents):\n",
    "        self.grid = grid\n",
    "        self.agents = agents\n",
    "        self.graph = None\n",
    "        self._unpacked_graph = None\n",
    "        self._dead_ends = set()\n",
    "        self._straight_rails = set()\n",
    "\n",
    "        # For each target position, store associated agents\n",
    "        self._targets = dict()\n",
    "        for agent in agents:\n",
    "            self._targets.setdefault(agent.target, []).append(agent.handle)\n",
    "\n",
    "        # Build the packed and unpacked graphs\n",
    "        self._generate_graph()\n",
    "\n",
    "        # Store the node to index and index to node mappings of\n",
    "        # the packed graph\n",
    "        self.node_to_index, self.index_to_node = self._build_vocab(\n",
    "            unpacked=False\n",
    "        )\n",
    "\n",
    "    def _generate_graph(self):\n",
    "        '''\n",
    "        Generate both the unpacked and the packed graph and\n",
    "        set default attributes to the nodes in the packed graph\n",
    "        '''\n",
    "        edges = self._generate_edges()\n",
    "        self._unpacked_graph = nx.DiGraph()\n",
    "        self._unpacked_graph.add_edges_from(edges)\n",
    "        nx.freeze(self._unpacked_graph)\n",
    "        self.graph = nx.DiGraph(self._unpacked_graph)\n",
    "        self._pack_graph()\n",
    "        self._set_nodes_attributes()\n",
    "\n",
    "    def _generate_edges(self):\n",
    "        '''\n",
    "        Translate the environment grid to the unpacked cell orientation graph\n",
    "        '''\n",
    "        edges = []\n",
    "        for i, row in enumerate(self.grid):\n",
    "            for j, _ in enumerate(row):\n",
    "                if self.grid[i][j] != 0:\n",
    "                    trans_int = self.grid[i][j]\n",
    "                    trans_bitmap = format(trans_int, 'b').rjust(16, '0')\n",
    "                    num_ones = trans_bitmap.count('1')\n",
    "                    if num_ones == 2:\n",
    "                        self._straight_rails.add((i, j))\n",
    "                    elif num_ones == 1:\n",
    "                        self._dead_ends.add((i, j))\n",
    "                    tmp_edges, tmp_actions = [], dict()\n",
    "                    for k, bit in enumerate(trans_bitmap):\n",
    "                        if bit == '1':\n",
    "                            original_dir, final_dir = self._BITMAP_TO_TRANS[k]\n",
    "                            new_position_x, new_position_y = get_new_position(\n",
    "                                [i, j], final_dir.value\n",
    "                            )\n",
    "                            tmp_action = agent_action(\n",
    "                                original_dir, final_dir\n",
    "                            )\n",
    "                            tmp_edges.append((\n",
    "                                (i, j, original_dir.value),\n",
    "                                (new_position_x, new_position_y, final_dir.value),\n",
    "                                tmp_action\n",
    "                            ))\n",
    "                            tmp_actions.setdefault(\n",
    "                                (i, j, original_dir.value),\n",
    "                                np.full((get_num_actions(),), False)\n",
    "                            )[tmp_action.value] = True\n",
    "\n",
    "                    for tmp_edge in tmp_edges:\n",
    "                        tmp_choice = self.map_action_to_choice(\n",
    "                            tmp_edge[2], tmp_actions[tmp_edge[0]]\n",
    "                        )\n",
    "                        edge = (\n",
    "                            tmp_edge[0],\n",
    "                            tmp_edge[1],\n",
    "                            {\n",
    "                                'weight': 1,\n",
    "                                'action': tmp_edge[2],\n",
    "                                'choice': tmp_choice\n",
    "                            }\n",
    "                        )\n",
    "                        edges.append(edge)\n",
    "        return edges\n",
    "\n",
    "    def _pack_graph(self):\n",
    "        '''\n",
    "        Generate a compact version of the cell orientation graph,\n",
    "        by only keeping junctions, targets and dead ends\n",
    "        '''\n",
    "        to_remove = self._straight_rails.difference(\n",
    "            set(self._targets.keys())\n",
    "        )\n",
    "        for cell in to_remove:\n",
    "            self._remove_cell(cell)\n",
    "\n",
    "    def _remove_node(self, node):\n",
    "        '''\n",
    "        Remove a node from the in-construction packed graph and\n",
    "        add an edge between the neighboring nodes, while\n",
    "        also propagating edges data\n",
    "        '''\n",
    "        sources = [\n",
    "            (source, data)\n",
    "            for source, _, data in self.graph.in_edges(node, data=True)\n",
    "        ]\n",
    "        targets = [\n",
    "            (target, data)\n",
    "            for _, target, data in self.graph.out_edges(node, data=True)\n",
    "        ]\n",
    "        new_edges = [\n",
    "            (\n",
    "                source[0], target[0],\n",
    "                {\n",
    "                    'weight': source[1]['weight'] + target[1]['weight'],\n",
    "                    'action': source[1]['action'],\n",
    "                    'choice': source[1]['choice']\n",
    "                }\n",
    "            )\n",
    "            for source in sources for target in targets\n",
    "        ]\n",
    "        self.graph.add_edges_from(new_edges)\n",
    "        self.graph.remove_node(node)\n",
    "\n",
    "    def _remove_cell(self, position):\n",
    "        '''\n",
    "        Remove the given cell with every direction component,\n",
    "        in order to build the packed graph\n",
    "        '''\n",
    "        nodes = self.get_nodes(position)\n",
    "        for node in nodes:\n",
    "            self._remove_node(node)\n",
    "\n",
    "    def _set_nodes_attribute(self, name, positions=None, value=None, default=None):\n",
    "        '''\n",
    "        Set the attribute \"name\" to the nodes given in the set \"positions\",\n",
    "        to be \"value\" (could be a single value or a dictionary indexed by \"positions\").\n",
    "        If the \"value\" argument is a dictionary, you can give a default value to be set\n",
    "        to the nodes which are not present in the set \"positions\"\n",
    "        '''\n",
    "        if default is not None:\n",
    "            nx.set_node_attributes(self.graph, default, name)\n",
    "        attributes = {}\n",
    "        if positions is not None and value is not None:\n",
    "            for pos in positions:\n",
    "                nodes = [pos]\n",
    "                if len(pos) == 2:\n",
    "                    nodes = self.get_nodes(pos)\n",
    "                for node in nodes:\n",
    "                    val = value\n",
    "                    if isinstance(value, dict):\n",
    "                        val = value[pos]\n",
    "                    attributes[node] = {name:  val}\n",
    "            nx.set_node_attributes(self.graph, attributes)\n",
    "\n",
    "    def _set_nodes_attributes(self):\n",
    "        '''\n",
    "        Set default attributes for each and every node in the packed graph\n",
    "        '''\n",
    "        self._set_nodes_attribute(\n",
    "            'is_dead_end', positions=self._dead_ends, value=True, default=False\n",
    "        )\n",
    "        self._set_nodes_attribute(\n",
    "            'is_target', positions=set(self._targets.keys()), value=True, default=False\n",
    "        )\n",
    "        fork_positions, join_positions = self._compute_decision_types()\n",
    "        self._set_nodes_attribute(\n",
    "            'is_fork', positions=fork_positions, value=True, default=False\n",
    "        )\n",
    "        self._set_nodes_attribute(\n",
    "            'is_join', positions=join_positions, value=True, default=False\n",
    "        )\n",
    "\n",
    "    def _compute_decision_types(self):\n",
    "        '''\n",
    "        Set decision types (at fork and/or at join) for each node in the packed graph\n",
    "        '''\n",
    "        fork_positions, join_positions = set(), set()\n",
    "        for node in self.graph.nodes:\n",
    "            if not self.graph.nodes[node]['is_dead_end']:\n",
    "                other_nodes = set(self.get_nodes(node)) - {node}\n",
    "                # If diamond crossing and/or fork set join for other nodes\n",
    "                num_successors = len(self.get_successors(node))\n",
    "                if len(other_nodes) == 3 or num_successors > 1:\n",
    "                    for other_node in other_nodes:\n",
    "                        join_positions.add(other_node)\n",
    "                # Set fork for current node\n",
    "                if num_successors > 1:\n",
    "                    fork_positions.add(node)\n",
    "        return fork_positions, join_positions\n",
    "\n",
    "    def _build_vocab(self, unpacked=False):\n",
    "        '''\n",
    "        Build a vocabulary, mapping nodes to indexes and vice-versa\n",
    "        '''\n",
    "        graph = self.graph if not unpacked else self._unpacked_graph\n",
    "        nodes = sorted(list(graph.nodes()))\n",
    "        node_to_index = {node: i for i, node in enumerate(nodes)}\n",
    "        index_to_node = {i: node for i, node in enumerate(nodes)}\n",
    "        return node_to_index, index_to_node\n",
    "\n",
    "    def is_straight_rail(self, cell):\n",
    "        '''\n",
    "        Check if the given cell is a straight rail\n",
    "        '''\n",
    "        if len(cell) > 2:\n",
    "            cell = cell[:-1]\n",
    "        return cell in self._straight_rails\n",
    "\n",
    "    def get_nodes(self, position, unpacked=False):\n",
    "        '''\n",
    "        Given a position (row, column), return a list\n",
    "        of nodes present in the packed or unpacked graph of the type\n",
    "        [(row, column, NORTH), ..., (row, column, WEST)]\n",
    "        '''\n",
    "        nodes = []\n",
    "        for direction in TRANS:\n",
    "            node = (position[0], position[1], direction.value)\n",
    "            node_in_packed = not unpacked and self.graph.has_node(node)\n",
    "            node_in_unpacked = unpacked and self._unpacked_graph.has_node(node)\n",
    "            if node_in_packed or node_in_unpacked:\n",
    "                nodes.append(node)\n",
    "        return nodes\n",
    "\n",
    "    def is_node(self, node, unpacked=False):\n",
    "        '''\n",
    "        Return true if the given node is present in the packed or\n",
    "        unpacked graph\n",
    "        '''\n",
    "        graph = self._unpacked_graph if unpacked else self.graph\n",
    "        return node in graph.nodes\n",
    "\n",
    "    def get_edge_data(self, u, v, t, unpacked=False):\n",
    "        '''\n",
    "        Return the feature `t` in edge `(u, v)`\n",
    "        '''\n",
    "        graph = self.graph if not unpacked else self._unpacked_graph\n",
    "        assert (u, v) in graph.edges\n",
    "        edge_data = graph.get_edge_data(u, v)\n",
    "        assert t in edge_data\n",
    "        return edge_data[t]\n",
    "\n",
    "    def get_predecessors(self, node, unpacked=False):\n",
    "        '''\n",
    "        Return the predecessors of the given node in the packed or\n",
    "        unpacked graph\n",
    "        '''\n",
    "        graph = self._unpacked_graph if unpacked else self.graph\n",
    "        if node not in graph.nodes:\n",
    "            return []\n",
    "        return list(graph.predecessors(node))\n",
    "\n",
    "    def get_successors(self, node, unpacked=False):\n",
    "        '''\n",
    "        Return the successors of the given node in the packed or\n",
    "        unpacked graph\n",
    "        '''\n",
    "        graph = self._unpacked_graph if unpacked else self.graph\n",
    "        if node not in graph.nodes:\n",
    "            return []\n",
    "        return list(graph.successors(node))\n",
    "\n",
    "    def next_node(self, cell):\n",
    "        '''\n",
    "        Return the closest node in the packed graph\n",
    "        w.r.t. the given cell in the unpacked graph,\n",
    "        in the same direction\n",
    "        '''\n",
    "        if cell in self.graph.nodes:\n",
    "            return cell, 0\n",
    "        weight = 0\n",
    "        successors = self._unpacked_graph.successors(cell)\n",
    "        while True:\n",
    "            try:\n",
    "                cell = next(successors)\n",
    "                weight += 1\n",
    "                if cell in self.graph.nodes:\n",
    "                    return cell, weight\n",
    "                successors = self._unpacked_graph.successors(cell)\n",
    "            except StopIteration:\n",
    "                break\n",
    "        return None\n",
    "\n",
    "    def previous_node(self, cell):\n",
    "        '''\n",
    "        Return the closest node in the packed graph\n",
    "        w.r.t. the given cell in the unpacked graph,\n",
    "        in the opposite direction\n",
    "        '''\n",
    "        if cell in self.graph.nodes:\n",
    "            return cell, 0\n",
    "        weight = 0\n",
    "        next_node, _ = self.next_node(cell)\n",
    "        predecessors = self._unpacked_graph.predecessors(cell)\n",
    "        while True:\n",
    "            try:\n",
    "                cell = next(predecessors)\n",
    "                weight += 1\n",
    "                edge = (cell, next_node)\n",
    "                if edge in self.graph.edges:\n",
    "                    return cell, weight\n",
    "                predecessors = itertools.chain(\n",
    "                    predecessors, self._unpacked_graph.predecessors(cell)\n",
    "                )\n",
    "            except StopIteration:\n",
    "                break\n",
    "        return None\n",
    "\n",
    "    def get_agent_cell(self, handle):\n",
    "        '''\n",
    "        Return the unpacked graph node in which the agent\n",
    "        identified by the given handle is\n",
    "        '''\n",
    "        position = None\n",
    "        agent = self.agents[handle]\n",
    "        if agent.status == TrainState.READY_TO_DEPART:\n",
    "            position = (\n",
    "                agent.initial_position[0],\n",
    "                agent.initial_position[1],\n",
    "                agent.initial_direction\n",
    "            )\n",
    "        elif agent.status == TrainState.MOVING:\n",
    "            position = (\n",
    "                agent.position[0],\n",
    "                agent.position[1],\n",
    "                agent.direction\n",
    "            )\n",
    "        elif agent.status == TrainState.DONE:\n",
    "            position = (\n",
    "                agent.target[0],\n",
    "                agent.target[1],\n",
    "                agent.direction\n",
    "            )\n",
    "        return position\n",
    "\n",
    "    def stop_moving_worst_alternative_weight(self, handle):\n",
    "        '''\n",
    "        Return the weight associated with the worst move alternative\n",
    "        to a stop choice, starting from the position of the agent\n",
    "        '''\n",
    "        position = self.get_agent_cell(handle)\n",
    "        node, weight = self.next_node(position)\n",
    "        nodes = []\n",
    "        if self.is_join(node):\n",
    "            nodes = [(node, weight)]\n",
    "        else:\n",
    "            successors = self.get_successors(node, unpacked=True)\n",
    "            for succ in successors:\n",
    "                succ_weight = self.get_edge_data(\n",
    "                    node, succ, 'weight', unpacked=True\n",
    "                )\n",
    "                assert succ_weight == 1\n",
    "                if self.is_join(succ):\n",
    "                    nodes.append((succ, succ_weight + weight))\n",
    "\n",
    "        max_weight = 0\n",
    "        for start_node, start_weight in nodes:\n",
    "            successors = self.get_successors(start_node, unpacked=False)\n",
    "            for succ in successors:\n",
    "                succ_weight = self.get_edge_data(\n",
    "                    start_node, succ, 'weight', unpacked=False\n",
    "                )\n",
    "                if succ_weight > max_weight:\n",
    "                    max_weight = succ_weight + start_weight\n",
    "                    max_succ = succ\n",
    "\n",
    "        return max_weight\n",
    "\n",
    "    def is_done(self, handle):\n",
    "        '''\n",
    "        Returns True if an agent arrived at its target\n",
    "        '''\n",
    "        return self.agents[handle].status in (\n",
    "            TrainState.DONE\n",
    "        )\n",
    "\n",
    "    def map_choice_to_action(self, choice, actions):\n",
    "        '''\n",
    "        Map the given RailEnvChoices choice to a RailEnvActions action\n",
    "        '''\n",
    "        # If CHOICE_LEFT, then priorities are MOVE_LEFT, MOVE_FORWARD, MOVE_RIGHT\n",
    "        if choice == RailEnvChoices.CHOICE_LEFT.value:\n",
    "            if actions[RailEnvActions.MOVE_LEFT.value]:\n",
    "                return RailEnvActions.MOVE_LEFT\n",
    "            elif actions[RailEnvActions.MOVE_FORWARD.value]:\n",
    "                return RailEnvActions.MOVE_FORWARD\n",
    "            elif actions[RailEnvActions.MOVE_RIGHT.value]:\n",
    "                return RailEnvActions.MOVE_RIGHT\n",
    "        # If CHOICE_RIGHT, then priorities are MOVE_RIGHT, MOVE_FORWARD\n",
    "        elif choice == RailEnvChoices.CHOICE_RIGHT.value:\n",
    "            if actions[RailEnvActions.MOVE_RIGHT.value]:\n",
    "                return RailEnvActions.MOVE_RIGHT\n",
    "            elif actions[RailEnvActions.MOVE_FORWARD.value]:\n",
    "                return RailEnvActions.MOVE_FORWARD\n",
    "        # If STOP, then the priority is STOP_MOVING\n",
    "        elif choice == RailEnvChoices.STOP.value:\n",
    "            return RailEnvActions.STOP_MOVING\n",
    "        # Otherwise, last resort is DO_NOTHING\n",
    "        return RailEnvActions.DO_NOTHING\n",
    "\n",
    "    def map_action_to_choice(self, action, actions):\n",
    "        '''\n",
    "        Map the given RailEnvActions action to a RailEnvChoices choice\n",
    "        '''\n",
    "        if action == RailEnvActions.MOVE_LEFT and actions[RailEnvActions.MOVE_LEFT.value]:\n",
    "            return RailEnvChoices.CHOICE_LEFT\n",
    "        if action == RailEnvActions.MOVE_RIGHT and actions[RailEnvActions.MOVE_RIGHT.value]:\n",
    "            if np.count_nonzero(actions) > 1:\n",
    "                return RailEnvChoices.CHOICE_RIGHT\n",
    "            elif np.count_nonzero(actions) == 1:\n",
    "                return RailEnvChoices.CHOICE_LEFT\n",
    "        if action == RailEnvActions.MOVE_FORWARD and actions[RailEnvActions.MOVE_FORWARD.value]:\n",
    "            if actions[RailEnvActions.MOVE_LEFT.value]:\n",
    "                return RailEnvChoices.CHOICE_RIGHT\n",
    "            if actions[RailEnvActions.MOVE_RIGHT.value]:\n",
    "                return RailEnvChoices.CHOICE_LEFT\n",
    "            return RailEnvChoices.CHOICE_LEFT\n",
    "        return RailEnvChoices.STOP\n",
    "\n",
    "    def get_possible_choices(self, position, actions):\n",
    "        '''\n",
    "        Map the given RailEnvActions actions to a list of RailEnvChoices\n",
    "        '''\n",
    "        # If only one agent, stop moving is not legal\n",
    "        possible_moves = np.full(\n",
    "            (RailEnvChoices.choice_size(),), False)\n",
    "        possible_moves[RailEnvChoices.STOP.value] = (\n",
    "            self.is_before_join(position) and not self.only_one_agent())\n",
    "\n",
    "        if actions[RailEnvActions.MOVE_FORWARD.value]:\n",
    "            # If RailEnvActions.MOVE_LEFT or RailEnvActions.MOVE_RIGHT in legal actions\n",
    "            if np.count_nonzero(actions) > 1:\n",
    "                possible_moves[RailEnvChoices.CHOICE_RIGHT.value] = True\n",
    "            possible_moves[RailEnvChoices.CHOICE_LEFT.value] = True\n",
    "        if actions[RailEnvActions.MOVE_LEFT.value]:\n",
    "            possible_moves[RailEnvChoices.CHOICE_LEFT.value] = True\n",
    "        if actions[RailEnvActions.MOVE_RIGHT.value]:\n",
    "            # If only RailEnvActions.MOVE_RIGHT in legal actions\n",
    "            if np.count_nonzero(actions) == 1:\n",
    "                possible_moves[RailEnvChoices.CHOICE_LEFT.value] = True\n",
    "            else:\n",
    "                possible_moves[RailEnvChoices.CHOICE_RIGHT.value] = True\n",
    "        return possible_moves\n",
    "\n",
    "    def get_legal_choices(self, handle, actions):\n",
    "        '''\n",
    "        Map the given RailEnvActions actions to a list of RailEnvChoices,\n",
    "        by considering the position of the agent\n",
    "        '''\n",
    "        # If the agent is arrived, only stop moving is possible\n",
    "        # (necessary because of flatland bug)\n",
    "        if self.is_done(handle):\n",
    "            return RailEnvChoices.default_choices()\n",
    "\n",
    "        return self.get_possible_choices(self.get_agent_cell(handle), actions)\n",
    "\n",
    "    def is_fork(self, position):\n",
    "        '''\n",
    "        Return True iff the given position is a fork\n",
    "        '''\n",
    "        if position in self.graph.nodes:\n",
    "            return self.graph.nodes[position]['is_fork']\n",
    "        return False\n",
    "\n",
    "    def is_join(self, position):\n",
    "        '''\n",
    "        Return True iff the given position is a join\n",
    "        '''\n",
    "        if position in self.graph.nodes and self.graph.nodes[position]['is_join']:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def is_before_join(self, position):\n",
    "        '''\n",
    "        Return True iff the given position is before a join cell\n",
    "        '''\n",
    "        successors = self.get_successors(position, unpacked=True)\n",
    "        for succ in successors:\n",
    "            if self.is_join(succ):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def is_at_fork(self, handle):\n",
    "        '''\n",
    "        Returns True iff the agent is at a fork\n",
    "        '''\n",
    "        return self.is_fork(self.get_agent_cell(handle))\n",
    "\n",
    "    def is_at_before_join(self, handle):\n",
    "        '''\n",
    "        Returns True iff the agent is before a join\n",
    "        '''\n",
    "        return self.is_before_join(self.get_agent_cell(handle))\n",
    "\n",
    "    def remaining_agents_handles(self):\n",
    "        '''\n",
    "        Return the number of remaining agents in the rail,\n",
    "        considering the ones that already reached their target\n",
    "        '''\n",
    "        return {\n",
    "            agent for agent in range(len(self.agents))\n",
    "            if not self.is_done(agent)\n",
    "        }\n",
    "\n",
    "    def remaining_agents(self):\n",
    "        '''\n",
    "        Return the number of remaining agents in the rail,\n",
    "        considering the ones that already reached their target\n",
    "        '''\n",
    "        return len(self.remaining_agents_handles())\n",
    "\n",
    "    def only_one_agent(self):\n",
    "        '''\n",
    "        Returns True iff only one agent remains in the railway\n",
    "        '''\n",
    "        return self.remaining_agents() < 2\n",
    "\n",
    "    def is_real_decision(self, handle):\n",
    "        '''\n",
    "        Returns True iff the agent has to make a decision\n",
    "        '''\n",
    "        return self.is_at_fork(handle) or (\n",
    "            self.is_at_before_join(handle) and not self.only_one_agent()\n",
    "        )\n",
    "\n",
    "    def get_actions(self, position):\n",
    "        '''\n",
    "        Return all the possible active actions that can be performed from a given position\n",
    "        '''\n",
    "        successors = self.get_successors(position, unpacked=True)\n",
    "        actions = []\n",
    "        for succ in successors:\n",
    "            actions.append(\n",
    "                self._unpacked_graph.get_edge_data(\n",
    "                    position, succ)['action'].value\n",
    "            )\n",
    "        return actions\n",
    "\n",
    "    def get_agent_actions(self, handle):\n",
    "        '''\n",
    "        Return all the possible active actions that an agent can perform\n",
    "        '''\n",
    "        return self.get_actions(self.get_agent_cell(handle))\n",
    "\n",
    "    def action_from_positions(self, source, dest, unpacked=True):\n",
    "        '''\n",
    "        Return the action that an agent has to make to transition\n",
    "        from the `source` node to the `dest` node\n",
    "        '''\n",
    "        graph = self._unpacked_graph if unpacked else self.graph\n",
    "        if (source, dest) in graph.edges:\n",
    "            return graph.get_edge_data(source, dest)['action']\n",
    "        return None\n",
    "\n",
    "    def position_by_action(self, position, action):\n",
    "        '''\n",
    "        Return the next node if the given action will be performed in the given position\n",
    "        '''\n",
    "        successors = self.get_successors(position, unpacked=True)\n",
    "        for succ in successors:\n",
    "            if self._unpacked_graph.get_edge_data(position, succ)['action'] == action:\n",
    "                return succ\n",
    "        return None\n",
    "\n",
    "    def agent_position_by_action(self, handle, action):\n",
    "        '''\n",
    "        Return the next node that the agent will occupy if it performs the given action\n",
    "        '''\n",
    "        self.position_by_action(self.get_agent_cell(handle), action)\n",
    "\n",
    "    def shortest_paths(self, handle):\n",
    "        '''\n",
    "        Compute the shortest paths from the current position and direction,\n",
    "        to the target of the agent identified by the given handle,\n",
    "        considering every possibile target arrival direction.\n",
    "        The shortest paths are then ordered by increasing lenght\n",
    "        '''\n",
    "        \n",
    "        agent = self.agents[handle]\n",
    "        position = self.get_agent_cell(handle)\n",
    "        source, weight = self.next_node(position)\n",
    "        targets = self.get_nodes(agent.target)\n",
    "        paths = []\n",
    "        for target in targets:\n",
    "            try:\n",
    "                lenght, path = nx.bidirectional_dijkstra(\n",
    "                    self.graph, source, target\n",
    "                )\n",
    "                if position != path[0]:\n",
    "                    path = [position] + path\n",
    "                    lenght += weight\n",
    "                paths.append((lenght, path))\n",
    "            except nx.NetworkXNoPath:\n",
    "                continue\n",
    "        if not paths:\n",
    "            return []\n",
    "        return sorted(paths, key=lambda x: x[0])\n",
    "\n",
    "    def deviation_paths(self, handle, source, node_to_avoid):\n",
    "        '''\n",
    "        Return alternative paths from `source` to the agent's target,\n",
    "        without considering the actual shortest path\n",
    "        '''\n",
    "        agent = self.agents[handle]\n",
    "        targets = self.get_nodes(agent.target)\n",
    "        paths = []\n",
    "        for succ in self.graph.successors(source):\n",
    "            if succ != node_to_avoid:\n",
    "                edge = self.graph.edges[(source, succ)]\n",
    "                weight = edge['weight']\n",
    "                for target in targets:\n",
    "                    try:\n",
    "                        lenght, path = nx.bidirectional_dijkstra(\n",
    "                            self.graph, succ, target\n",
    "                        )\n",
    "                        path = [source] + path\n",
    "                        lenght += weight\n",
    "                        paths.append((lenght, path))\n",
    "                    except nx.NetworkXNoPath:\n",
    "                        continue\n",
    "        if len(paths) == 0:\n",
    "            return []\n",
    "        return sorted(paths, key=lambda x: x[0])\n",
    "\n",
    "    def meaningful_subgraph(self, handle):\n",
    "        '''\n",
    "        Return the subgraph which could be visited by the agent\n",
    "        identified by the given handle\n",
    "        '''\n",
    "        nodes = {}\n",
    "        source, _ = self.next_node(self.get_agent_cell(handle))\n",
    "        for path in nx.all_simple_paths(self.graph, source, self.agents[handle].target):\n",
    "            nodes.update(path)\n",
    "        return nx.subgraph(self.graph, nodes)\n",
    "\n",
    "    def get_agents_distance(self, handle_one, handle_two):\n",
    "        '''\n",
    "        Return the minimum distance between the given agents\n",
    "        '''\n",
    "        pos_one = self.get_agent_cell(handle_one)\n",
    "        pos_two = self.get_agent_cell(handle_two)\n",
    "        if pos_one is None or pos_two is None:\n",
    "            return None\n",
    "        node_one, weight_one = self.next_node(pos_one)\n",
    "        node_two, weight_two = self.next_node(pos_two)\n",
    "        try:\n",
    "            distance = nx.dijkstra_path_length(\n",
    "                self.graph, node_one, node_two\n",
    "            )\n",
    "            return distance + weight_one + weight_two\n",
    "        except nx.NetworkXNoPath:\n",
    "            return None\n",
    "\n",
    "    def get_distance(self, source, dest):\n",
    "        '''\n",
    "        Return the minimum distance between the source\n",
    "        and destination nodes\n",
    "        '''\n",
    "        if (source not in self._unpacked_graph.nodes or\n",
    "                dest not in self._unpacked_graph.nodes):\n",
    "            return np.inf\n",
    "        return nx.dijkstra_path_length(\n",
    "            self._unpacked_graph, source, dest\n",
    "        )\n",
    "\n",
    "    def get_adjacency_matrix(self, unpacked=False):\n",
    "        '''\n",
    "        Return the adjacency matrix of the specified graph,\n",
    "        as a SciPy sparse COO matrix\n",
    "        '''\n",
    "        graph = self.graph if not unpacked else self._unpacked_graph\n",
    "        return graph.to_scipy_sparse_matrix(\n",
    "            dtype=np.dtype('long'), weight='weight', format='coo'\n",
    "        )\n",
    "\n",
    "    def get_graph_edges(self, unpacked=False, data=False):\n",
    "        '''\n",
    "        Return edges and associated features of the specified graph\n",
    "        '''\n",
    "        graph = self.graph if not unpacked else self._unpacked_graph\n",
    "        return graph.edges(data=data)\n",
    "\n",
    "    def get_graph_nodes(self, unpacked=False, data=False):\n",
    "        '''\n",
    "        Return nodes and associated features of the specified graph\n",
    "        '''\n",
    "        graph = self.graph if not unpacked else self._unpacked_graph\n",
    "        return graph.nodes(data=data)\n",
    "\n",
    "    def edges_from_path(self, path):\n",
    "        '''\n",
    "        Given a path in the packed graph as a sequence of nodes,\n",
    "        return the corresponding sequence of edges\n",
    "        '''\n",
    "        edges = []\n",
    "        starting_index = 0\n",
    "        if path[0] not in self.graph.nodes:\n",
    "            fake_weight, mini_path = nx.bidirectional_dijkstra(\n",
    "                self._unpacked_graph, path[0], path[1]\n",
    "            )\n",
    "            edges.append((\n",
    "                path[0], path[1],\n",
    "                {\n",
    "                    'weight': fake_weight,\n",
    "                    'action': self._unpacked_graph.get_edge_data(mini_path[0], mini_path[1])['action'],\n",
    "                    'choice': RailEnvChoices.CHOICE_LEFT\n",
    "                }\n",
    "            ))\n",
    "            starting_index = 1\n",
    "        for i in range(starting_index, len(path) - 1):\n",
    "            if path[i] != path[i + 1]:\n",
    "                edge = (path[i], path[i + 1])\n",
    "                edge_attributes = self.graph.get_edge_data(*edge)\n",
    "                edges.append((*edge, edge_attributes))\n",
    "        return edges\n",
    "\n",
    "    def positions_from_path(self, path, max_lenght=None):\n",
    "        '''\n",
    "        Given a path in the packed graph, return the corresponding\n",
    "        path in the unpacked graph, without the direction component\n",
    "        '''\n",
    "        positions = [path[0]]\n",
    "        for i in range(0, len(path) - 1):\n",
    "            _, mini_path = nx.bidirectional_dijkstra(\n",
    "                self._unpacked_graph, path[i], path[i + 1]\n",
    "            )\n",
    "            positions.extend(mini_path[1:])\n",
    "            if max_lenght is not None and len(positions) >= max_lenght:\n",
    "                return positions[:max_lenght]\n",
    "        return positions\n",
    "\n",
    "    def different_direction_nodes(self, node):\n",
    "        '''\n",
    "        Given a node, described by row, column and direction,\n",
    "        return every other node in the packed graph with\n",
    "        a different direction component\n",
    "        '''\n",
    "        nodes = []\n",
    "        row, col, direction = node\n",
    "        for new_direction in range(len(TRANS)):\n",
    "            new_node = (row, col, new_direction)\n",
    "            if new_node != node and new_node in self.graph:\n",
    "                nodes.append(new_node)\n",
    "        return nodes\n",
    "\n",
    "    def no_successors_nodes(self, unpacked=False):\n",
    "        '''\n",
    "        Return a list of nodes that have no successors in the graph\n",
    "        '''\n",
    "        graph = self._unpacked_graph if unpacked else self.graph\n",
    "        no_succ = []\n",
    "        for node in graph.nodes:\n",
    "            succ = self.get_successors(node, unpacked=unpacked)\n",
    "            if len(succ) == 0:\n",
    "                no_succ.append(node)\n",
    "        return no_succ\n",
    "\n",
    "    def draw_graph(self):\n",
    "        '''\n",
    "        Show the packed graph, with labels on nodes\n",
    "        '''\n",
    "        nx.draw(self.graph, with_labels=True)\n",
    "        plt.show()\n",
    "\n",
    "    def draw_unpacked_graph(self):\n",
    "        '''\n",
    "        Show the unpacked graph, with labels on nodes\n",
    "        '''\n",
    "        nx.draw(self._unpacked_graph, with_labels=True)\n",
    "        plt.show()\n",
    "\n",
    "    def draw_path(self, path):\n",
    "        '''\n",
    "        Show a path in the packed graph, where edges belonging\n",
    "        to the path are colored in red\n",
    "        '''\n",
    "        if path[0] not in self.graph.nodes:\n",
    "            path = path[1:]\n",
    "        pos = nx.spring_layout(self.graph)\n",
    "        nx.draw(self.graph, pos)\n",
    "        path_edges = list(zip(path, path[1:]))\n",
    "        nx.draw_networkx_nodes(self.graph, pos, nodelist=path, node_color='r')\n",
    "        nx.draw_networkx_edges(\n",
    "            self.graph, pos, edgelist=path_edges, edge_color='r', width=5\n",
    "        )\n",
    "        plt.axis('equal')\n",
    "        plt.show()\n",
    "        \n",
    "## Binary tree observation ##\n",
    "\n",
    "BT_LOWER, BT_UPPER = -1, 1\n",
    "BT_UNDER, BT_OVER = -2, 2\n",
    "\n",
    "\n",
    "def dumb_normalize_binary_tree_obs(observation):\n",
    "    '''\n",
    "    Substitute infinite values with a lower bound (e.g. -1),\n",
    "    but avoid scaling observations\n",
    "    '''\n",
    "    normalized_observation = observation.copy()\n",
    "    normalized_observation[normalized_observation == -np.inf] = BT_LOWER\n",
    "    normalized_observation[normalized_observation == np.inf] = BT_LOWER\n",
    "    return normalized_observation\n",
    "\n",
    "\n",
    "def normalize_binary_tree_obs(observation, remaining_agents, max_malfunction, fixed_radius):\n",
    "    '''\n",
    "    Normalize the given observations by performing min-max scaling\n",
    "    over individual features\n",
    "    '''\n",
    "    normalized_observation = observation.copy()\n",
    "    num_agents = normalized_observation[:, :, 0:4]\n",
    "    agent_distances = normalized_observation[:, :, 4:6]\n",
    "    malfunctions = normalized_observation[:, :, 6:8]\n",
    "    target_distances = normalized_observation[:, :, 8]\n",
    "    turns_to_node = normalized_observation[:, :, 9]\n",
    "    c_nodes = normalized_observation[:, :, 10]\n",
    "    deadlocks = normalized_observation[:, :, 11]\n",
    "    deadlock_distances = normalized_observation[:, :, 12]\n",
    "    are_forks = normalized_observation[:, :, 13]\n",
    "    stop_actions = normalized_observation[:, :, 14]\n",
    "\n",
    "    # Normalize number of agents in path\n",
    "    num_agents = min_max_scaling(\n",
    "        num_agents, BT_LOWER, BT_UPPER, BT_LOWER, BT_UPPER,\n",
    "        known_min=0, known_max=remaining_agents\n",
    "    )\n",
    "\n",
    "    # Normalize malfunctions\n",
    "    malfunctions = min_max_scaling(\n",
    "        malfunctions, BT_LOWER, BT_UPPER, BT_LOWER, BT_UPPER,\n",
    "        known_min=0, known_max=max_malfunction\n",
    "    )\n",
    "\n",
    "    # Normalize common nodes\n",
    "    c_nodes = min_max_scaling(\n",
    "        c_nodes, BT_LOWER, BT_UPPER, BT_LOWER, BT_UPPER,\n",
    "        known_min=0, known_max=remaining_agents\n",
    "    )\n",
    "\n",
    "    # Normalize deadlocks\n",
    "    deadlocks = min_max_scaling(\n",
    "        deadlocks, BT_LOWER, BT_UPPER, BT_LOWER, BT_UPPER,\n",
    "        known_min=0, known_max=remaining_agents\n",
    "    )\n",
    "\n",
    "    # Normalize distances\n",
    "    agent_distances = min_max_scaling(\n",
    "        agent_distances, BT_LOWER, BT_UPPER, BT_LOWER, BT_UPPER,\n",
    "        known_min=-fixed_radius, known_max=fixed_radius\n",
    "    )\n",
    "    target_distances = min_max_scaling(\n",
    "        target_distances, BT_LOWER, BT_UPPER, BT_LOWER, BT_UPPER,\n",
    "        known_min=0, known_max=fixed_radius\n",
    "    )\n",
    "    turns_to_node = min_max_scaling(\n",
    "        turns_to_node, BT_LOWER, BT_UPPER, BT_LOWER, BT_UPPER,\n",
    "        known_min=0, known_max=fixed_radius\n",
    "    )\n",
    "    deadlock_distances = min_max_scaling(\n",
    "        deadlock_distances, BT_LOWER, BT_UPPER, BT_LOWER, BT_UPPER,\n",
    "        known_min=-fixed_radius, known_max=fixed_radius\n",
    "    )\n",
    "\n",
    "    # Normalize stop actions\n",
    "    stop_actions = min_max_scaling(\n",
    "        stop_actions, BT_LOWER, BT_UPPER, BT_LOWER, BT_UPPER,\n",
    "        known_min=0, known_max=fixed_radius\n",
    "    )\n",
    "\n",
    "    # Build the normalized observation\n",
    "    normalized_observation[:, :, 0:4] = num_agents\n",
    "    normalized_observation[:, :, 4:6] = agent_distances\n",
    "    normalized_observation[:, :, 6:8] = malfunctions\n",
    "    normalized_observation[:, :, 8] = target_distances\n",
    "    normalized_observation[:, :, 9] = turns_to_node\n",
    "    normalized_observation[:, :, 10] = c_nodes\n",
    "    normalized_observation[:, :, 11] = deadlocks\n",
    "    normalized_observation[:, :, 12] = deadlock_distances\n",
    "    normalized_observation[:, :, 13] = are_forks\n",
    "    normalized_observation[:, :, 14] = stop_actions\n",
    "\n",
    "    # Sanity check\n",
    "    normalized_observation[normalized_observation == -np.inf] = BT_LOWER\n",
    "    normalized_observation[normalized_observation == np.inf] = BT_UPPER\n",
    "    normalized_observation = np.clip(\n",
    "        normalized_observation, BT_LOWER, BT_UPPER\n",
    "    )\n",
    "\n",
    "    # Check if the output is in range [BT_LOWER, BT_UPPER]\n",
    "    assert np.logical_and(\n",
    "        normalized_observation >= BT_LOWER,\n",
    "        normalized_observation <= BT_UPPER\n",
    "    ).all(), (observation, normalized_observation)\n",
    "\n",
    "    return normalized_observation\n",
    "\n",
    "\n",
    "## Tree observation \n",
    "\n",
    "T_CLIP_MIN, T_CLIP_MAX = -1, 1\n",
    "\n",
    "\n",
    "def max_lt(seq, val):\n",
    "    '''\n",
    "    Return greatest item in seq for which item < val applies.\n",
    "    None is returned if seq was empty or all items in seq were >= val.\n",
    "    '''\n",
    "    max = 0\n",
    "    idx = len(seq) - 1\n",
    "    while idx >= 0:\n",
    "        if seq[idx] < val and seq[idx] >= 0 and seq[idx] > max:\n",
    "            max = seq[idx]\n",
    "        idx -= 1\n",
    "    return max\n",
    "\n",
    "\n",
    "def min_gt(seq, val):\n",
    "    '''\n",
    "    Return smallest item in seq for which item > val applies.\n",
    "    None is returned if seq was empty or all items in seq were >= val.\n",
    "    '''\n",
    "    min = np.inf\n",
    "    idx = len(seq) - 1\n",
    "    while idx >= 0:\n",
    "        if seq[idx] >= val and seq[idx] < min:\n",
    "            min = seq[idx]\n",
    "        idx -= 1\n",
    "    return min\n",
    "\n",
    "\n",
    "def norm_obs_clip(obs, clip_min=-1, clip_max=1, fixed_radius=0, normalize_to_range=False):\n",
    "    '''\n",
    "    This function returns the difference between min and max value of an observation\n",
    "    '''\n",
    "    if fixed_radius > 0:\n",
    "        max_obs = fixed_radius\n",
    "    else:\n",
    "        max_obs = max(1, max_lt(obs, 1000)) + 1\n",
    "\n",
    "    min_obs = 0\n",
    "    if normalize_to_range:\n",
    "        min_obs = min_gt(obs, 0)\n",
    "    if min_obs > max_obs:\n",
    "        min_obs = max_obs\n",
    "    if max_obs == min_obs:\n",
    "        return np.clip(np.array(obs) / max_obs, clip_min, clip_max)\n",
    "    norm = np.abs(max_obs - min_obs)\n",
    "    return np.clip((np.array(obs) - min_obs) / norm, clip_min, clip_max)\n",
    "\n",
    "\n",
    "def _split_node_into_feature_groups(node):\n",
    "    '''\n",
    "    This function separates features of the given node into logical groups\n",
    "    '''\n",
    "    # Data features\n",
    "    data = np.zeros(6)\n",
    "    data[0] = node.dist_own_target_encountered\n",
    "    data[1] = node.dist_other_target_encountered\n",
    "    data[2] = node.dist_other_agent_encountered\n",
    "    data[3] = node.dist_potential_conflict\n",
    "    data[4] = node.dist_unusable_switch\n",
    "    data[5] = node.dist_to_next_branch\n",
    "\n",
    "    # Distance features\n",
    "    distance = np.zeros(1)\n",
    "    distance[0] = node.dist_min_to_target\n",
    "\n",
    "    # Agent data features\n",
    "    agent_data = np.zeros(4)\n",
    "    agent_data[0] = node.num_agents_same_direction\n",
    "    agent_data[1] = node.num_agents_opposite_direction\n",
    "    agent_data[2] = node.num_agents_malfunctioning\n",
    "    agent_data[3] = node.speed_min_fractional\n",
    "\n",
    "    return data, distance, agent_data\n",
    "\n",
    "\n",
    "def _split_subtree_into_feature_groups(node, current_tree_depth, max_tree_depth):\n",
    "    '''\n",
    "    This function recursively extracts information starting from the given node\n",
    "    '''\n",
    "    if node == -np.inf:\n",
    "        remaining_depth = max_tree_depth - current_tree_depth\n",
    "        num_remaining_nodes = int((4 ** (remaining_depth + 1) - 1) / (4 - 1))\n",
    "        return (\n",
    "            [-np.inf] * num_remaining_nodes * 6,\n",
    "            [-np.inf] * num_remaining_nodes,\n",
    "            [-np.inf] * num_remaining_nodes * 4\n",
    "        )\n",
    "\n",
    "    data, distance, agent_data = _split_node_into_feature_groups(node)\n",
    "    if not node.childs:\n",
    "        return data, distance, agent_data\n",
    "\n",
    "    for direction in TreeObsForRailEnv.tree_explored_actions_char:\n",
    "        sub_data, sub_distance, sub_agent_data = _split_subtree_into_feature_groups(\n",
    "            node.childs[direction], current_tree_depth + 1, max_tree_depth\n",
    "        )\n",
    "        data = np.concatenate((data, sub_data))\n",
    "        distance = np.concatenate((distance, sub_distance))\n",
    "        agent_data = np.concatenate((agent_data, sub_agent_data))\n",
    "\n",
    "    return data, distance, agent_data\n",
    "\n",
    "\n",
    "def split_tree_into_feature_groups(tree, max_tree_depth):\n",
    "    '''\n",
    "    This function splits the tree into three difference arrays of values\n",
    "    '''\n",
    "    data, distance, agent_data = _split_node_into_feature_groups(tree)\n",
    "\n",
    "    for direction in TreeObsForRailEnv.tree_explored_actions_char:\n",
    "        sub_data, sub_distance, sub_agent_data = _split_subtree_into_feature_groups(\n",
    "            tree.childs[direction], 1, max_tree_depth\n",
    "        )\n",
    "        data = np.concatenate((data, sub_data))\n",
    "        distance = np.concatenate((distance, sub_distance))\n",
    "        agent_data = np.concatenate((agent_data, sub_agent_data))\n",
    "\n",
    "    return data, distance, agent_data\n",
    "\n",
    "\n",
    "def normalize_tree_obs(observation, tree_depth, radius):\n",
    "    '''\n",
    "    This function normalizes the observation used by the RL algorithm\n",
    "    '''\n",
    "    data, distance, agent_data = split_tree_into_feature_groups(\n",
    "        observation, tree_depth\n",
    "    )\n",
    "\n",
    "    data = norm_obs_clip(\n",
    "        data, clip_min=T_CLIP_MIN, clip_max=T_CLIP_MAX,\n",
    "        fixed_radius=radius\n",
    "    )\n",
    "    distance = norm_obs_clip(\n",
    "        distance, clip_min=T_CLIP_MIN, clip_max=T_CLIP_MAX,\n",
    "        normalize_to_range=True\n",
    "    )\n",
    "    agent_data = np.clip(agent_data, T_CLIP_MIN, T_CLIP_MAX)\n",
    "    normalized_obs = np.concatenate(\n",
    "        (np.concatenate((data, distance)), agent_data)\n",
    "    )\n",
    "    return normalized_obs\n",
    "\n",
    "## Custom binary tree observation\n",
    "\n",
    "'''\n",
    "Observation:\n",
    "    - Structure:\n",
    "        * Tensor of shape (1 + max_deviations, max_depth, features), where max_depth\n",
    "          is the maximum number of nodes in the packed graph to consider and\n",
    "          features is the total amount of features for each node\n",
    "        * The feature matrix contains the features of the nodes in the shortest path\n",
    "          as the first row and the features of the nodes in the deviation paths\n",
    "          (which are exactly max_depth - 1) as the following rows\n",
    "        * The feature matrix is then shaped like a binary tree, where branches identify\n",
    "          choices, i.e. CHOICE_LEFT or CHOICE_RIGHT\n",
    "    - Features:\n",
    "        1. Number of agents (going in my direction) identified in the subpath\n",
    "           from the root up to each node in the path\n",
    "        2. Number of agents (going in a direction different from mine) identified\n",
    "           in the subpath from the root up to each node in the path\n",
    "        3. Number of malfunctioning agents (going in my direction) identified in the subpath\n",
    "           from the root up to each node in the path\n",
    "        4. Number of malfunctioning agents (going in a direction different from mine) identified\n",
    "           in the subpath from the root up to each node in the path\n",
    "        5. Minimum distances from an agent to other agents (going in my direction)\n",
    "           in each edge of the path\n",
    "        6. Minimum distances from an agent to other agents (going in a direction\n",
    "           different than mine) in each edge of the path\n",
    "        7. Maximum number of malfunctioning turns of other agents (going in my direction),\n",
    "           in each edge of the path\n",
    "        8. Maximum number of malfunctioning turns of other agents (going in a direction\n",
    "           different from mine), in each edge of the path\n",
    "        9. Distances from the target, from each node in the path\n",
    "        10. Path weights in turns to reach the given node from the root one\n",
    "        11. Number of agents using the node to reach their target in the shortest path\n",
    "        12. Number of agents in deadlock in the previous path, assuming that all the\n",
    "            other agents follow their shortest path\n",
    "        13. How many turns before a possible deadlock\n",
    "        14. If the node is a fork or not\n",
    "        15. How many turns I've been repeatedly selecting the stop action\n",
    "'''\n",
    "\n",
    "# SpeedData:\n",
    "# - `times` represents the total number of turns required for an agent to complete a cell\n",
    "# - `remaining` represents the remaining number of steps required for an agent to complete the current cell\n",
    "SpeedData = namedtuple('SpeedData', ['times', 'remaining'])\n",
    "\n",
    "# Node:\n",
    "# - `position` represents the position of the node in the railway\n",
    "# - `features` represents the features associated to a node\n",
    "# - `left` represents its left child\n",
    "# - `right` represents its right child\n",
    "Node = namedtuple('Node', ['position', 'features', 'left', 'right'])\n",
    "\n",
    "\n",
    "class BinaryTreeObservator(ObservationBuilder):\n",
    "\n",
    "    def __init__(self, max_depth, predictor):\n",
    "        super().__init__()\n",
    "        self.max_depth = max_depth\n",
    "        self.predictor = predictor\n",
    "        self.observations = dict()\n",
    "        self.observation_dim = 15\n",
    "\n",
    "    def _init_agents(self):\n",
    "        '''\n",
    "        Store agent-related info:\n",
    "        - `speed_data`: a SpeedData object for each agent\n",
    "        - `agent_handles`: set of agent handles\n",
    "        - `other_agents`: list of other agent's handles for each agent\n",
    "        - `last_nodes`: list of last visited nodes for each agent\n",
    "          (along with corresponding weights)\n",
    "        '''\n",
    "        self.agent_handles = set(self.env.get_agent_handles())\n",
    "        self.other_agents = dict()\n",
    "        self.speed_data = dict()\n",
    "        self.last_nodes = []\n",
    "        for handle, agent in enumerate(self.env.agents):\n",
    "            times_per_cell = int(np.reciprocal(agent.speed_data[\"speed\"]))\n",
    "            self.speed_data[handle] = SpeedData(\n",
    "                times=times_per_cell, remaining=0\n",
    "            )\n",
    "            self.other_agents[handle] = self.agent_handles - {handle}\n",
    "            agent_position = self.env.railway_encoding.get_agent_cell(handle)\n",
    "            prev_node, prev_weight = self.env.railway_encoding.previous_node(\n",
    "                agent_position\n",
    "            )\n",
    "            self.last_nodes.append(\n",
    "                (prev_node, prev_weight * times_per_cell)\n",
    "            )\n",
    "\n",
    "    def reset(self):\n",
    "        self._init_agents()\n",
    "        if self.predictor is not None:\n",
    "            self.predictor.reset()\n",
    "\n",
    "    def set_env(self, env):\n",
    "        super().set_env(env)\n",
    "        if self.predictor:\n",
    "            self.predictor.set_env(self.env)\n",
    "\n",
    "    def _update_shortest(self, handle, prediction):\n",
    "        '''\n",
    "        Store shortest paths, shortest positions and shortest cumulative weights\n",
    "        for the current observation of the given agent\n",
    "        '''\n",
    "        # Update speed data\n",
    "        remaining_turns_in_cell = 0\n",
    "        if self.env.agents[handle].speed_data[\"speed\"] < 1.0:\n",
    "            remaining_turns_in_cell = int(\n",
    "                (1 - np.clip(self.env.agents[handle].speed_data[\"position_fraction\"], 0.0, 1.0)) /\n",
    "                self.env.agents[handle].speed_data[\"speed\"]\n",
    "            )\n",
    "        self.speed_data[handle] = SpeedData(\n",
    "            times=self.speed_data[handle].times,\n",
    "            remaining=remaining_turns_in_cell\n",
    "        )\n",
    "\n",
    "        # Update shortest paths\n",
    "        shortest_path = np.array(prediction.path, np.dtype('int, int, int'))\n",
    "        self._shortest_paths[handle, :shortest_path.shape[0]] = shortest_path\n",
    "\n",
    "        # Update shortest positions\n",
    "        shortest_positions = np.array(\n",
    "            [node[:-1] for node in prediction.path], np.dtype('int, int')\n",
    "        )\n",
    "        self._shortest_positions[handle, :shortest_positions.shape[0]] = (\n",
    "            shortest_positions\n",
    "        )\n",
    "\n",
    "        # Update shortest cumulative weights\n",
    "        self._shortest_cum_weights[handle] = self.compute_cumulative_weights(\n",
    "            handle, prediction.lenght, prediction.edges, remaining_turns_in_cell\n",
    "        )\n",
    "\n",
    "        # Update last visited node and last positions\n",
    "        prev_node, prev_weight = self.env.railway_encoding.previous_node(\n",
    "            prediction.path[0]\n",
    "        )\n",
    "        self.last_nodes[handle] = (\n",
    "            prev_node, prev_weight*self.speed_data[handle].times)\n",
    "\n",
    "    def get_many(self, handles=None):\n",
    "        self.predictions = self.predictor.get_many()\n",
    "        self._shortest_paths = np.full(\n",
    "            (len(self.agent_handles), self.max_depth),\n",
    "            -1, np.dtype('int, int, int')\n",
    "        )\n",
    "        self._shortest_positions = np.full(\n",
    "            (len(self.agent_handles), self.max_depth), -1, np.dtype('int, int')\n",
    "        )\n",
    "        self._shortest_cum_weights = np.zeros(\n",
    "            (len(self.agent_handles), self.max_depth)\n",
    "        )\n",
    "        for handle, prediction in self.predictions.items():\n",
    "            # Check if agent is not at target\n",
    "            if self.predictions[handle] is not None:\n",
    "                shortest_prediction = prediction[0]\n",
    "                self._update_shortest(handle, shortest_prediction)\n",
    "\n",
    "        return super().get_many(handles)\n",
    "\n",
    "    def get(self, handle=0):\n",
    "        dim = sum(2 ** i for i in range(self.max_depth)) * self.observation_dim\n",
    "        self.observations[handle] = np.full(dim, BT_UNDER)\n",
    "        features = np.full(\n",
    "            (\n",
    "                1 + self.predictor.max_deviations,\n",
    "                self.max_depth, self.observation_dim\n",
    "            ), -np.inf\n",
    "        )\n",
    "\n",
    "        # Compute features if necessary\n",
    "        if (self.predictions[handle] is not None and (\n",
    "                self.env.railway_encoding.is_real_decision(handle) or\n",
    "                self.env.agents[handle].status == TrainState.READY_TO_DEPART)):\n",
    "            shortest_path_prediction, deviation_paths_prediction = self.predictions[handle]\n",
    "            packed_positions, packed_weights = self._get_shortest_packed_positions()\n",
    "            shortest_feats = self._fill_path_values(\n",
    "                handle, shortest_path_prediction, packed_positions, packed_weights\n",
    "            )\n",
    "            prev_num_agents = shortest_feats[:, :4]\n",
    "            features[0, :, :] = shortest_feats\n",
    "\n",
    "            # Compute deviation paths features\n",
    "            for i, deviation_prediction in enumerate(deviation_paths_prediction):\n",
    "                prev_deadlocks = 0\n",
    "                prev_num_agents_values = None\n",
    "                if i >= 1:\n",
    "                    prev_deadlocks = shortest_feats[i - 1, 11]\n",
    "                    prev_num_agents_values = prev_num_agents[i - 1, :]\n",
    "                dev_feats = self._fill_path_values(\n",
    "                    handle, deviation_prediction, packed_positions, packed_weights,\n",
    "                    turns_to_deviation=self._shortest_cum_weights[handle, i],\n",
    "                    prev_deadlocks=prev_deadlocks, prev_num_agents=prev_num_agents_values,\n",
    "                    deviation=True\n",
    "                )\n",
    "                features[i + 1, :, :] = dev_feats\n",
    "\n",
    "            # Normalize features\n",
    "            features = normalize_binary_tree_obs(\n",
    "                features,\n",
    "                self.env.railway_encoding.remaining_agents(),\n",
    "                self.env.malfunction_generator.get_process_data().max_duration,\n",
    "                self.env.params.observator.binary_tree.radius\n",
    "            )\n",
    "\n",
    "            # Build the binary tree\n",
    "            binary_tree = self.get_agent_binary_tree(\n",
    "                handle, self.predictions[handle], features\n",
    "            )\n",
    "\n",
    "            # Linearize the binary tree and store it as an observation\n",
    "            self.observations[handle] = self.concat_nodes(binary_tree)\n",
    "\n",
    "        return self.observations[handle]\n",
    "\n",
    "    def _fill_path_values(self, handle, prediction, packed_positions, packed_weights,\n",
    "                          turns_to_deviation=0, prev_deadlocks=0, prev_num_agents=None, deviation=False):\n",
    "        '''\n",
    "        Compute observations for the given prediction and return\n",
    "        a suitable feature matrix\n",
    "        '''\n",
    "        # Adjust weights and positions based on which kind of path\n",
    "        # we are analyzing (shortest or deviation)\n",
    "        path_weights = self._shortest_cum_weights[handle]\n",
    "        path = prediction.path\n",
    "        positions = [node[:-1] for node in path]\n",
    "        if deviation == False:\n",
    "            positions = packed_positions[handle].tolist()[:len(path)]\n",
    "            positions_weights = packed_weights[handle]\n",
    "        else:\n",
    "            path_weights = np.array(\n",
    "                self.compute_cumulative_weights(\n",
    "                    handle, prediction.lenght, prediction.edges, turns_to_deviation\n",
    "                )\n",
    "            )\n",
    "            positions_weights = path_weights\n",
    "\n",
    "        # Compute features\n",
    "        num_agents, agent_distances, malfunctions = self.agents_in_path(\n",
    "            handle, path, path_weights, prev_num_agents=prev_num_agents\n",
    "        )\n",
    "        target_distances = self.distance_from_target(\n",
    "            handle, prediction.lenght, path, path_weights, turns_to_deviation\n",
    "        )\n",
    "        c_nodes = self.common_nodes(handle, positions)\n",
    "        deadlocks, deadlock_distances = self.find_deadlocks(\n",
    "            handle, positions, positions_weights, packed_positions, packed_weights,\n",
    "            prev_deadlocks=prev_deadlocks\n",
    "        )\n",
    "        are_forks = self.compute_is_fork(path)\n",
    "        stop_actions = np.full(are_forks.shape, self.env.stop_actions[handle])\n",
    "\n",
    "        # Build the feature matrix\n",
    "        feature_matrix = np.vstack([\n",
    "            num_agents, agent_distances, malfunctions,\n",
    "            target_distances, path_weights, c_nodes, deadlocks, deadlock_distances,\n",
    "            are_forks, stop_actions\n",
    "        ]).T\n",
    "\n",
    "        return feature_matrix\n",
    "\n",
    "    def get_binary_tree(self, position, depth, prediction, features, choices=[]):\n",
    "        '''\n",
    "        Recursive function that build a binary tree starting from the given position\n",
    "        and adds the correct feature set to each node\n",
    "        '''\n",
    "        if depth == 0:\n",
    "            return None\n",
    "        children = {\"left\": None, \"right\": None}\n",
    "        if self.env.railway_encoding.is_node(position, unpacked=False):\n",
    "            successors = self.env.railway_encoding.get_successors(\n",
    "                position, unpacked=False\n",
    "            )\n",
    "            for succ in successors:\n",
    "                choice = self.env.railway_encoding.get_edge_data(\n",
    "                    position, succ, 'choice', unpacked=False\n",
    "                )\n",
    "                if choice == RailEnvChoices.CHOICE_LEFT:\n",
    "                    children[\"left\"] = succ\n",
    "                elif choice == RailEnvChoices.CHOICE_RIGHT:\n",
    "                    children[\"right\"] = succ\n",
    "        return Node(\n",
    "            position=position,\n",
    "            features=self.get_node_features(\n",
    "                prediction, features, choices, depth\n",
    "            ),\n",
    "            left=self.get_binary_tree(\n",
    "                children[\"left\"], depth - 1, prediction, features,\n",
    "                choices=choices + [RailEnvChoices.CHOICE_LEFT]\n",
    "            ),\n",
    "            right=self.get_binary_tree(\n",
    "                children[\"right\"], depth - 1, prediction, features,\n",
    "                choices=choices + [RailEnvChoices.CHOICE_RIGHT]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def get_node_features(self, prediction, features, choices, depth):\n",
    "        '''\n",
    "        Logically traverse the binary tree based on the given sequence of choices\n",
    "        and extract the features at that level in the feature matrix\n",
    "        '''\n",
    "        sp_prediction, dp_predictions = prediction\n",
    "        pos = self.max_depth - depth\n",
    "\n",
    "        # Root node\n",
    "        if pos == 0:\n",
    "            return features[0, 0, :]\n",
    "\n",
    "        # Non-root node\n",
    "        sp_edges = [edge[2]['choice'] for edge in sp_prediction.edges[:pos]]\n",
    "        dp_edges = [[c for c in sp_edges[:i]] for i in range(len(sp_edges))]\n",
    "        for i, dp_prediction in enumerate(dp_predictions[:pos]):\n",
    "            for c in dp_prediction.edges[:pos - i]:\n",
    "                dp_edges[i].append(c[2]['choice'])\n",
    "\n",
    "        # Node on shortest path\n",
    "        if choices == sp_edges:\n",
    "            return features[0, pos, :]\n",
    "\n",
    "        # Node on deviation path\n",
    "        for i, dp in enumerate(dp_edges):\n",
    "            if choices == dp:\n",
    "                return features[i + 1, pos - i, :]\n",
    "\n",
    "        # Fallback to default filling values\n",
    "        return np.full(self.observation_dim, BT_LOWER)\n",
    "\n",
    "    def get_agent_binary_tree(self, handle, prediction, features):\n",
    "        '''\n",
    "        Build the observation binary tree for the given agent and fill\n",
    "        it with the given features\n",
    "        '''\n",
    "        position = self.env.railway_encoding.get_agent_cell(handle)\n",
    "        node, _ = self.env.railway_encoding.next_node(position)\n",
    "        return self.get_binary_tree(node, self.max_depth, prediction, features)\n",
    "\n",
    "    def concat_nodes(self, node):\n",
    "        '''\n",
    "        Linearize the given binary tree features in a single array\n",
    "        '''\n",
    "        if node is None:\n",
    "            return []\n",
    "        return np.concatenate((\n",
    "            node.features,\n",
    "            self.concat_nodes(node.left),\n",
    "            self.concat_nodes(node.right)\n",
    "        ))\n",
    "\n",
    "    def compute_is_fork(self, path):\n",
    "        '''\n",
    "        Given a path, returns for each node if it is a fork or not\n",
    "        '''\n",
    "        are_forks = np.full((self.max_depth,), -np.inf)\n",
    "        for ind, node in enumerate(path):\n",
    "            are_forks[ind] = self.env.railway_encoding.is_fork(node)\n",
    "        return are_forks\n",
    "\n",
    "    def compute_cumulative_weights(self, handle, lenght, edges, initial_distance):\n",
    "        '''\n",
    "        Given a list of edges, compute the cumulative sum of weights,\n",
    "        representing the number of turns the given agent must perform\n",
    "        to reach each node in the path\n",
    "        '''\n",
    "        np_weights = np.zeros((self.max_depth,))\n",
    "        if lenght == np.inf:\n",
    "            np_weights = np.full((self.max_depth,), np.inf)\n",
    "        weights = [initial_distance] + [\n",
    "            e[2]['weight'] * self.speed_data[handle].times for e in edges\n",
    "        ]\n",
    "        np_weights[:len(weights)] = np.cumsum(weights)\n",
    "        return np_weights\n",
    "\n",
    "    def agents_in_path(self, handle, path, cum_weights, prev_num_agents=None):\n",
    "        '''\n",
    "        Return three arrays:\n",
    "        - Number of agents identified in the subpath from the root up to\n",
    "          each node in the path (in both directions and both malfunctioning or not)\n",
    "        - Minimum distances from an agent to other agent's\n",
    "          in each edge of the path (in both directions)\n",
    "        - Maximum turns that an agent has to wait because it is malfunctioning,\n",
    "          in each edge of the path (in both directions)\n",
    "\n",
    "        The directions are considered as:\n",
    "        - Same direction, if two agents \"follow\" each other\n",
    "        - Other direction, otherwise\n",
    "        '''\n",
    "        num_agents = np.zeros((self.max_depth, 4))\n",
    "        if prev_num_agents is not None:\n",
    "            num_agents[:] = np.array(prev_num_agents)\n",
    "        distances = np.full((self.max_depth, 2), np.inf)\n",
    "        malfunctions = np.zeros((self.max_depth, 2))\n",
    "\n",
    "        # For each agent different than myself\n",
    "        for agent in self.other_agents[handle]:\n",
    "            position = self.env.railway_encoding.get_agent_cell(agent)\n",
    "            # Check if agent is not DONE_REMOVED (position would be None)\n",
    "            if position is not None:\n",
    "                # Take the other agent's next node in the packed graph\n",
    "                node, next_node_distance = self.env.railway_encoding.next_node(\n",
    "                    position\n",
    "                )\n",
    "                # Take every possible direction for the given node in the packed graph\n",
    "                nodes = self.env.railway_encoding.get_nodes((node[0], node[1]))\n",
    "                # Check the next nodes of the next node in order to see\n",
    "                # the other agent's entry direction\n",
    "                next_nodes = self.env.railway_encoding.get_successors(node)\n",
    "\n",
    "                # Check if one of the next nodes of the other agent are in my path\n",
    "                for other_node in nodes:\n",
    "                    index = get_index(path, other_node)\n",
    "                    if index is not None:\n",
    "                        # Initialize distances\n",
    "                        distance = cum_weights[index]\n",
    "                        if cum_weights[index] < self.speed_data[handle].times:\n",
    "                            distance = (\n",
    "                                self.speed_data[handle].remaining -\n",
    "                                self.speed_data[handle].times\n",
    "                            )\n",
    "                        turns_to_reach_other_agent = abs(\n",
    "                            (next_node_distance - (self.speed_data[agent].remaining / self.speed_data[agent].times)) *\n",
    "                            self.speed_data[handle].times\n",
    "                        )\n",
    "\n",
    "                        # Check if same direction or other direction\n",
    "                        different_node = other_node != node\n",
    "                        more_than_one_choice = len(next_nodes) > 1\n",
    "                        last_node_in_path = len(path) <= index + 1\n",
    "                        different_one_choice = (\n",
    "                            not last_node_in_path and\n",
    "                            len(next_nodes) > 0 and\n",
    "                            next_nodes[0] != path[index + 1]\n",
    "                        )\n",
    "                        if (different_node and (more_than_one_choice or last_node_in_path or different_one_choice)):\n",
    "                            direction = 1\n",
    "                        else:\n",
    "                            turns_to_reach_other_agent = -turns_to_reach_other_agent\n",
    "                            direction = 0\n",
    "\n",
    "                        # Update number of agents\n",
    "                        num_agents[index:len(path), direction] += 1\n",
    "\n",
    "                        # Update distances s.t. we always keep the greatest one (if distance is negative),\n",
    "                        # otherwise the minimum one (if distance is positive)\n",
    "                        distance += turns_to_reach_other_agent\n",
    "                        if ((distances[index, direction] == np.inf) or\n",
    "                            (distance >= 0 and distances[index, direction] > distance) or\n",
    "                                (distance <= 0 and distances[index, direction] < distance) or\n",
    "                                (distance >= 0 and distances[index, direction] < 0)):\n",
    "                            distances[index, direction] = distance\n",
    "\n",
    "                        # Update malfunctions\n",
    "                        malfunction = self.env.agents[agent].malfunction_data['malfunction']\n",
    "                        if malfunction > 0:\n",
    "                            num_agents[index:len(path), direction + 2] += 1\n",
    "                        if malfunctions[index, direction] < malfunction:\n",
    "                            malfunctions[index, direction] = malfunction\n",
    "                        break\n",
    "\n",
    "        return np.transpose(num_agents), np.transpose(distances), np.transpose(malfunctions)\n",
    "\n",
    "    def distance_from_target(self, handle, lenght, path, cum_weights, turns_to_deviation=0):\n",
    "        '''\n",
    "        For a shortest path:\n",
    "        - `lenght` should be the actual length of the shortest path\n",
    "        - `cum_weights` should be the cumulative number of turns to reach each node\n",
    "        - `turns_to_deviation` should be zero\n",
    "\n",
    "        For a deviation path:\n",
    "        - `lenght` should be the actual length of the deviation path\n",
    "        - `cum_weights` should be the cumulative number of turns to reach each node\n",
    "          (starting from the agent's position instead of the root of the deviation path)\n",
    "        - `turns_to_deviation` should be the number of turns required to reach the root\n",
    "           of the deviation path\n",
    "\n",
    "        Returns the actual distance from each node of the path to its target\n",
    "        '''\n",
    "        # If the agent cannot arrive to the target\n",
    "        if lenght == np.inf:\n",
    "            return np.full((self.max_depth,), np.inf)\n",
    "\n",
    "        # Initialize each node with the distance from the agent to the target\n",
    "        distances = np.zeros((self.max_depth,))\n",
    "        max_distance = (\n",
    "            (lenght * self.speed_data[handle].times)\n",
    "            + turns_to_deviation\n",
    "        )\n",
    "        distances[:len(path)] = np.full((len(path),), max_distance)\n",
    "\n",
    "        # Compute actual distances for each node\n",
    "        distances -= cum_weights\n",
    "        return distances\n",
    "\n",
    "    def common_nodes(self, handle, positions):\n",
    "        '''\n",
    "        Given an agent's positions and the shortest positions for every other agent,\n",
    "        compute the number of agents intersecting at each node\n",
    "        '''\n",
    "        c_nodes = np.zeros((self.max_depth,))\n",
    "        if len(positions) > 0:\n",
    "            nd_positions = np.array(positions, np.dtype('int, int'))\n",
    "            computed = np.zeros((len(positions),))\n",
    "            for row in self.other_agents[handle]:\n",
    "                computed += np.count_nonzero(\n",
    "                    np.isin(\n",
    "                        nd_positions, self._shortest_positions[row, :]\n",
    "                    ).reshape(1, len(nd_positions)),\n",
    "                    axis=0\n",
    "                )\n",
    "            c_nodes[:computed.shape[0]] = computed\n",
    "        return c_nodes\n",
    "\n",
    "    def _get_shortest_packed_positions(self):\n",
    "        '''\n",
    "        For each agent's shortest path, substitute the first node for\n",
    "        its previous node in the packed graph, if it doesn't\n",
    "        already match with the agent's position\n",
    "\n",
    "        Return the modified path (without the direction component),\n",
    "        along with the associated cumulative weights (which are re-computed\n",
    "        starting from the original cumulative weights)\n",
    "        '''\n",
    "        prev_weights = []\n",
    "        prev_nodes = [node[0] for node in self.last_nodes]\n",
    "        for agent, path in enumerate(self._shortest_paths):\n",
    "            # If the agent's position is not on the packed graph\n",
    "            if tuple(path[0]) != prev_nodes[agent]:\n",
    "                prev_weights.append(\n",
    "                    - (self.last_nodes[agent][1] +\n",
    "                       self.speed_data[agent].times -\n",
    "                       self.speed_data[agent].remaining)\n",
    "                )\n",
    "            # If the agent's position is already in the packed path,\n",
    "            # do not change the cumulative weights of the first node\n",
    "            else:\n",
    "                prev_weights.append(self._shortest_cum_weights[agent, 0])\n",
    "\n",
    "        # Remove the first column of the original shortest positions\n",
    "        # and replace it with the previous node\n",
    "        packed_positions = np.hstack([\n",
    "            np.array(\n",
    "                [node[:-1] for node in prev_nodes], np.dtype('int, int')\n",
    "            ).reshape(self._shortest_positions.shape[0], 1),\n",
    "            self._shortest_positions[:, 1:]\n",
    "        ])\n",
    "        # Update the corresponding cumulative weights\n",
    "        packed_weights = np.hstack([\n",
    "            np.array(prev_weights).reshape(\n",
    "                self._shortest_cum_weights.shape[0], 1\n",
    "            ),\n",
    "            self._shortest_cum_weights[:, 1:]\n",
    "        ])\n",
    "        return packed_positions, packed_weights\n",
    "\n",
    "    def find_deadlocks(self, handle, positions, cum_weights, packed_positions, packed_weights, prev_deadlocks=0):\n",
    "        '''\n",
    "        For a shortest path and a deviation path:\n",
    "        - `positions` should be the packed positions\n",
    "        - `cum_weights` should be the packed cumulative weights\n",
    "        - `packed_positions` should be the list of packed shortest positions for each agent\n",
    "        - `packed_weights` should be the list of packed cumulative weights for each agent\n",
    "\n",
    "        Returns two lists:\n",
    "        - `deadlocks`: the number of possible deadlocks for each node in `path`\n",
    "        - `crash_turns`: the number of turns to the first deadlock for each node in `path`\n",
    "        '''\n",
    "        deadlocks = np.full((self.max_depth,), prev_deadlocks)\n",
    "        crash_turns = np.full((self.max_depth,), np.inf)\n",
    "\n",
    "        # For each agent different than myself\n",
    "        for agent in self.other_agents[handle]:\n",
    "            deadlock_found = False\n",
    "            agent_path = packed_positions[agent].tolist()\n",
    "            # For each node in the other agent's path\n",
    "            for i in range(len(agent_path) - 1):\n",
    "                # Avoid non-informative pair of nodes\n",
    "                if tuple(agent_path[i]) != (-1, -1) and tuple(agent_path[i + 1]) != (-1, -1):\n",
    "                    # For each node in my path\n",
    "                    for j in range(len(positions) - 1):\n",
    "                        source, dest = positions[j], positions[j + 1]\n",
    "                        from_dest_to_source = (\n",
    "                            source == agent_path[i + 1] and\n",
    "                            dest == agent_path[i]\n",
    "                        )\n",
    "                        intersecting_turns = (\n",
    "                            not cum_weights[j] > packed_weights[agent, i + 1] and\n",
    "                            not cum_weights[j + 1] < packed_weights[agent, i]\n",
    "                        )\n",
    "                        deadlock_found = from_dest_to_source and intersecting_turns\n",
    "                        if deadlock_found:\n",
    "                            space = (\n",
    "                                cum_weights[j + 1] - cum_weights[j]\n",
    "                            ) / self.speed_data[handle].times\n",
    "\n",
    "                            # Both agents in same edge: reduce space by how much they\n",
    "                            # already have traversed\n",
    "                            if cum_weights[j] < 0 and packed_weights[agent, i] < 0:\n",
    "                                space += (\n",
    "                                    cum_weights[j] /\n",
    "                                    self.speed_data[handle].times\n",
    "                                )\n",
    "                                space += (\n",
    "                                    packed_weights[agent, i] /\n",
    "                                    self.speed_data[agent].times\n",
    "                                )\n",
    "                            # My entry turn is greater than the other agent's entry turn:\n",
    "                            # reduce space by how the other agent's has already traversed,\n",
    "                            # by the time my agent enters the edge\n",
    "                            elif cum_weights[j] > packed_weights[agent, i]:\n",
    "                                space -= abs(\n",
    "                                    cum_weights[j] -\n",
    "                                    abs(packed_weights[agent, i])\n",
    "                                ) / self.speed_data[agent].times\n",
    "                            # The opposite of the previous case\n",
    "                            elif packed_weights[agent, i] > cum_weights[j]:\n",
    "                                space += abs(\n",
    "                                    packed_weights[agent, i] -\n",
    "                                    abs(cum_weights[j])\n",
    "                                ) / self.speed_data[agent].times\n",
    "\n",
    "                            # Compute the distance in turns from my agent to\n",
    "                            # the possible identified deadlock\n",
    "                            crash_turn = np.ceil(\n",
    "                                np.clip(cum_weights[j], 0, None) +\n",
    "                                space / reciprocal_sum(\n",
    "                                    self.speed_data[agent].times,\n",
    "                                    self.speed_data[handle].times\n",
    "                                )\n",
    "                            )\n",
    "                            # Store only the minimum distance\n",
    "                            if crash_turns[j] > crash_turn:\n",
    "                                crash_turns[j] = crash_turn\n",
    "\n",
    "                            # Update number of deadlocks\n",
    "                            deadlocks[j:len(positions)] += 1\n",
    "\n",
    "                            # If one deadlock is found, do not check any other\n",
    "                            # between the same pair of agents\n",
    "                            break\n",
    "\n",
    "                    # If one deadlock is found, do not check any other\n",
    "                    # between the same pair of agents\n",
    "                    if deadlock_found:\n",
    "                        break\n",
    "\n",
    "        return deadlocks, crash_turns\n",
    "    \n",
    "    \n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "class FOVObservator(ObservationBuilder):\n",
    "    '''\n",
    "    An Observator that return a local observation of each agent in the form of\n",
    "    a tensor of size max_depth centered around the agent position\n",
    "\n",
    "    Features:\n",
    "        0. Cell type of the rail in the agent's FOV\n",
    "        1. Cell orientation of the rail in the agent's FOV\n",
    "        2. Distances in shortest path in the agent's FOV\n",
    "        3. Other agents positions in the agent's FOV (direction of each agent)\n",
    "        4. Agents targets in the agent's FOV (1 agent target, 0 other agent, -1 otherwise)\n",
    "        5. Agents malfunctioning turns\n",
    "        6. Agents fractional speeds\n",
    "        7+. Distances in deviation paths in the agent's FOV\n",
    "    '''\n",
    "\n",
    "    def __init__(self, max_depth, predictor):\n",
    "        super().__init__()\n",
    "        # Always keep an odd number of \"squares\", so that the agent\n",
    "        # is centered w.r.t. its FOV\n",
    "        assert max_depth % 2 != 0, 'FOV window must be an odd number'\n",
    "        self.max_depth = max_depth\n",
    "        self.predictor = predictor\n",
    "        self.observations = dict()\n",
    "        self.observation_dim = 7 + self.predictor.max_deviations\n",
    "        self.possible_transitions_dict = self.compute_all_possible_transitions()\n",
    "        self.agent_positions = None\n",
    "        self.agent_malfunctions = None\n",
    "        self.agent_speeds = None\n",
    "        self.agent_targets = None\n",
    "        self.data_object = None\n",
    "\n",
    "    def reset(self):\n",
    "        if self.predictor is not None:\n",
    "            self.predictor.reset()\n",
    "        rail_obs_16_channels = np.zeros((self.env.height, self.env.width, 16))\n",
    "        for i in range(rail_obs_16_channels.shape[0]):\n",
    "            for j in range(rail_obs_16_channels.shape[1]):\n",
    "                bitlist = [\n",
    "                    int(digit) for digit in\n",
    "                    bin(self.env.rail.get_full_transitions(i, j))[2:]\n",
    "                ]\n",
    "                bitlist = [0] * (16 - len(bitlist)) + bitlist\n",
    "                rail_obs_16_channels[i, j] = np.array(bitlist)\n",
    "        self.rail_obs = self.convert_transitions_map(rail_obs_16_channels)\n",
    "        self.agent_targets = np.full(\n",
    "            (self.env.get_num_agents(), self.env.rail.height, self.env.rail.width), -1\n",
    "        )\n",
    "        for handle, agent in enumerate(self.env.agents):\n",
    "            target = agent.target\n",
    "            if target is not None:\n",
    "                self.agent_targets[handle, target[0], target[1]] = 1\n",
    "                other_agents = set(self.env.get_agent_handles()) - {handle}\n",
    "                for other in other_agents:\n",
    "                    if self.agent_targets[other, target[0], target[1]] == -1:\n",
    "                        self.agent_targets[other, target[0], target[1]] = 0\n",
    "\n",
    "    def set_env(self, env):\n",
    "        super().set_env(env)\n",
    "        if self.predictor:\n",
    "            self.predictor.set_env(self.env)\n",
    "\n",
    "    def convert_transitions_map(self, obs_transitions_map):\n",
    "        '''\n",
    "        Given an np.array of shape (env_height, env_width_, 16),\n",
    "        convert it to (env_height,env_width, 2) where the first channel\n",
    "        encodes cell types (empty cell 0 is encoded as -1,\n",
    "        while cell types 1 to 10 are encoded as-is)\n",
    "        and the second channel orientations (0, 90, 180, 270 as 0, 1, 2, 3)\n",
    "        '''\n",
    "        new_transitions_map = np.full(\n",
    "            (obs_transitions_map.shape[0], obs_transitions_map.shape[1], 2), -1\n",
    "        )\n",
    "\n",
    "        for i in range(obs_transitions_map.shape[0]):\n",
    "            for j in range(obs_transitions_map.shape[1]):\n",
    "                transition_bitmap = obs_transitions_map[i, j]\n",
    "                int_transition_bitmap = int(\n",
    "                    transition_bitmap.dot(\n",
    "                        2 ** np.arange(transition_bitmap.size)[::-1]\n",
    "                    )\n",
    "                )\n",
    "                if int_transition_bitmap != 0:\n",
    "                    new_transitions_map[i, j] = (\n",
    "                        self.possible_transitions_dict[int_transition_bitmap]\n",
    "                    )\n",
    "\n",
    "        return new_transitions_map\n",
    "\n",
    "    def compute_all_possible_transitions(self):\n",
    "        '''\n",
    "        Given transitions list considering cell types,\n",
    "        outputs all possible transitions bitmap,\n",
    "        considering cell rotations too\n",
    "        '''\n",
    "        # Bitmaps are read in decimal numbers\n",
    "        transitions = RailEnvTransitions()\n",
    "        transitions_with_rotation_dict = {}\n",
    "        rotation_degrees = [0, 90, 180, 270]\n",
    "\n",
    "        for index, transition in enumerate(transitions.transition_list):\n",
    "            for rot_type, rot in enumerate(rotation_degrees):\n",
    "                rot_transition = transitions.rotate_transition(transition, rot)\n",
    "                if rot_transition not in transitions_with_rotation_dict:\n",
    "                    transitions_with_rotation_dict[rot_transition] = (\n",
    "                        np.array([index, rot_type])\n",
    "                    )\n",
    "        return transitions_with_rotation_dict\n",
    "\n",
    "    def extract_path_fov(self, path, lenght, pad=0):\n",
    "        '''\n",
    "        Given a path returns the matrix fov marking the occupied positions assuming\n",
    "        the first position as the center one of the fov\n",
    "        '''\n",
    "        path_fov = np.full((self.max_depth, self.max_depth), pad)\n",
    "        distance = lenght\n",
    "        if distance < np.inf:\n",
    "            y, x = self.max_depth // 2, self.max_depth // 2\n",
    "            prev_pos = path[0]\n",
    "            for pos in path[1:]:\n",
    "                if y >= 0 and y < self.max_depth and x >= 0 and x < self.max_depth:\n",
    "                    path_fov[y, x] = distance\n",
    "                    if pos[0] != prev_pos[0] or pos[1] != prev_pos[1]:\n",
    "                        distance -= 1\n",
    "                y += pos[0] - prev_pos[0]\n",
    "                x += pos[1] - prev_pos[1]\n",
    "                prev_pos = pos\n",
    "\n",
    "            # Add last element\n",
    "            if y >= 0 and y < self.max_depth and x >= 0 and x < self.max_depth:\n",
    "                path_fov[y, x] = distance\n",
    "\n",
    "        return path_fov\n",
    "\n",
    "    def get_many(self, handles=None):\n",
    "        self.agent_positions = np.full(\n",
    "            (self.env.rail.height, self.env.rail.width), -1\n",
    "        )\n",
    "        self.agent_malfunctions = np.full(\n",
    "            (self.env.rail.height, self.env.rail.width), -1\n",
    "        )\n",
    "        self.agent_speeds = np.full(\n",
    "            (self.env.rail.height, self.env.rail.width), -1\n",
    "        )\n",
    "        if self.predictor is not None:\n",
    "            self.predictions = self.predictor.get_many()\n",
    "        for handle, agent in enumerate(self.env.agents):\n",
    "            agent_position = self.env.railway_encoding.get_agent_cell(\n",
    "                handle\n",
    "            )\n",
    "            if agent_position is not None:\n",
    "                self.agent_positions[\n",
    "                    agent_position[0], agent_position[1]\n",
    "                ] = agent_position[2]\n",
    "                self.agent_malfunctions[\n",
    "                    agent_position[0], agent_position[1]\n",
    "                ] = agent.malfunction_data['malfunction']\n",
    "                self.agent_speeds[\n",
    "                    agent_position[0], agent_position[1]\n",
    "                ] = agent.speed_data['speed']\n",
    "\n",
    "        # Compute adjacency matrix and store it in a\n",
    "        # PyTorch Geometric Data object\n",
    "        adjacency = self.env.agents_adjacency_matrix(\n",
    "            radius=self.max_depth\n",
    "        )\n",
    "        edge_index = torch.from_numpy(\n",
    "            np.argwhere(adjacency != 0)\n",
    "        ).long().t().contiguous()\n",
    "        edge_weight = torch.from_numpy(\n",
    "            adjacency[np.nonzero(adjacency)]\n",
    "        ).float()\n",
    "        edge_index, edge_weight = add_remaining_self_loops(\n",
    "            edge_index, edge_weight, fill_value=1, num_nodes=self.env.get_num_agents()\n",
    "        )\n",
    "\n",
    "        # Add features to PyTorch Geometric Data object\n",
    "        states = super().get_many(handles)\n",
    "        self.data_object = Data(\n",
    "            edge_index=edge_index,\n",
    "            edge_weight=edge_weight,\n",
    "            num_nodes=self.env.get_num_agents(),\n",
    "            states=torch.tensor(\n",
    "                list(states.values()), dtype=torch.float\n",
    "            )\n",
    "        )\n",
    "        self.observations = {\n",
    "            handle: self.data_object for handle in self.env.get_agent_handles()\n",
    "        }\n",
    "        return self.observations\n",
    "\n",
    "    def get(self, handle=0):\n",
    "        self.observations[handle] = np.full(\n",
    "            (self.observation_dim, self.max_depth, self.max_depth), -1\n",
    "        )\n",
    "        if (self.predictions[handle] is not None):\n",
    "            agent_position = self.env.railway_encoding.get_agent_cell(\n",
    "                handle\n",
    "            )\n",
    "            if agent_position is not None:\n",
    "                shortest_pred, deviations_pred = self.predictions[handle]\n",
    "                num_devs = self.predictor.max_deviations\n",
    "\n",
    "                # Cell type of the rail in the agent's FOV\n",
    "                cell_type = extract_fov(\n",
    "                    self.rail_obs[:, :, 0], agent_position, self.max_depth, -1\n",
    "                )\n",
    "\n",
    "                # Cell orientation of the rail in the agent's FOV\n",
    "                cell_orientation = extract_fov(\n",
    "                    self.rail_obs[:, :, 1], agent_position, self.max_depth, -1\n",
    "                )\n",
    "\n",
    "                # Distances in shortest path in the agent's FOV\n",
    "                path_fov = self.extract_path_fov(\n",
    "                    shortest_pred.positions, shortest_pred.lenght, pad=-1\n",
    "                )\n",
    "\n",
    "                # Distances in deviation paths in the agent's FOV\n",
    "                dev_paths_fov = np.full(\n",
    "                    (num_devs, self.max_depth, self.max_depth), -1\n",
    "                )\n",
    "                for i, dev_pred in enumerate(deviations_pred):\n",
    "                    dev_path_fov = np.full(\n",
    "                        (self.max_depth, self.max_depth), -1\n",
    "                    )\n",
    "                    if dev_pred.lenght < np.inf:\n",
    "                        source_index = shortest_pred.positions.index(\n",
    "                            dev_pred.positions[0]\n",
    "                        )\n",
    "                        if source_index != -1:\n",
    "                            dev_pos = (\n",
    "                                shortest_pred.positions[:source_index] +\n",
    "                                dev_pred.positions\n",
    "                            )\n",
    "                            dev_path_fov = self.extract_path_fov(\n",
    "                                dev_pos, len(dev_pos), pad=-1\n",
    "                            )\n",
    "                    dev_paths_fov[i] = dev_path_fov\n",
    "\n",
    "                # Other agents positions in the agent's FOV (direction of each agent)\n",
    "                agents_fov = extract_fov(\n",
    "                    self.agent_positions, agent_position, self.max_depth, -1\n",
    "                )\n",
    "\n",
    "                # Agents targets in the agent's FOV (1 agent target, 0 other agent, -1 otherwise)\n",
    "                targets_fov = extract_fov(\n",
    "                    self.agent_targets[handle, :, :],\n",
    "                    agent_position, self.max_depth, -1\n",
    "                )\n",
    "\n",
    "                # Malfunctioning turns\n",
    "                malf_fov = extract_fov(\n",
    "                    self.agent_malfunctions, agent_position, self.max_depth, -1\n",
    "                )\n",
    "\n",
    "                # Speed information\n",
    "                speed_fov = extract_fov(\n",
    "                    self.agent_speeds, agent_position, self.max_depth, -1\n",
    "                )\n",
    "\n",
    "                # Update observations\n",
    "                self.observations[handle][0] = cell_type\n",
    "                self.observations[handle][1] = cell_orientation\n",
    "                self.observations[handle][2] = path_fov\n",
    "                self.observations[handle][3] = agents_fov\n",
    "                self.observations[handle][4] = targets_fov\n",
    "                self.observations[handle][5] = malf_fov\n",
    "                self.observations[handle][6] = speed_fov\n",
    "                self.observations[handle][7: 7 + num_devs + 1] = dev_paths_fov\n",
    "\n",
    "        return self.observations[handle]\n",
    "    \n",
    "    \n",
    "## Graph Observator \n",
    "class GraphObservator(ObservationBuilder):\n",
    "\n",
    "    def __init__(self, max_depth, predictor):\n",
    "        super().__init__()\n",
    "        self.max_depth = max_depth\n",
    "        self.predictor = predictor\n",
    "        self.observations = dict()\n",
    "        self.observation_dim = 2\n",
    "        self.data_object = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.data_object = self._init_graph()\n",
    "        if self.predictor is not None:\n",
    "            self.predictor.reset()\n",
    "\n",
    "    def set_env(self, env):\n",
    "        super().set_env(env)\n",
    "        if self.predictor:\n",
    "            self.predictor.set_env(self.env)\n",
    "\n",
    "    def get_many(self, handles=None):\n",
    "        self.predictions = self.predictor.get_many()\n",
    "        return super().get_many(handles)\n",
    "\n",
    "    def get(self, handle=0):\n",
    "        # Compute node features\n",
    "        nodes = self.env.railway_encoding.get_graph_nodes(\n",
    "            unpacked=False, data=True\n",
    "        )\n",
    "        x = [None] * len(nodes)\n",
    "        agents_positions = {\n",
    "            self.env.railway_encoding.get_agent_cell(h)\n",
    "            for h in range(len(self.env.agents)) if h != handle\n",
    "        }\n",
    "        for n, _ in nodes:\n",
    "            target_distance = self.env.distance_map.get()[\n",
    "                handle, n[0], n[1], n[2]\n",
    "            ]\n",
    "            is_occupied = n in agents_positions\n",
    "            x[self.env.railway_encoding.node_to_index[n]] = [\n",
    "                # d[\"is_dead_end\"], d[\"is_fork\"], d[\"is_join\"], d[\"is_target\"],\n",
    "                target_distance, is_occupied\n",
    "            ]\n",
    "        x = torch.tensor(x, dtype=torch.float)\n",
    "\n",
    "        # Store a list of important positions, so that the DQN is called with\n",
    "        # the GNN embeddings of these nodes\n",
    "        agent_position = self.env.railway_encoding.get_agent_cell(handle)\n",
    "        agent_pos_index = -1\n",
    "        successors = []\n",
    "        if agent_position is not None:\n",
    "            agent_in_packed = self.env.railway_encoding.is_node(\n",
    "                agent_position, unpacked=False\n",
    "            )\n",
    "            if agent_in_packed:\n",
    "                successors = self.env.railway_encoding.get_successors(\n",
    "                    agent_position, unpacked=False\n",
    "                )\n",
    "            else:\n",
    "                actual_agent_position = tuple(agent_position)\n",
    "                agent_position, _ = self.env.railway_encoding.previous_node(\n",
    "                    actual_agent_position\n",
    "                )\n",
    "                successor, _ = self.env.railway_encoding.next_node(\n",
    "                    actual_agent_position\n",
    "                )\n",
    "                successors = [successor]\n",
    "            agent_pos_index = self.env.railway_encoding.node_to_index[agent_position]\n",
    "\n",
    "        successors_indexes = {\"left\": -1, \"right\": -1}\n",
    "        for succ in successors:\n",
    "            succ_index = self.env.railway_encoding.node_to_index[succ]\n",
    "            succ_choice = self.env.railway_encoding.get_edge_data(\n",
    "                agent_position, succ, 'choice', unpacked=False\n",
    "            )\n",
    "            if succ_choice == RailEnvChoices.CHOICE_LEFT:\n",
    "                successors_indexes[\"left\"] = succ_index\n",
    "            elif succ_choice == RailEnvChoices.CHOICE_RIGHT:\n",
    "                successors_indexes[\"right\"] = succ_index\n",
    "        pos = torch.tensor([\n",
    "            successors_indexes[\"left\"],\n",
    "            successors_indexes[\"right\"],\n",
    "            agent_pos_index\n",
    "        ], dtype=torch.long)\n",
    "\n",
    "        # Create a PyTorch Geometric Data object\n",
    "        self.observations[handle] = Data(\n",
    "            edge_index=self.data_object.edge_index,\n",
    "            edge_weight=self.data_object.edge_weight,\n",
    "            pos=pos, x=x\n",
    "        )\n",
    "        return self.observations[handle]\n",
    "\n",
    "    def _init_graph(self):\n",
    "        '''\n",
    "        Initialize the graph structure, which is the same\n",
    "        for all agents in an episode\n",
    "        '''\n",
    "        # Compute edges and edges attributes\n",
    "        edges = self.env.railway_encoding.get_graph_edges(\n",
    "            unpacked=False, data=True\n",
    "        )\n",
    "        edge_index, edge_weight = [], []\n",
    "        for u, v, d in edges:\n",
    "            edge_index.append([\n",
    "                self.env.railway_encoding.node_to_index[u],\n",
    "                self.env.railway_encoding.node_to_index[v]\n",
    "            ])\n",
    "            edge_weight.append(d['weight'])\n",
    "        edge_index = torch.tensor(\n",
    "            edge_index, dtype=torch.long\n",
    "        ).t().contiguous()\n",
    "        edge_weight = torch.tensor(edge_weight, dtype=torch.float)\n",
    "        return Data(\n",
    "            edge_index=edge_index, edge_weight=edge_weight\n",
    "        )\n",
    "        \n",
    "## UTILS ##\n",
    "\n",
    "OBSERVATORS = {\n",
    "    \"tree\": TreeObsForRailEnv,\n",
    "    \"binary_tree\": BinaryTreeObservator,\n",
    "    \"graph\": GraphObservator,\n",
    "    \"decentralized_fov\": FOVObservator\n",
    "}\n",
    "PREDICTORS = {\n",
    "    \"tree\": ShortestPathPredictorForRailEnv,\n",
    "    \"binary_tree\": ShortestDeviationPathPredictor,\n",
    "    \"graph\": NullPredictor,\n",
    "    \"decentralized_fov\": ShortestDeviationPathPredictor\n",
    "}\n",
    "\n",
    "\n",
    "class RailEnvChoices(IntEnum):\n",
    "\n",
    "    CHOICE_LEFT = 0\n",
    "    CHOICE_RIGHT = 1\n",
    "    STOP = 2\n",
    "\n",
    "    @staticmethod\n",
    "    def value_of(value):\n",
    "        '''\n",
    "        Return an instance of RailEnvChoices from the given choice type int\n",
    "        '''\n",
    "        for _, choice_type in RailEnvChoices.__members__.items():\n",
    "            if choice_type.value == value.capitalize():\n",
    "                return choice_type\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def values():\n",
    "        '''\n",
    "        Return a list of every possible RailEnvChoices\n",
    "        '''\n",
    "        return [\n",
    "            choice_type\n",
    "            for _, choice_type in RailEnvChoices.__members__.items()\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def choice_size():\n",
    "        '''\n",
    "        Return the number of values that can be assigned\n",
    "        to a RailEnvChoices instance\n",
    "        '''\n",
    "        return len(RailEnvChoices.values())\n",
    "\n",
    "    @staticmethod\n",
    "    def default_choices():\n",
    "        '''\n",
    "        Return a mask of choices, s.t. the only choice that\n",
    "        can always be applied is STOP\n",
    "        '''\n",
    "        return [False, False, True]\n",
    "\n",
    "\n",
    "def get_num_actions():\n",
    "    '''\n",
    "    Return the number of possible RailEnvActions\n",
    "    '''\n",
    "    return len([\n",
    "        action_type for _, action_type in RailEnvActions.__members__.items()\n",
    "    ])\n",
    "\n",
    "\n",
    "def create_rail_env(args, load_env=\"\"):\n",
    "    '''\n",
    "    Build a RailEnv object with the specified parameters,\n",
    "    as described in the .yml file\n",
    "    '''\n",
    "    # Check if an environment file is provided\n",
    "    if load_env:\n",
    "        rail_generator = rail_from_file(load_env)\n",
    "    else:\n",
    "        rail_generator = sparse_rail_generator(\n",
    "            max_num_cities=args.env.max_cities,\n",
    "            grid_mode=args.env.grid,\n",
    "            max_rails_between_cities=args.env.max_rails_between_cities,\n",
    "            seed=args.env.seed\n",
    "        )\n",
    "\n",
    "    # Build predictor and observator\n",
    "    obs_type = args.policy.type.get_true_key()\n",
    "    if PREDICTORS[obs_type] is ShortestDeviationPathPredictor:\n",
    "        predictor = PREDICTORS[obs_type](\n",
    "            max_depth=args.observator.max_depth,\n",
    "            max_deviations=args.predictor.max_depth\n",
    "        )\n",
    "    else:\n",
    "        predictor = PREDICTORS[obs_type](max_depth=args.predictor.max_depth, max_deviations = args.predictor.max_deviations)\n",
    "    observator = OBSERVATORS[obs_type](args.observator.max_depth, predictor)\n",
    "\n",
    "    # Initialize malfunctions\n",
    "    malfunctions = None\n",
    "    if args.env.malfunctions.enabled:\n",
    "        malfunctions = ParamMalfunctionGen(\n",
    "            MalfunctionParameters(\n",
    "                malfunction_rate=args.env.malfunctions.rate,\n",
    "                min_duration=args.env.malfunctions.min_duration,\n",
    "                max_duration=args.env.malfunctions.max_duration\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Initialize agents speeds\n",
    "    speed_map = None\n",
    "    if args.env.variable_speed:\n",
    "        speed_map = {\n",
    "            1.: 0.25,\n",
    "            1. / 2.: 0.25,\n",
    "            1. / 3.: 0.25,\n",
    "            1. / 4.: 0.25\n",
    "        }\n",
    "    schedule_generator = sparse_line_generator(\n",
    "        speed_map, seed=args.env.seed\n",
    "    )\n",
    "\n",
    "    # Build the environment\n",
    "    return RailEnvWrapper(\n",
    "        params=args,\n",
    "        width=args.env.width,\n",
    "        height=args.env.height,\n",
    "        rail_generator=rail_generator,\n",
    "        number_of_agents=args.env.num_trains,\n",
    "        obs_builder_object=observator,\n",
    "        malfunction_generator=malfunctions,\n",
    "        remove_agents_at_target=True,\n",
    "        random_seed=args.env.seed\n",
    "    )\n",
    "\n",
    "\n",
    "def create_save_env(path, width, height, num_trains, max_cities,\n",
    "                    max_rails_between_cities, max_rails_in_cities, grid=False, seed=0):\n",
    "    '''\n",
    "    Create a RailEnv environment with the given settings and save it as pickle\n",
    "    '''\n",
    "    rail_generator = sparse_rail_generator(\n",
    "        max_num_cities=max_cities,\n",
    "        seed=seed,\n",
    "        grid_mode=grid,\n",
    "        max_rails_between_cities=max_rails_between_cities,\n",
    "        max_rails_in_city=max_rails_in_cities,\n",
    "    )\n",
    "    env = RailEnv(\n",
    "        width=width,\n",
    "        height=height,\n",
    "        rail_generator=rail_generator,\n",
    "        number_of_agents=num_trains\n",
    "    )\n",
    "    env.save(path)\n",
    "\n",
    "\n",
    "def get_seed(env, seed=None):\n",
    "    '''\n",
    "    Exploit the RailEnv to get a random seed\n",
    "    '''\n",
    "    seed = env._seed(seed)\n",
    "    return seed[0]\n",
    "\n",
    "\n",
    "def copy_obs(obs):\n",
    "    '''\n",
    "    Return a deep copy of the given observation\n",
    "    '''\n",
    "    if hasattr(obs, \"copy\"):\n",
    "        return obs.copy()\n",
    "    return copy.deepcopy(obs)\n",
    "\n",
    "\n",
    "def agent_action(original_dir, final_dir):\n",
    "    '''\n",
    "    Return the action performed by an agent, by analyzing\n",
    "    the starting direction and the final direction of the movement\n",
    "    '''\n",
    "    value = (final_dir.value - original_dir.value) % 4\n",
    "    if value in (1, -3):\n",
    "        return RailEnvActions.MOVE_RIGHT\n",
    "    elif value in (-1, 3):\n",
    "        return RailEnvActions.MOVE_LEFT\n",
    "    return RailEnvActions.MOVE_FORWARD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy\n",
    "\n",
    "Policy utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMSELoss(nn.Module):\n",
    "    '''\n",
    "    MSE loss with masked inputs/targets\n",
    "    '''\n",
    "\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(MaskedMSELoss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, target, mask=None):\n",
    "        if mask is None:\n",
    "            return F.mse_loss(input, target, reduction=self.reduction)\n",
    "\n",
    "        flattened_mask = torch.flatten(mask).float()\n",
    "        diff = ((\n",
    "            torch.flatten(input) - torch.flatten(target)\n",
    "        ) ** 2.0) * flattened_mask\n",
    "        mask_sum = (\n",
    "            torch.sum(flattened_mask)\n",
    "            if self.reduction == 'mean'\n",
    "            else 1.0\n",
    "        )\n",
    "        return torch.sum(diff) / mask_sum\n",
    "\n",
    "\n",
    "class MaskedHuberLoss(nn.Module):\n",
    "    '''\n",
    "    Huber loss with masked inputs/targets\n",
    "    '''\n",
    "\n",
    "    def __init__(self, reduction='mean', beta=1.0):\n",
    "        super(MaskedHuberLoss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "        self.beta = float(beta)\n",
    "\n",
    "    def forward(self, input, target, mask=None):\n",
    "        if mask is None:\n",
    "            return F.smooth_l1_loss(\n",
    "                input, target, reduction=self.reduction, beta=self.beta\n",
    "            )\n",
    "\n",
    "        flattened_mask = torch.flatten(mask).float()\n",
    "        errors = torch.abs(torch.flatten(input) -\n",
    "                           torch.flatten(target)) * flattened_mask\n",
    "        diff = torch.zeros_like(errors)\n",
    "        less = errors < self.beta\n",
    "        diff[less] = (\n",
    "            (0.5 * (errors[less] ** 2) / self.beta) * flattened_mask[less]\n",
    "        )\n",
    "        diff[~less] = (errors[~less] - 0.5 * self.beta) * flattened_mask[~less]\n",
    "        mask_sum = (\n",
    "            torch.sum(flattened_mask)\n",
    "            if self.reduction == 'mean'\n",
    "            else 1.0\n",
    "        )\n",
    "        return torch.sum(diff) / mask_sum\n",
    "\n",
    "\n",
    "class Sequential(nn.Sequential):\n",
    "    '''\n",
    "    Extension of the PyTorch Sequential module,\n",
    "    to handle a variable number of arguments\n",
    "    '''\n",
    "\n",
    "    def forward(self, input, **kwargs):\n",
    "        for module in self:\n",
    "            input = module(input, **kwargs)\n",
    "        return input\n",
    "\n",
    "\n",
    "def masked_softmax(vec, mask, dim=1, temperature=1):\n",
    "    '''\n",
    "    Softmax only on valid outputs\n",
    "    '''\n",
    "    assert vec.shape == mask.shape\n",
    "    assert np.all(mask.astype(bool).any(axis=dim)), mask\n",
    "\n",
    "    exps = vec.copy()\n",
    "    exps = np.exp(vec / temperature)\n",
    "    exps[~mask.astype(bool)] = 0\n",
    "    return exps / exps.sum(axis=dim, keepdims=True)\n",
    "\n",
    "\n",
    "def masked_max(vec, mask, dim=1):\n",
    "    '''\n",
    "    Max only on valid outputs\n",
    "    '''\n",
    "    assert vec.shape == mask.shape\n",
    "    assert np.all(mask.astype(bool).any(axis=dim)), mask\n",
    "\n",
    "    res = vec.copy()\n",
    "    res[~mask.astype(bool)] = np.nan\n",
    "    return np.nanmax(res, axis=dim, keepdims=True)\n",
    "\n",
    "\n",
    "def masked_argmax(vec, mask, dim=1):\n",
    "    '''\n",
    "    Argmax only on valid outputs\n",
    "    '''\n",
    "    assert vec.shape == mask.shape\n",
    "    assert np.all(mask.astype(bool).any(axis=dim)), mask\n",
    "\n",
    "    res = vec.copy()\n",
    "    res[~mask.astype(bool)] = np.nan\n",
    "    argmax_arr = np.nanargmax(res, axis=dim)\n",
    "\n",
    "    # Argmax has no keepdims argument\n",
    "    if dim > 0:\n",
    "        new_shape = list(res.shape)\n",
    "        new_shape[dim] = 1\n",
    "        argmax_arr = argmax_arr.reshape(tuple(new_shape))\n",
    "\n",
    "    return argmax_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Action selection\n",
    "\n",
    "class ParameterDecay:\n",
    "\n",
    "    def __init__(self, parameter_start, parameter_end,\n",
    "                 parameter_decay=None, total_episodes=None, decaying_episodes=None):\n",
    "        parameter_decay_choice = parameter_decay is not None\n",
    "        episodes_decay_choice = total_episodes is not None and decaying_episodes is not None\n",
    "        assert parameter_decay_choice or episodes_decay_choice\n",
    "\n",
    "        self.parameter_start = parameter_start\n",
    "        self.parameter_end = parameter_end\n",
    "        self.parameter_decay = parameter_decay\n",
    "\n",
    "    def decay(self, parameter):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class NullParameterDecay(ParameterDecay):\n",
    "\n",
    "    def __init__(self, parameter_start, *args):\n",
    "        super(NullParameterDecay, self).__init__(\n",
    "            parameter_start, parameter_start, parameter_decay=0\n",
    "        )\n",
    "\n",
    "    def decay(self, parameter):\n",
    "        return parameter\n",
    "\n",
    "\n",
    "class LinearParameterDecay(ParameterDecay):\n",
    "\n",
    "    def __init__(self, parameter_start, parameter_end,\n",
    "                 parameter_decay=None, total_episodes=None, decaying_episodes=None):\n",
    "        super(LinearParameterDecay, self).__init__(\n",
    "            parameter_start, parameter_end,\n",
    "            parameter_decay=parameter_decay,\n",
    "            total_episodes=total_episodes,\n",
    "            decaying_episodes=decaying_episodes\n",
    "        )\n",
    "        if self.parameter_decay is None:\n",
    "            self.parameter_decay = (\n",
    "                (self.parameter_start - self.parameter_end) /\n",
    "                (total_episodes * decaying_episodes)\n",
    "            )\n",
    "\n",
    "    def decay(self, parameter):\n",
    "        return max(\n",
    "            self.parameter_end, parameter - self.parameter_decay\n",
    "        )\n",
    "\n",
    "\n",
    "class ExponentialParameterDecay(ParameterDecay):\n",
    "\n",
    "    def __init__(self, parameter_start, parameter_end,\n",
    "                 parameter_decay=None, total_episodes=None, decaying_episodes=None):\n",
    "        super(ExponentialParameterDecay, self).__init__(\n",
    "            parameter_start, parameter_end,\n",
    "            parameter_decay=parameter_decay,\n",
    "            total_episodes=total_episodes,\n",
    "            decaying_episodes=decaying_episodes\n",
    "        )\n",
    "        if self.parameter_decay is None:\n",
    "            self.parameter_decay = (\n",
    "                (self.parameter_end / self.parameter_start) ^\n",
    "                (1 / (total_episodes * decaying_episodes))\n",
    "            )\n",
    "\n",
    "    def decay(self, parameter):\n",
    "        return max(\n",
    "            self.parameter_end, parameter * self.parameter_decay\n",
    "        )\n",
    "\n",
    "\n",
    "PARAMETER_DECAYS = {\n",
    "    \"none\": NullParameterDecay,\n",
    "    \"linear\": LinearParameterDecay,\n",
    "    \"exponential\": ExponentialParameterDecay\n",
    "}\n",
    "\n",
    "\n",
    "class ActionSelector:\n",
    "\n",
    "    def __init__(self, decay_schedule):\n",
    "        assert isinstance(decay_schedule, ParameterDecay)\n",
    "        self.decay_schedule = decay_schedule\n",
    "\n",
    "    def select(self, actions, legal_actions=None, training=False):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def select_many(self, actions, moving_agents, legal_actions, training=False):\n",
    "        assert len(moving_agents.shape) == 1\n",
    "        assert len(legal_actions.shape) == 2\n",
    "        assert len(actions.shape) == 2\n",
    "        assert moving_agents.shape[0] == legal_actions.shape[0] == actions.shape[0]\n",
    "        assert legal_actions.shape[1] == actions.shape[1]\n",
    "        num_agents = moving_agents.shape[0]\n",
    "        choices = np.full((num_agents,), -1)\n",
    "        is_best = np.full((num_agents,), False)\n",
    "        for handle in range(num_agents):\n",
    "            if moving_agents[handle]:\n",
    "                choices[handle], is_best[handle] = self.select(\n",
    "                    actions[handle], legal_actions[handle], training=training\n",
    "                )\n",
    "        return choices, is_best\n",
    "\n",
    "    def decay(self):\n",
    "        return None\n",
    "\n",
    "    def reset(self):\n",
    "        return None\n",
    "\n",
    "    def get_parameter(self):\n",
    "        return None\n",
    "\n",
    "\n",
    "class EpsilonGreedyActionSelector(ActionSelector):\n",
    "\n",
    "    def __init__(self, decay_schedule):\n",
    "        super(EpsilonGreedyActionSelector, self).__init__(decay_schedule)\n",
    "        self.epsilon = decay_schedule.parameter_start\n",
    "\n",
    "    def select(self, actions, legal_actions=None, training=False):\n",
    "        if legal_actions is None:\n",
    "            legal_actions = np.ones_like(actions, dtype=bool)\n",
    "        max_action = masked_argmax(actions, legal_actions, dim=0)\n",
    "        if not training or random.random() > self.epsilon:\n",
    "            return max_action, True\n",
    "        random_action = np.random.choice(\n",
    "            np.arange(actions.size)[legal_actions]\n",
    "        )\n",
    "        return (random_action, max_action == random_action)\n",
    "\n",
    "    def decay(self):\n",
    "        self.epsilon = self.decay_schedule.decay(self.epsilon)\n",
    "\n",
    "    def reset(self):\n",
    "        self.epsilon = self.decay_schedule.parameter_start\n",
    "\n",
    "    def get_parameter(self):\n",
    "        return self.epsilon\n",
    "\n",
    "\n",
    "class RandomActionSelector(EpsilonGreedyActionSelector):\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        super(RandomActionSelector, self).__init__(\n",
    "            NullParameterDecay(parameter_start=1)\n",
    "        )\n",
    "\n",
    "\n",
    "class GreedyActionSelector(EpsilonGreedyActionSelector):\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        super(GreedyActionSelector, self).__init__(\n",
    "            NullParameterDecay(parameter_start=0)\n",
    "        )\n",
    "\n",
    "\n",
    "class BoltzmannActionSelector(ActionSelector):\n",
    "\n",
    "    def __init__(self, decay_schedule):\n",
    "        super(BoltzmannActionSelector, self).__init__(decay_schedule)\n",
    "        self.temperature = decay_schedule.parameter_start\n",
    "\n",
    "    def select(self, actions, legal_actions=None, training=False):\n",
    "        if legal_actions is None:\n",
    "            legal_actions = np.ones_like(actions, dtype=bool)\n",
    "        max_action = masked_argmax(actions, legal_actions, dim=0)\n",
    "        if not training:\n",
    "            return max_action, True\n",
    "        dist = masked_softmax(\n",
    "            actions, legal_actions, dim=0, temperature=self.temperature\n",
    "        )\n",
    "        random_action = np.random.choice(\n",
    "            np.arange(actions.size)[legal_actions], p=dist[legal_actions]\n",
    "        )\n",
    "        is_equal = max_action == random_action\n",
    "        return random_action, is_equal\n",
    "\n",
    "    def decay(self):\n",
    "        self.temperature = self.decay_schedule.decay(self.temperature)\n",
    "\n",
    "    def reset(self):\n",
    "        self.temperature = self.decay_schedule.parameter_start\n",
    "\n",
    "    def get_parameter(self):\n",
    "        return self.temperature\n",
    "\n",
    "\n",
    "class CategoricalActionSelector(BoltzmannActionSelector):\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        super(CategoricalActionSelector, self).__init__(\n",
    "            NullParameterDecay(parameter_start=1)\n",
    "        )\n",
    "\n",
    "\n",
    "ACTION_SELECTORS = {\n",
    "    \"eps_greedy\":  EpsilonGreedyActionSelector,\n",
    "    \"random\": RandomActionSelector,\n",
    "    \"greedy\": GreedyActionSelector,\n",
    "    \"boltzmann\": BoltzmannActionSelector,\n",
    "    \"categorical\": CategoricalActionSelector\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\n",
    "    \"Experience\", field_names=[\n",
    "        \"state\", \"choice\", \"reward\", \"next_state\",\n",
    "        \"next_legal_choices\", \"finished\", \"moving\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    '''\n",
    "    Fixed-size buffer to store experience tuples\n",
    "    '''\n",
    "\n",
    "    def __init__(self, choice_size, batch_size, buffer_size, device):\n",
    "        '''\n",
    "        Initialize a ReplayBuffer object\n",
    "        '''\n",
    "        self.choice_size = choice_size\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.device = device\n",
    "\n",
    "    def add(self, experience):\n",
    "        '''\n",
    "        Add a new experience to memory\n",
    "        '''\n",
    "        self.memory.append(Experience(*experience))\n",
    "\n",
    "    def sample(self):\n",
    "        '''\n",
    "        Randomly sample a batch of experiences from memory.\n",
    "        Each returned tensor has shape (batch_size, *)\n",
    "        '''\n",
    "        states, choices, rewards, next_states, next_legal_choices, finished, moving = zip(\n",
    "            *random.sample(self.memory, k=self.batch_size)\n",
    "        )\n",
    "\n",
    "        # Check for PyTorch Geometric\n",
    "        if isinstance(states[0], np.ndarray):\n",
    "            states = torch.tensor(\n",
    "                states, dtype=torch.float32, device=self.device\n",
    "            )\n",
    "            next_states = torch.tensor(\n",
    "                next_states, dtype=torch.float32, device=self.device\n",
    "            )\n",
    "        elif isinstance(states[0], Data):\n",
    "            states = Batch.from_data_list(states).to(self.device)\n",
    "            next_states = Batch.from_data_list(next_states).to(self.device)\n",
    "\n",
    "        choices = torch.tensor(\n",
    "            choices, dtype=torch.int64, device=self.device\n",
    "        )\n",
    "        rewards = torch.tensor(\n",
    "            rewards, dtype=torch.float32, device=self.device\n",
    "        )\n",
    "        next_legal_choices = torch.tensor(\n",
    "            next_legal_choices, dtype=torch.bool, device=self.device\n",
    "        )\n",
    "        finished = torch.tensor(\n",
    "            finished, dtype=torch.uint8, device=self.device\n",
    "        )\n",
    "        moving = torch.tensor(\n",
    "            moving, dtype=torch.bool, device=self.device\n",
    "        )\n",
    "\n",
    "        return states, choices, rewards, next_states, next_legal_choices, finished, moving\n",
    "\n",
    "    def can_sample(self):\n",
    "        '''\n",
    "        Check if there are enough samples in the replay buffer\n",
    "        '''\n",
    "        return len(self.memory) >= self.batch_size\n",
    "\n",
    "    def save(self, filename):\n",
    "        '''\n",
    "        Save the current replay buffer to a pickle file\n",
    "        '''\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(list(self.memory), f)\n",
    "\n",
    "    def load(self, filename):\n",
    "        '''\n",
    "        Load the current replay buffer from the given pickle file\n",
    "        '''\n",
    "        with open(filename, 'rb') as f:\n",
    "            self.memory = pickle.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Return the current size of internal memory\n",
    "        '''\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSSES = {\n",
    "    \"huber\": MaskedHuberLoss(),\n",
    "    \"mse\": MaskedMSELoss(),\n",
    "}\n",
    "\n",
    "class Policy:\n",
    "    '''\n",
    "    Policy abstract class\n",
    "    '''\n",
    "\n",
    "    def __init__(self, params=None, state_size=None, choice_size=None, choice_selector=None, training=False):\n",
    "        self.params = params\n",
    "        self.state_size = state_size\n",
    "        self.choice_size = choice_size\n",
    "        self.choice_selector = choice_selector\n",
    "        self.training = training\n",
    "\n",
    "    def act(self, state, legal_choices=None, training=False):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def step(self, experience):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def save(self, filename):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def load(self, filename):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class RandomPolicy(Policy):\n",
    "    '''\n",
    "    Policy which chooses random moves\n",
    "    '''\n",
    "\n",
    "    def __init__(self, params=None, state_size=None, choice_selector=None, training=False):\n",
    "        super(RandomPolicy, self).__init__(\n",
    "            params, state_size, choice_size=RailEnvChoices.choice_size(),\n",
    "            choice_selector=RandomActionSelector(), training=training\n",
    "        )\n",
    "\n",
    "    def act(self, states, legal_choices, moving_agents, training=False):\n",
    "        choice_values = np.zeros((moving_agents.shape[0], self.choice_size))\n",
    "        return self.choice_selector.select_many(\n",
    "            choice_values, moving_agents, np.array(legal_choices),\n",
    "            training=(training and self.training)\n",
    "        )\n",
    "\n",
    "    def step(self, experience):\n",
    "        return None\n",
    "\n",
    "    def save(self, filename):\n",
    "        return None\n",
    "\n",
    "    def load(self, filename):\n",
    "        return None\n",
    "\n",
    "\n",
    "class DQNPolicy(Policy):\n",
    "    '''\n",
    "    DQN policy\n",
    "    '''\n",
    "\n",
    "    def __init__(self, params, state_size, choice_selector, training=False):\n",
    "        '''\n",
    "        Initialize DQNPolicy object\n",
    "        '''\n",
    "        super(DQNPolicy, self).__init__(\n",
    "            params, state_size, choice_size=RailEnvChoices.choice_size(),\n",
    "            choice_selector=choice_selector, training=training\n",
    "        )\n",
    "        assert isinstance(\n",
    "            choice_selector, ActionSelector\n",
    "        ), \"The choice selection object must be an instance of ActionSelector\"\n",
    "\n",
    "        # Parameters\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        if self.params.generic.use_gpu and torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda:0\")\n",
    "            print(\"🐇 Using GPU\")\n",
    "\n",
    "        # Q-Network\n",
    "        net = DuelingDQN if self.params.model.dqn.dueling.enabled else DQN\n",
    "        self.qnetwork_local = net(\n",
    "            self.state_size, RailEnvChoices.choice_size(),\n",
    "            self.params.model.dqn, device=self.device\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Training parameters\n",
    "        if self.training:\n",
    "            self.time_step = 0\n",
    "            self.qnetwork_target = copy.deepcopy(self.qnetwork_local)\n",
    "            self.optimizer = optim.Adam(\n",
    "                self.qnetwork_local.parameters(), lr=self.params.learning.learning_rate\n",
    "            )\n",
    "            self.criterion = LOSSES[self.params.learning.loss.get_true_key()]\n",
    "            self.loss = torch.tensor(0.0)\n",
    "            self.memory = ReplayBuffer(\n",
    "                RailEnvChoices.choice_size(), self.params.replay_buffer.batch_size,\n",
    "                self.params.replay_buffer.size, self.device\n",
    "            )\n",
    "\n",
    "    def enable_wandb(self):\n",
    "        '''\n",
    "        Log gradients and parameters to wandb\n",
    "        '''\n",
    "        wandb.watch(\n",
    "            self.qnetwork_local, self.criterion,\n",
    "            log=\"all\", log_freq=self.params.generic.wandb_gradients.checkpoint\n",
    "        )\n",
    "\n",
    "    def act(self, states, legal_choices, moving_agents, training=False):\n",
    "        '''\n",
    "        Perform action selection based on the Q-values returned by the network\n",
    "        '''\n",
    "        choice_values = np.zeros((moving_agents.shape[0], self.choice_size),)\n",
    "        # If no agent is active, then skip computations\n",
    "        if moving_agents.any():\n",
    "\n",
    "            # Add 1 dimension to state to simulate a mini-batch of size 1\n",
    "            if self.params.policy.type.decentralized_fov:\n",
    "                states = Batch.from_data_list([states[0]]).to(self.device)\n",
    "            elif self.params.policy.type.graph:\n",
    "                states = Batch.from_data_list(states).to(self.device)\n",
    "            else:\n",
    "                states = torch.tensor(\n",
    "                    states, dtype=torch.float, device=self.device\n",
    "                )\n",
    "\n",
    "            # Convert moving agents to tensor\n",
    "            t_moving_agents = torch.from_numpy(\n",
    "                moving_agents\n",
    "            ).bool().to(self.device)\n",
    "\n",
    "            # Call the network\n",
    "            self.qnetwork_local.eval()\n",
    "            with torch.no_grad():\n",
    "                choice_values = self.qnetwork_local(\n",
    "                    states, mask=t_moving_agents\n",
    "                ).detach().cpu().numpy()\n",
    "\n",
    "        # Select a legal choice based on the action selector\n",
    "        return self.choice_selector.select_many(\n",
    "            choice_values, moving_agents, np.array(legal_choices),\n",
    "            training=(training and self.training)\n",
    "        )\n",
    "\n",
    "    def step(self, experiences):\n",
    "        '''\n",
    "        Add an experience to memory and eventually perform a training step\n",
    "        '''\n",
    "        assert self.training, \"Policy has been initialized for evaluation only\"\n",
    "        for experience in experiences:\n",
    "            # Save experience in replay memory\n",
    "            self.memory.add(experience)\n",
    "\n",
    "            # Learn every `checkpoint` time steps\n",
    "            # (if enough samples are available in memory, get random subset and learn)\n",
    "            self.time_step = (\n",
    "                self.time_step + 1\n",
    "            ) % self.params.replay_buffer.checkpoint\n",
    "            if self.time_step == 0 and self.memory.can_sample():\n",
    "                self.qnetwork_local.train()\n",
    "                self._learn()\n",
    "\n",
    "    def _learn(self):\n",
    "        '''\n",
    "        Perform a learning step\n",
    "        '''\n",
    "        # Sample a batch of experiences\n",
    "        experiences = self.memory.sample()\n",
    "        states, choices, rewards, next_states, next_legal_choices, finished, moving = experiences\n",
    "\n",
    "        # Get expected Q-values from local model\n",
    "        q_expected = self.qnetwork_local(states, mask=moving).gather(\n",
    "            1, choices.flatten().unsqueeze(1)\n",
    "        ).squeeze(1)\n",
    "\n",
    "        # Get expected Q-values from target model\n",
    "        q_targets_next = torch.from_numpy(\n",
    "            self._get_q_targets_next(\n",
    "                next_states, next_legal_choices.cpu().numpy(), moving\n",
    "            )\n",
    "        ).squeeze(1).to(self.device)\n",
    "\n",
    "        # Compute Q-targets for current states\n",
    "        q_targets = (\n",
    "            torch.flatten(rewards) + (\n",
    "                self.params.learning.discount *\n",
    "                q_targets_next * (1 - torch.flatten(finished))\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Compute and minimize the loss\n",
    "        self.loss = self.criterion(q_expected, q_targets, mask=moving)\n",
    "        self.optimizer.zero_grad()\n",
    "        self.loss.backward()\n",
    "        if self.params.learning.gradient.clip_norm:\n",
    "            nn.utils.clip_grad.clip_grad_norm_(\n",
    "                self.qnetwork_local.parameters(), self.params.learning.gradient.max_norm\n",
    "            )\n",
    "        elif self.params.learning.gradient.clamp_values:\n",
    "            nn.utils.clip_grad.clip_grad_value_(\n",
    "                self.qnetwork_local.parameters(), self.params.learning.gradient.value_limit\n",
    "            )\n",
    "        self.optimizer.step()\n",
    "\n",
    "        '''\n",
    "        print(\n",
    "            q_expected.shape, \n",
    "            q_targets_next.shape, \n",
    "            q_targets.shape, \n",
    "            rewards.shape, \n",
    "            finished.shape, \n",
    "            self.loss\n",
    "        )\n",
    "        '''\n",
    "\n",
    "        # Update target network\n",
    "        self._soft_update(self.qnetwork_local, self.qnetwork_target)\n",
    "\n",
    "    def _get_q_targets_next(self, next_states, next_legal_choices, moving):\n",
    "        '''\n",
    "        Get expected Q-values from target network\n",
    "        '''\n",
    "\n",
    "        def _double_dqn():\n",
    "            q_targets_next = self.qnetwork_target(\n",
    "                next_states, mask=moving\n",
    "            ).detach().cpu().numpy()\n",
    "            q_locals_next = self.qnetwork_local(\n",
    "                next_states, mask=moving\n",
    "            ).detach().cpu().numpy()\n",
    "\n",
    "            # Softmax Bellman\n",
    "            if self.params.learning.softmax_bellman.enabled:\n",
    "                return np.sum(\n",
    "                    q_targets_next * masked_softmax(\n",
    "                        q_locals_next,\n",
    "                        next_legal_choices.reshape(q_locals_next.shape),\n",
    "                        temperature=self.params.learning.softmax_bellman.temperature\n",
    "                    ), axis=1, keepdims=True\n",
    "                )\n",
    "\n",
    "            # Standard Bellman\n",
    "            best_choices = masked_argmax(\n",
    "                q_locals_next,\n",
    "                next_legal_choices.reshape(q_locals_next.shape)\n",
    "            )\n",
    "            return np.take_along_axis(q_targets_next, best_choices, axis=1)\n",
    "\n",
    "        def _dqn():\n",
    "            q_targets_next = self.qnetwork_target(\n",
    "                next_states\n",
    "            ).detach().cpu().numpy()\n",
    "\n",
    "            # Standard or softmax Bellman\n",
    "            return (\n",
    "                masked_max(\n",
    "                    q_targets_next,\n",
    "                    next_legal_choices.reshape(q_targets_next.shape)\n",
    "                )\n",
    "                if not self.params.learning.softmax_bellman.enabled\n",
    "                else np.sum(\n",
    "                    q_targets_next * masked_softmax(\n",
    "                        q_targets_next,\n",
    "                        next_legal_choices.reshape(q_targets_next.shape),\n",
    "                        temperature=self.params.learning.softmax_bellman.temperature\n",
    "                    ), axis=1, keepdims=True\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return _double_dqn() if self.params.model.dqn.double else _dqn()\n",
    "\n",
    "    def _soft_update(self, local_model, target_model):\n",
    "        '''\n",
    "        Soft update model parameters: θ_target = τ * θ_local + (1 - τ) * θ_target\n",
    "        '''\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                self.params.learning.tau * local_param.data +\n",
    "                (1.0 - self.params.learning.tau) * target_param.data\n",
    "            )\n",
    "\n",
    "    def save(self, filename):\n",
    "        '''\n",
    "        Save both local and targets networks parameters\n",
    "        '''\n",
    "        torch.save(self.qnetwork_local.state_dict(), filename + \".local\")\n",
    "        torch.save(self.qnetwork_target.state_dict(), filename + \".target\")\n",
    "\n",
    "    def load(self, filename):\n",
    "        '''\n",
    "        Load only the local network if evaluating,\n",
    "        otherwise load both local and target networks\n",
    "        '''\n",
    "        if os.path.exists(filename + \".local\"):\n",
    "            self.qnetwork_local.load_state_dict(\n",
    "                torch.load(filename + \".local\", map_location=self.device)\n",
    "            )\n",
    "            if self.training and os.path.exists(filename + \".target\"):\n",
    "                self.qnetwork_target.load_state_dict(\n",
    "                    torch.load(filename + \".target\", map_location=self.device)\n",
    "                )\n",
    "        else:\n",
    "            print(\"Model not found. Please, check the given path.\")\n",
    "\n",
    "    def save_replay_buffer(self, filename):\n",
    "        '''\n",
    "        Save the current replay buffer\n",
    "        '''\n",
    "        self.memory.save(filename)\n",
    "\n",
    "    def load_replay_buffer(self, filename):\n",
    "        '''\n",
    "        Load a stored representation of the replay buffer\n",
    "        '''\n",
    "        self.memory.load(filename)\n",
    "\n",
    "\n",
    "class DQNGNNPolicy(DQNPolicy):\n",
    "    '''\n",
    "    DQN + GNN policy\n",
    "    '''\n",
    "\n",
    "    def __init__(self, params, state_size, choice_selector, training=False):\n",
    "        '''\n",
    "        Initialize DQNGNNPolicy object\n",
    "        '''\n",
    "        super(DQNGNNPolicy, self).__init__(\n",
    "            params, (\n",
    "                params.model.entire_gnn.embedding_size *\n",
    "                params.model.entire_gnn.pos_size\n",
    "            ),\n",
    "            choice_selector, training=training\n",
    "        )\n",
    "\n",
    "        self.qnetwork_local = Sequential(\n",
    "            EntireGNN(\n",
    "                state_size, self.params.observator.max_depth,\n",
    "                self.params.model.entire_gnn, device=self.device\n",
    "            ).to(self.device),\n",
    "            self.qnetwork_local\n",
    "        )\n",
    "\n",
    "        if training:\n",
    "            self.qnetwork_target = copy.deepcopy(self.qnetwork_local)\n",
    "\n",
    "\n",
    "class DecentralizedFOVDQNPolicy(DQNPolicy):\n",
    "    '''\n",
    "    Decentralized FOV CNN + GNN + DQN policy\n",
    "    '''\n",
    "\n",
    "    def __init__(self, params, state_size, choice_selector, training=False):\n",
    "        '''\n",
    "        Initialize MultiAgentDQNGNNPolicy object\n",
    "        '''\n",
    "        super(DecentralizedFOVDQNPolicy, self).__init__(\n",
    "            params, params.model.multi_gnn.gnn_communication.embedding_size,\n",
    "            choice_selector, training=training\n",
    "        )\n",
    "\n",
    "        self.qnetwork_local = Sequential(\n",
    "            MultiGNN(\n",
    "                self.params.observator.max_depth,\n",
    "                self.params.observator.max_depth,\n",
    "                state_size, self.params.model.multi_gnn,\n",
    "                device=self.device\n",
    "            ).to(self.device),\n",
    "            self.qnetwork_local\n",
    "        )\n",
    "\n",
    "        if training:\n",
    "            self.qnetwork_target = copy.deepcopy(self.qnetwork_local)\n",
    "\n",
    "\n",
    "POLICIES = {\n",
    "    \"tree\": DQNPolicy,\n",
    "    \"binary_tree\": DQNPolicy,\n",
    "    \"graph\": DQNGNNPolicy,\n",
    "    \"decentralized_fov\": DecentralizedFOVDQNPolicy,\n",
    "    \"random\": RandomPolicy\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Replay buffer status: 0/100000 experiences\n",
      "\n",
      "🚉 Starting training \t Training 7 trains on 48x27 grid for 7500 episodes \tEvaluating on 20 episodes every 500 episodes\n",
      "\n",
      "🧠 Model with training id 20240310-222743\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 494\u001b[0m\n\u001b[0;32m    490\u001b[0m     writer\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 494\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[68], line 489\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    487\u001b[0m writer \u001b[38;5;241m=\u001b[39m SummaryWriter()\n\u001b[0;32m    488\u001b[0m args \u001b[38;5;241m=\u001b[39m Struct(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m--> 489\u001b[0m \u001b[43mtrain_agents\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    490\u001b[0m writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn[68], line 160\u001b[0m, in \u001b[0;36mtrain_agents\u001b[1;34m(args, writer)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# Initialize data structures\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnum_trains):\n\u001b[1;32m--> 160\u001b[0m     legal_actions[handle] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrailway_encoding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_agent_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m     legal_choices[handle] \u001b[38;5;241m=\u001b[39m train_env\u001b[38;5;241m.\u001b[39mrailway_encoding\u001b[38;5;241m.\u001b[39mget_legal_choices(handle, legal_actions[handle])\n\u001b[0;32m    162\u001b[0m     choice_dict\u001b[38;5;241m.\u001b[39mupdate({handle: RailEnvChoices\u001b[38;5;241m.\u001b[39mCHOICE_LEFT\u001b[38;5;241m.\u001b[39mvalue})\n",
      "Cell \u001b[1;32mIn[66], line 1072\u001b[0m, in \u001b[0;36mCellOrientationGraph.get_agent_actions\u001b[1;34m(self, handle)\u001b[0m\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_agent_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, handle):\n\u001b[0;32m   1069\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;124;03m    Return all the possible active actions that an agent can perform\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m-> 1072\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_actions(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_agent_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[66], line 844\u001b[0m, in \u001b[0;36mCellOrientationGraph.get_agent_cell\u001b[1;34m(self, handle)\u001b[0m\n\u001b[0;32m    839\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    840\u001b[0m \u001b[38;5;124;03mReturn the unpacked graph node in which the agent\u001b[39;00m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;124;03midentified by the given handle is\u001b[39;00m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    843\u001b[0m position \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 844\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magents\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TrainState\u001b[38;5;241m.\u001b[39mREADY_TO_DEPART:\n\u001b[0;32m    846\u001b[0m     position \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    847\u001b[0m         agent\u001b[38;5;241m.\u001b[39minitial_position[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    848\u001b[0m         agent\u001b[38;5;241m.\u001b[39minitial_position[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m    849\u001b[0m         agent\u001b[38;5;241m.\u001b[39minitial_direction\n\u001b[0;32m    850\u001b[0m     )\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "def tensorboard_log(writer, name, x, y, plot=['min', 'max', 'mean', 'std', 'hist']):\n",
    "    '''\n",
    "    Log the given x/y values to tensorboard\n",
    "    '''\n",
    "    if not isinstance(x, np.ndarray) and not isinstance(x, list):\n",
    "        writer.add_scalar(name, x, y)\n",
    "    else:\n",
    "        if ((isinstance(x, list) and len(x) == 0) or\n",
    "                (isinstance(x, np.ndarray) and x.size == 0)):\n",
    "            return\n",
    "        if 'min' in plot:\n",
    "            writer.add_scalar(f\"{name}_min\", np.min(x), y)\n",
    "        if 'max' in plot:\n",
    "            writer.add_scalar(f\"{name}_max\", np.max(x), y)\n",
    "        if 'mean' in plot:\n",
    "            writer.add_scalar(f\"{name}_mean\", np.mean(x), y)\n",
    "        if 'std' in plot:\n",
    "            writer.add_scalar(f\"{name}_std\", np.std(x), y)\n",
    "        if 'hist' in plot:\n",
    "            writer.add_histogram(name, np.array(x), y)\n",
    "\n",
    "\n",
    "def format_choices_probabilities(choices_probabilities):\n",
    "    '''\n",
    "    Helper function to pretty print choices probabilities\n",
    "    '''\n",
    "    choices_probabilities = np.round(choices_probabilities, 3)\n",
    "    choices = [\"←\", \"→\", \"◼\"]\n",
    "\n",
    "    buffer = \"\"\n",
    "    for choice, choice_prob in zip(choices, choices_probabilities):\n",
    "        buffer += choice + \" \" + \"{:^4.2%}\".format(choice_prob) + \" \"\n",
    "\n",
    "    return buffer\n",
    "\n",
    "\n",
    "def train_agents(args, writer):\n",
    "    '''\n",
    "    Train and evaluate agents on the specified environments\n",
    "    '''\n",
    "    # Initialize threads and seeds\n",
    "    set_num_threads(args.generic.num_threads)\n",
    "    if args.generic.fix_random:\n",
    "        fix_random(args.generic.random_seed)\n",
    "\n",
    "    # Setup the environments\n",
    "    train_env = create_rail_env(\n",
    "        args, load_env=args.training.train_env.load\n",
    "    )\n",
    "    eval_env = create_rail_env(\n",
    "        args, load_env=args.training.eval_env.load\n",
    "    )\n",
    "\n",
    "    # Define \"static\" random seeds for evaluation purposes\n",
    "    eval_seeds = [args.env.seed] * args.training.eval_env.episodes\n",
    "    if args.training.eval_env.all_random:\n",
    "        eval_seeds = [\n",
    "            get_seed(eval_env)\n",
    "            for e in range(args.training.eval_env.episodes)\n",
    "        ]\n",
    "\n",
    "    # Pick action selector and parameter decay\n",
    "    pd_type = args.parameter_decay.type.get_true_key()\n",
    "    parameter_decay = PARAMETER_DECAYS[pd_type](\n",
    "        parameter_start=args.parameter_decay.start,\n",
    "        parameter_end=args.parameter_decay.end,\n",
    "        total_episodes=args.training.train_env.episodes,\n",
    "        decaying_episodes=args.parameter_decay.decaying_episodes\n",
    "    )\n",
    "    as_type = args.action_selector.type.get_true_key()\n",
    "    action_selector = ACTION_SELECTORS[as_type](parameter_decay)\n",
    "\n",
    "    # Initialize the agents policy\n",
    "    policy_type = args.policy.type.get_true_key()\n",
    "    policy = POLICIES[policy_type](\n",
    "        args, train_env.state_size, action_selector, training=True\n",
    "    )\n",
    "    if args.generic.enable_wandb and args.generic.wandb_gradients.enabled:\n",
    "        policy.enable_wandb()\n",
    "\n",
    "    # Handle replay buffer\n",
    "    if args.replay_buffer.load:\n",
    "        try:\n",
    "            policy.load_replay_buffer(args.replay_buffer.load)\n",
    "        except RuntimeError as e:\n",
    "            print(\n",
    "                \"\\n🛑 Could't load replay buffer, were the experiences generated using the same depth?\"\n",
    "            )\n",
    "            print(e)\n",
    "            exit(1)\n",
    "    print(\"\\n💾 Replay buffer status: {}/{} experiences\".format(\n",
    "        len(policy.memory), args.replay_buffer.size\n",
    "    ))\n",
    "\n",
    "    # Set the unique ID for this training\n",
    "    now = datetime.now()\n",
    "    training_id = now.strftime('%Y%m%d-%H%M%S')\n",
    "    if args.training.renderer.training and args.training.renderer.save_frames:\n",
    "        frames_dir = f\"tmp/frames/{training_id}\"\n",
    "        os.makedirs(frames_dir, exist_ok=True)\n",
    "\n",
    "    # Print initial training info\n",
    "    training_timer = Timer()\n",
    "    training_timer.start()\n",
    "    print(\"\\n🚉 Starting training \\t Training {} trains on {}x{} grid for {} episodes \\tEvaluating on {} episodes every {} episodes\".format(\n",
    "        args.env.num_trains,\n",
    "        args.env.width, args.env.height,\n",
    "        args.training.train_env.episodes,\n",
    "        args.training.eval_env.episodes,\n",
    "        args.training.checkpoint\n",
    "    ))\n",
    "    print(f\"\\n🧠 Model with training id {training_id}\\n\")\n",
    "\n",
    "    # Do the specified number of episodes\n",
    "    scores, custom_scores, completions, steps, deadlocks = [], [], [], [], []\n",
    "    choices_taken = np.zeros((args.training.train_env.episodes + 1,))\n",
    "    for episode in range(args.training.train_env.episodes + 1):\n",
    "\n",
    "        # Initialize timers\n",
    "        step_timer = Timer()\n",
    "        reset_timer = Timer()\n",
    "        learn_timer = Timer()\n",
    "        inference_timer = Timer()\n",
    "\n",
    "        # Reset environment and renderer\n",
    "        reset_timer.start()\n",
    "        if not args.training.train_env.all_random:\n",
    "            obs, info = train_env.reset(random_seed=args.env.seed)\n",
    "        else:\n",
    "            obs, info = train_env.reset(\n",
    "                regenerate_rail=True, regenerate_schedule=True\n",
    "            )\n",
    "        reset_timer.end()\n",
    "        if args.training.renderer.training and episode % args.training.renderer.train_checkpoint == 0:\n",
    "            env_renderer = train_env.get_renderer()\n",
    "\n",
    "        # Compute agents with same source\n",
    "        agents_with_same_start = train_env.get_agents_same_start()\n",
    "\n",
    "        # Create data structures for training info\n",
    "        score, custom_score, final_step = 0.0, 0.0, 0\n",
    "        choices_count = np.zeros((RailEnvChoices.choice_size(),))\n",
    "        num_exploration_choices = np.zeros_like(choices_count)\n",
    "        legal_choices = np.full(\n",
    "            (args.env.num_trains, RailEnvChoices.choice_size()),\n",
    "            RailEnvChoices.default_choices()\n",
    "        )\n",
    "        legal_actions = np.full(\n",
    "            (args.env.num_trains, get_num_actions()), False\n",
    "        )\n",
    "        moving_agents = np.full((args.env.num_trains,), False)\n",
    "        action_dict, choice_dict = dict(), dict()\n",
    "        prev_obs, prev_choices = dict(), dict()\n",
    "\n",
    "        # Initialize data structures\n",
    "        for handle in range(args.env.num_trains):\n",
    "            legal_actions[handle] = train_env.railway_encoding.get_agent_actions(handle)\n",
    "            legal_choices[handle] = train_env.railway_encoding.get_legal_choices(handle, legal_actions[handle])\n",
    "            choice_dict.update({handle: RailEnvChoices.CHOICE_LEFT.value})\n",
    "            \n",
    "            if obs[handle] is not None:\n",
    "                prev_obs[handle] = copy_obs(obs[handle])\n",
    "\n",
    "        # Update initial previous choices based on the policy type\n",
    "        for handle in range(args.env.num_trains):\n",
    "            if args.policy.type.decentralized_fov:\n",
    "                prev_choices[handle] = dict(choice_dict)\n",
    "            else:\n",
    "                prev_choices[handle] = choice_dict[handle]\n",
    "\n",
    "        # Do an episode\n",
    "        for step in range(train_env._max_episode_steps):\n",
    "\n",
    "            # Prioritize entry of faster agents in the environment\n",
    "            for position in agents_with_same_start:\n",
    "                if len(agents_with_same_start[position]) > 0:\n",
    "                    del agents_with_same_start[position][0]\n",
    "                    for agent in agents_with_same_start[position]:\n",
    "                        info['action_required'][agent] = False\n",
    "\n",
    "            # Policy act\n",
    "            inference_timer.start()\n",
    "            legal_actions, legal_choices, moving_agents = train_env.pre_act()\n",
    "            choices, is_best = policy.act(\n",
    "                list(obs.values()), legal_choices,\n",
    "                moving_agents, training=True\n",
    "            )\n",
    "\n",
    "            # Update training info after policy act\n",
    "            action_dict, metadata = train_env.post_act(\n",
    "                choices, is_best, legal_actions, moving_agents\n",
    "            )\n",
    "            current_choices_count = metadata['choices_count']\n",
    "            choices_count += current_choices_count\n",
    "            choices_taken[episode] += np.sum(current_choices_count)\n",
    "            num_exploration_choices += metadata['num_exploration_choices']\n",
    "            choice_dict.update(metadata['choice_dict'])\n",
    "            inference_timer.end()\n",
    "\n",
    "            # Environment step\n",
    "            step_timer.start()\n",
    "            next_obs, rewards, custom_rewards, done, info = train_env.step(\n",
    "                action_dict\n",
    "            )\n",
    "            step_timer.end()\n",
    "\n",
    "            # Render an episode at some interval\n",
    "            if args.training.renderer.training and episode % args.training.renderer.train_checkpoint == 0:\n",
    "                env_renderer.render_env(\n",
    "                    show=True, show_observations=False, show_predictions=True, show_rowcols=True\n",
    "                )\n",
    "                # Save renderer frame\n",
    "                if args.training.renderer.save_frames:\n",
    "                    env_renderer.gl.save_image(\n",
    "                        \"{:s}/{:04d}.png\".format(frames_dir, step)\n",
    "                    )\n",
    "\n",
    "            # Policy step\n",
    "            learn_timer.start()\n",
    "            experience = (\n",
    "                prev_obs,\n",
    "                prev_choices,\n",
    "                custom_rewards,\n",
    "                obs,\n",
    "                legal_choices,\n",
    "                moving_agents\n",
    "            )\n",
    "            experiences = train_env.pre_step(experience)\n",
    "            policy.step(experiences)\n",
    "            learn_timer.end()\n",
    "\n",
    "            # Update training info after policy step\n",
    "            metadata = train_env.post_step(\n",
    "                obs, choice_dict, next_obs,\n",
    "                moving_agents, rewards, custom_rewards\n",
    "            )\n",
    "            obs.update(metadata['obs'])\n",
    "            prev_obs.update(metadata['prev_obs'])\n",
    "            prev_choices.update(metadata['prev_choices'])\n",
    "            score += metadata['score']\n",
    "            custom_score += metadata['custom_score']\n",
    "\n",
    "            # Break if every agent arrived\n",
    "            final_step = step\n",
    "            if done['__all__'] or train_env.check_if_all_blocked(info[\"deadlocks\"]):\n",
    "                break\n",
    "\n",
    "        # Close window\n",
    "        if args.training.renderer.training and episode % args.training.renderer.train_checkpoint == 0:\n",
    "            env_renderer.close_window()\n",
    "\n",
    "        # Parameter decay\n",
    "        policy.choice_selector.decay()\n",
    "\n",
    "        # Save final scores\n",
    "        scores.append(\n",
    "            score / (\n",
    "                train_env._max_episode_steps *\n",
    "                train_env.get_num_agents()\n",
    "            )\n",
    "        )\n",
    "        custom_scores.append(custom_score / train_env.get_num_agents())\n",
    "        completions.append(\n",
    "            sum(done[idx] for idx in train_env.get_agent_handles()) /\n",
    "            train_env.get_num_agents()\n",
    "        )\n",
    "        steps.append(final_step)\n",
    "        deadlocks.append(\n",
    "            sum(int(v) for v in info[\"deadlocks\"].values()) /\n",
    "            train_env.get_num_agents()\n",
    "        )\n",
    "        choices_probs = choices_count / np.sum(choices_count)\n",
    "\n",
    "        # Save model and replay buffer at checkpoint\n",
    "        if episode % args.training.checkpoint == 0:\n",
    "            policy.save(f'./checkpoints/{training_id}-{episode}')\n",
    "\n",
    "            # Save replay buffer\n",
    "            if args.replay_buffer.save:\n",
    "                policy.save_replay_buffer(\n",
    "                    f'./replay_buffers/{training_id}-{episode}.pkl'\n",
    "                )\n",
    "\n",
    "        # Print episode info\n",
    "        print(\n",
    "            '\\r🚂 Episode {:4n}'\n",
    "            '\\t 🏆 Score: {:<+5.4f}'\n",
    "            ' Avg: {:>+5.4f}'\n",
    "            '\\t 🏅 Custom score: {:<+5.4f}'\n",
    "            ' Avg: {:>+5.4f}'\n",
    "            '\\t 💯 Done: {:<7.2%}'\n",
    "            ' Avg: {:>7.2%}'\n",
    "            '\\t 💀 Deadlocks: {:<7.2%}'\n",
    "            ' Avg: {:>7.2%}'\n",
    "            '\\t 🦶 Steps: {:4n}/{:4n}'\n",
    "            '\\t 🎲 Exploration prob: {:4.3f} '\n",
    "            '\\t 🤔 Choices: {:4n}'\n",
    "            '\\t 🤠 Exploration: {:3n}'\n",
    "            '\\t 🔀 Choices probs: {:^}'.format(\n",
    "                episode,\n",
    "                scores[-1],\n",
    "                np.mean(scores),\n",
    "                custom_scores[-1],\n",
    "                np.mean(custom_scores),\n",
    "                completions[-1],\n",
    "                np.mean(completions),\n",
    "                deadlocks[-1],\n",
    "                np.mean(deadlocks),\n",
    "                steps[-1],\n",
    "                train_env._max_episode_steps,\n",
    "                policy.choice_selector.get_parameter(),\n",
    "                choices_taken[episode],\n",
    "                np.sum(num_exploration_choices),\n",
    "                format_choices_probabilities(choices_probs)\n",
    "            ), end=\"\\n\"\n",
    "        )\n",
    "\n",
    "        # Evaluate policy and log results at some interval\n",
    "        # (always evaluate the final episode)\n",
    "        if (args.training.eval_env.episodes > 0 and\n",
    "            ((episode > 0 and episode % args.training.checkpoint == 0) or\n",
    "             (episode == args.training.train_env.episodes))):\n",
    "            eval_policy(args, writer, eval_env, policy, eval_seeds, episode)\n",
    "\n",
    "    # Print final training info\n",
    "    print(\"\\n\\r🏁 Training ended \\tTrained {} trains on {}x{} grid for {} episodes \\t Evaluated on {} episodes every {} episodes\".format(\n",
    "        args.env.num_trains,\n",
    "        args.env.width, args.env.height,\n",
    "        args.training.train_env.episodes,\n",
    "        args.training.eval_env.episodes,\n",
    "        args.training.checkpoint\n",
    "    ))\n",
    "    print(\n",
    "        f\"\\n💾 Replay buffer status: {len(policy.memory)}/{args.replay_buffer.size} experiences\"\n",
    "    )\n",
    "\n",
    "    # Save trained models\n",
    "    print(f\"\\n🧠 Saving model with training id {training_id}\")\n",
    "    policy.save(f'./checkpoints/{training_id}-latest')\n",
    "    if args.generic.enable_wandb:\n",
    "        wandb.save(f'./checkpoints/{training_id}-latest.local')\n",
    "    if args.replay_buffer.save:\n",
    "        policy.save_replay_buffer(\n",
    "            f'./replay_buffers/{training_id}-latest.pkl'\n",
    "        )\n",
    "\n",
    "\n",
    "def eval_policy(args, writer, env, policy, eval_seeds, train_episode):\n",
    "    '''\n",
    "    Perform a validation round with the given policy\n",
    "    in the specified environment\n",
    "    '''\n",
    "    choices_taken = np.zeros((len(eval_seeds),))\n",
    "    scores, custom_scores, completions, steps, deadlocks = [], [], [], [], []\n",
    "\n",
    "    # Do the specified number of episodes\n",
    "    print('\\nStarting validation:')\n",
    "    for episode, seed in enumerate(eval_seeds):\n",
    "        score, custom_score, final_step = 0.0, 0.0, 0\n",
    "\n",
    "        # Reset environment and renderer\n",
    "        if not args.training.eval_env.all_random:\n",
    "            obs, info = env.reset(random_seed=seed)\n",
    "        else:\n",
    "            obs, info = env.reset(\n",
    "                regenerate_rail=True, regenerate_schedule=True,\n",
    "            )\n",
    "        if args.training.renderer.evaluation and episode % args.training.renderer.eval_checkpoint == 0:\n",
    "            env_renderer = env.get_renderer()\n",
    "\n",
    "        # Compute agents with same source\n",
    "        agents_with_same_start = env.get_agents_same_start()\n",
    "\n",
    "        # Do an episode\n",
    "        for step in range(env._max_episode_steps):\n",
    "\n",
    "            # Prioritize enter of faster agent in the environment\n",
    "            for position in agents_with_same_start:\n",
    "                if len(agents_with_same_start[position]) > 0:\n",
    "                    del agents_with_same_start[position][0]\n",
    "                    for agent in agents_with_same_start[position]:\n",
    "                        info['action_required'][agent] = False\n",
    "\n",
    "            # Policy act\n",
    "            legal_actions, legal_choices, moving_agents = env.pre_act()\n",
    "            choices, is_best = policy.act(\n",
    "                list(obs.values()), legal_choices,\n",
    "                moving_agents, training=False\n",
    "            )\n",
    "            action_dict, metadata = env.post_act(\n",
    "                choices, is_best, legal_actions, moving_agents\n",
    "            )\n",
    "            current_choices_count = metadata['choices_count']\n",
    "            choices_taken[episode] += np.sum(current_choices_count)\n",
    "\n",
    "            # Environment step\n",
    "            obs, rewards, custom_rewards, done, info = env.step(\n",
    "                action_dict\n",
    "            )\n",
    "\n",
    "            # Render an episode at some interval\n",
    "            if args.training.renderer.evaluation and episode % args.training.renderer.eval_checkpoint == 0:\n",
    "                env_renderer.render_env(\n",
    "                    show=True, show_observations=False, show_predictions=True, show_rowcols=True\n",
    "                )\n",
    "\n",
    "            # Update agents scores\n",
    "            for agent in env.get_agent_handles():\n",
    "                score += rewards[agent]\n",
    "                custom_score += custom_rewards[agent]\n",
    "\n",
    "            # Break if every agent arrived\n",
    "            final_step = step\n",
    "            if done['__all__'] or env.check_if_all_blocked(info[\"deadlocks\"]):\n",
    "                break\n",
    "\n",
    "        # Close window\n",
    "        if args.training.renderer.evaluation and episode % args.training.renderer.eval_checkpoint == 0:\n",
    "            env_renderer.close_window()\n",
    "\n",
    "        # Save final scores\n",
    "        scores.append(\n",
    "            score / (\n",
    "                env._max_episode_steps *\n",
    "                env.get_num_agents()\n",
    "            )\n",
    "        )\n",
    "        custom_scores.append(custom_score / env.get_num_agents())\n",
    "        completions.append(\n",
    "            sum(done[idx] for idx in env.get_agent_handles()) /\n",
    "            env.get_num_agents()\n",
    "        )\n",
    "        steps.append(final_step)\n",
    "        deadlocks.append(\n",
    "            sum(int(v) for v in info[\"deadlocks\"].values()) /\n",
    "            env.get_num_agents()\n",
    "        )\n",
    "\n",
    "        # Print evaluation results on one episode\n",
    "        print(\n",
    "            '\\r🚂 Validation {:3n}'\n",
    "            '\\t 🏆 Score: {:+5.4f}'\n",
    "            '\\t 🏅 Custom score: {:+5.4f}'\n",
    "            '\\t 💯 Done: {:7.2%}'\n",
    "            '\\t 💀 Deadlocks: {:7.2%}'\n",
    "            '\\t 🦶 Steps: {:4n}/{:4n}'\n",
    "            '\\t 🤔 Choices: {:4n}'.format(\n",
    "                episode,\n",
    "                scores[-1],\n",
    "                custom_scores[-1],\n",
    "                completions[-1],\n",
    "                deadlocks[-1],\n",
    "                steps[-1],\n",
    "                env._max_episode_steps,\n",
    "                choices_taken[episode]\n",
    "            ), end=\"\\n\"\n",
    "        )\n",
    "\n",
    "    # Print validation results\n",
    "    print(\n",
    "        '\\r✅ Validation ended'\n",
    "        '\\t 🏆 Avg score: {:+5.2f}'\n",
    "        '\\t 🏅 Avg custom score: {:+5.2f}'\n",
    "        '\\t 💯 Avg done: {:7.2%}'\n",
    "        '\\t 💀 Avg deadlocks: {:7.2%}'\n",
    "        '\\t 🦶 Avg steps: {:5.2f}'\n",
    "        '\\t 🤔 Avg choices: {:5.2f}'.format(\n",
    "            np.mean(scores),\n",
    "            np.mean(custom_scores),\n",
    "            np.mean(completions),\n",
    "            np.mean(deadlocks),\n",
    "            np.mean(steps),\n",
    "            np.mean(choices_taken)\n",
    "        ), end=\"\\n\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    Train environment with custom observation and prediction\n",
    "    '''\n",
    "    with open('lelia/parameters.yml', 'r') as conf:\n",
    "        args = yaml.load(conf, Loader=yaml.FullLoader)\n",
    "    writer = SummaryWriter()\n",
    "    args = Struct(**args)\n",
    "    train_agents(args, writer)\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚉 Starting testing \t Testing 7 trains on 48x27 grid for 500 episodes\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RailEnvWrapper' object has no attribute 'schedule_generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 268\u001b[0m\n\u001b[0;32m    264\u001b[0m     test_agents(args)\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 268\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[28], line 264\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    262\u001b[0m     args \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(conf, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[0;32m    263\u001b[0m args \u001b[38;5;241m=\u001b[39m Struct(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m--> 264\u001b[0m \u001b[43mtest_agents\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[28], line 79\u001b[0m, in \u001b[0;36mtest_agents\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     76\u001b[0m score, custom_score, final_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Generate a new railway and renderer\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m obs, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregenerate_rail\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregenerate_schedule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     81\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mtesting\u001b[38;5;241m.\u001b[39mrenderer\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m     83\u001b[0m     env_renderer \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_renderer()\n",
      "Cell \u001b[1;32mIn[7], line 113\u001b[0m, in \u001b[0;36mRailEnvWrapper.reset\u001b[1;34m(self, regenerate_rail, regenerate_schedule, activate_agents, random_seed)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optionals \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124magents_hints\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m optionals:\n\u001b[0;32m    111\u001b[0m     agents_hints \u001b[38;5;241m=\u001b[39m optionals[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124magents_hints\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 113\u001b[0m schedule \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschedule_generator\u001b[49m(\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrail, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumber_of_agents,\n\u001b[0;32m    115\u001b[0m     agents_hints, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_resets, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnp_random\n\u001b[0;32m    116\u001b[0m )\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents \u001b[38;5;241m=\u001b[39m EnvAgent\u001b[38;5;241m.\u001b[39mfrom_schedule(schedule)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps \u001b[38;5;241m=\u001b[39m schedule\u001b[38;5;241m.\u001b[39mmax_episode_steps\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RailEnvWrapper' object has no attribute 'schedule_generator'"
     ]
    }
   ],
   "source": [
    "def print_agents_info(env, info, actions):\n",
    "    '''\n",
    "    Print information for each agent in a specific step\n",
    "    '''\n",
    "    _status_table = []\n",
    "    for handle, agent in enumerate(env.agents):\n",
    "        _status_table.append([\n",
    "            handle,\n",
    "            agent.status,\n",
    "            agent.speed_data[\"speed\"],\n",
    "            agent.speed_data['position_fraction'],\n",
    "            (\n",
    "                agent.initial_position[0],\n",
    "                agent.initial_position[1],\n",
    "                agent.direction\n",
    "            ) if agent.status == TrainState.READY_TO_DEPART else\n",
    "            (\n",
    "                agent.position[0],\n",
    "                agent.position[1],\n",
    "                agent.direction\n",
    "            ) if agent.status != TrainState.DONE else (\n",
    "                'DONE'\n",
    "            ),\n",
    "            agent.target,\n",
    "            actions[handle],\n",
    "            agent.malfunction_data['malfunction'],\n",
    "            info[\"deadlocks\"][handle]\n",
    "        ])\n",
    "    print(tabulate(\n",
    "        _status_table,\n",
    "        [\n",
    "            \"Handle\", \"Status\", \"Speed\", \"Position fraction\",\n",
    "            \"Position\", \"Target\", \"Action Taken\", \"Malfunction\", \"Deadlock\"\n",
    "        ],\n",
    "        colalign=[\"center\"] * 9\n",
    "    ))\n",
    "\n",
    "\n",
    "def test_agents(args):\n",
    "    '''\n",
    "    Test agents on the specified environment\n",
    "    '''\n",
    "    choices_taken = np.zeros((args.testing.episodes,))\n",
    "    scores, custom_scores, completions, steps, deadlocks = [], [], [], [], []\n",
    "\n",
    "    # Initialize threads and seeds\n",
    "    set_num_threads(args.generic.num_threads)\n",
    "    if args.generic.fix_random:\n",
    "        fix_random(args.generic.random_seed)\n",
    "\n",
    "    # Create railway environment\n",
    "    env = create_rail_env(args, load_env=args.testing.load)\n",
    "\n",
    "    # Load the model if provided\n",
    "    if args.testing.model:\n",
    "        parameter_decay = PARAMETER_DECAYS[\"none\"](\n",
    "            parameter_start=args.parameter_decay.start\n",
    "        )\n",
    "        action_selector = ACTION_SELECTORS[\"greedy\"](parameter_decay)\n",
    "        policy_type = args.policy.type.get_true_key()\n",
    "        policy = POLICIES[policy_type](\n",
    "            args, env.state_size, action_selector, training=False\n",
    "        )\n",
    "        policy.load(args.testing.model)\n",
    "    else:\n",
    "        policy = POLICIES[\"random\"]()\n",
    "\n",
    "    print(\"\\n🚉 Starting testing \\t Testing {} trains on {}x{} grid for {} episodes\".format(\n",
    "        args.env.num_trains,\n",
    "        args.env.width, args.env.height,\n",
    "        args.testing.episodes,\n",
    "    ))\n",
    "\n",
    "    # Perform the given number of episodes\n",
    "    for episode in range(args.testing.episodes):\n",
    "        score, custom_score, final_step = 0.0, 0.0, 0\n",
    "\n",
    "        # Generate a new railway and renderer\n",
    "        obs, info = env.reset(\n",
    "            regenerate_rail=True, regenerate_schedule=True\n",
    "        )\n",
    "        if args.testing.renderer.enabled:\n",
    "            env_renderer = env.get_renderer()\n",
    "\n",
    "        # Print agents tasks\n",
    "        if args.testing.verbose:\n",
    "            _tasks_table = []\n",
    "            for handle, agent in enumerate(env.agents):\n",
    "                _tasks_table.append([\n",
    "                    handle,\n",
    "                    agent.status,\n",
    "                    agent.speed_data[\"speed\"],\n",
    "                    (\n",
    "                        agent.initial_position[0],\n",
    "                        agent.initial_position[1],\n",
    "                        agent.direction\n",
    "                    ),\n",
    "                    agent.target\n",
    "                ])\n",
    "            print(f\"Episode {episode}\")\n",
    "            print(tabulate(\n",
    "                _tasks_table,\n",
    "                [\"Handle\", \"Status\", \"Speed\", \"Source\", \"Target\"],\n",
    "                colalign=[\"center\"] * 5\n",
    "            ))\n",
    "            print()\n",
    "\n",
    "        # Create frames directory\n",
    "        now = datetime.now()\n",
    "        test_id = now.strftime('%Y%m%d-%H%M%S')\n",
    "        if args.testing.renderer.enabled and args.testing.renderer.save_frames:\n",
    "            frames_dir = f\"tmp/frames/{test_id}\"\n",
    "            os.makedirs(frames_dir, exist_ok=True)\n",
    "\n",
    "        # Compute agents with same source\n",
    "        agents_with_same_start = env.get_agents_same_start()\n",
    "\n",
    "        for step in range(env._max_episode_steps):\n",
    "\n",
    "            # Prioritize entry of faster agent in the environment\n",
    "            for position in agents_with_same_start:\n",
    "                if len(agents_with_same_start[position]) > 0:\n",
    "                    del agents_with_same_start[position][0]\n",
    "                    for agent in agents_with_same_start[position]:\n",
    "                        info['action_required'][agent] = False\n",
    "\n",
    "            # Policy act\n",
    "            legal_actions, legal_choices, moving_agents = env.pre_act()\n",
    "            choices, is_best = policy.act(\n",
    "                list(obs.values()), legal_choices,\n",
    "                moving_agents, training=False\n",
    "            )\n",
    "            action_dict, metadata = env.post_act(\n",
    "                choices, is_best, legal_actions, moving_agents\n",
    "            )\n",
    "            current_choices_count = metadata['choices_count']\n",
    "            choices_taken[episode] += np.sum(current_choices_count)\n",
    "\n",
    "            # Environment step\n",
    "            obs, rewards, custom_rewards, done, info = env.step(\n",
    "                action_dict\n",
    "            )\n",
    "\n",
    "            # Render an episode at some interval\n",
    "            if args.testing.renderer.enabled:\n",
    "                env_renderer.render_env(\n",
    "                    show=True, show_observations=False, show_predictions=True, show_rowcols=True\n",
    "                )\n",
    "\n",
    "                # Wait to observe the current frame\n",
    "                if args.testing.renderer.sleep > 0:\n",
    "                    time.sleep(args.testing.renderer.sleep)\n",
    "\n",
    "                # Save renderer frame\n",
    "                if args.testing.renderer.save_frames:\n",
    "                    env_renderer.gl.save_image(\n",
    "                        \"{:s}/{:04d}.png\".format(frames_dir, step)\n",
    "                    )\n",
    "\n",
    "            # Update agents score\n",
    "            for handle in range(env.get_num_agents()):\n",
    "                score += rewards[handle]\n",
    "                custom_score += custom_rewards[handle]\n",
    "\n",
    "            # Compute statistics\n",
    "            if args.testing.verbose:\n",
    "                normalized_score = (\n",
    "                    score / (env._max_episode_steps * env.get_num_agents())\n",
    "                )\n",
    "                normalized_custom_score = custom_score / env.get_num_agents()\n",
    "                print(\n",
    "                    f\"Score: {round(normalized_score, 4)} /\"\n",
    "                    f\"Custom score: {round(normalized_custom_score, 4)}\"\n",
    "                )\n",
    "                print_agents_info(env, info, action_dict)\n",
    "                print()\n",
    "\n",
    "            # Check if every agent is arrived\n",
    "            final_step = step\n",
    "            if done['__all__'] or env.check_if_all_blocked(info[\"deadlocks\"]):\n",
    "                break\n",
    "\n",
    "        # Close window\n",
    "        if args.testing.renderer.enabled:\n",
    "            env_renderer.close_window()\n",
    "\n",
    "        # Save final scores\n",
    "        scores.append(\n",
    "            score / (\n",
    "                env._max_episode_steps *\n",
    "                env.get_num_agents()\n",
    "            )\n",
    "        )\n",
    "        custom_scores.append(custom_score / env.get_num_agents())\n",
    "        completions.append(\n",
    "            sum(done[idx] for idx in env.get_agent_handles()) /\n",
    "            env.get_num_agents()\n",
    "        )\n",
    "        steps.append(final_step)\n",
    "        deadlocks.append(\n",
    "            sum(int(v) for v in info[\"deadlocks\"].values()) /\n",
    "            env.get_num_agents()\n",
    "        )\n",
    "\n",
    "        # Print episode info\n",
    "        print(\n",
    "            '\\r🚂 Test {:4n}'\n",
    "            '\\t 🏆 Score: {:<+5.4f}'\n",
    "            ' Avg: {:>+5.4f}'\n",
    "            '\\t 🏅 Custom score: {:<+5.4f}'\n",
    "            ' Avg: {:>+5.4f}'\n",
    "            '\\t 💯 Done: {:<7.2%}'\n",
    "            ' Avg: {:>7.2%}'\n",
    "            '\\t 💀 Deadlocks: {:<7.2%}'\n",
    "            ' Avg: {:>7.2%}'\n",
    "            '\\t 🦶 Steps: {:4n}/{:4n}'\n",
    "            '\\t 🤔 Choices: {:4n}'.format(\n",
    "                episode,\n",
    "                scores[-1],\n",
    "                np.mean(scores),\n",
    "                custom_scores[-1],\n",
    "                np.mean(custom_scores),\n",
    "                completions[-1],\n",
    "                np.mean(completions),\n",
    "                deadlocks[-1],\n",
    "                np.mean(deadlocks),\n",
    "                steps[-1],\n",
    "                env._max_episode_steps,\n",
    "                choices_taken[episode]\n",
    "            ), end=\"\\n\"\n",
    "        )\n",
    "\n",
    "    # Print final testing info\n",
    "    print(\"\\n\\r🏁 Testing ended \\tTested {} trains on {}x{} grid for {} episodes\".format(\n",
    "        args.env.num_trains, args.env.width, args.env.height, args.testing.episodes\n",
    "    ))\n",
    "\n",
    "    # Print final testing results\n",
    "    print(\n",
    "        '\\r✅ Testing ended'\n",
    "        '\\t 🏆 Avg score: {:+7.4f}'\n",
    "        '\\t 🏅 Avg custom score: {:+7.4f}'\n",
    "        '\\t 💯 Avg done: {:7.4f}'\n",
    "        '\\t 💀 Avg deadlocks: {:7.4f}'\n",
    "        '\\t 🦶 Avg steps: {:5.2f}'\n",
    "        '\\t 🤔 Avg choices: {:5.2f}'.format(\n",
    "            np.mean(scores),\n",
    "            np.mean(custom_scores),\n",
    "            np.mean(completions),\n",
    "            np.mean(deadlocks),\n",
    "            np.mean(steps),\n",
    "            np.mean(choices_taken)\n",
    "        ), end=\"\\n\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    Test environment with the given model\n",
    "    '''\n",
    "    with open('lelia/parameters.yml', 'r') as conf:\n",
    "        args = yaml.load(conf, Loader=yaml.FullLoader)\n",
    "    args = Struct(**args)\n",
    "    test_agents(args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Other training based on the methods used in the project\n",
    "\n",
    "train_agent(model, policy, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
