{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map-related methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('src/')\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "from ast import literal_eval\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import time\n",
    "import random\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from collections import deque\n",
    "from typing import List, Optional, Tuple, Union, Callable, Dict, Sequence, NamedTuple\n",
    "from pathlib import Path\n",
    "\n",
    "from flatland.envs.rail_generators import sparse_rail_generator\n",
    "from flatland.envs.line_generators import SparseLineGen\n",
    "from flatland.envs.malfunction_generators import malfunction_from_params\n",
    "from flatland.utils.rendertools import RenderTool, AgentRenderVariant\n",
    "from flatland.envs.rail_env import RailEnvActions\n",
    "from training import train_agent\n",
    "\n",
    "# Base flatland environment\n",
    "from flatland.envs.line_generators import SparseLineGen\n",
    "from flatland.envs.malfunction_generators import (\n",
    "    MalfunctionParameters,\n",
    "    ParamMalfunctionGen,\n",
    ")\n",
    "from flatland.envs.rail_env import RailEnv\n",
    "from flatland.envs.rail_generators import SparseRailGen\n",
    "from flatland.envs.observations import GlobalObsForRailEnv\n",
    "\n",
    "from flatland.envs.observations import TreeObsForRailEnv\n",
    "from flatland.envs.distance_map import DistanceMap\n",
    "import flatland.envs.rail_env_shortest_paths as sp\n",
    "\n",
    "from src import test_utils, training, rewards\n",
    "from src.observation_utils import normalize_observation\n",
    "from src.models import *\n",
    "from src.deep_model_policy import DeepPolicy, PolicyParameters\n",
    "\n",
    "from flatland.core.env import Environment\n",
    "from flatland.core.env_observation_builder import ObservationBuilder\n",
    "from flatland.core.grid.grid4_utils import get_new_position, direction_to_point\n",
    "from flatland.envs.rail_env import RailEnvActions\n",
    "from flatland.envs.step_utils.states import TrainState\n",
    "\n",
    "from importlib_resources import path\n",
    "\n",
    "# Visualization\n",
    "from flatland.utils.rendertools import RenderTool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "\n",
    "env = RailEnv(\n",
    "    width=20,\n",
    "    height=20,\n",
    "    rail_generator=SparseRailGen(\n",
    "        max_num_cities=2,  # Number of cities\n",
    "        grid_mode=True,\n",
    "        max_rails_between_cities=2,\n",
    "        max_rail_pairs_in_city=1,\n",
    "    ),\n",
    "    line_generator=SparseLineGen(speed_ratio_map={1.: 1.}\n",
    "        ),\n",
    "    number_of_agents=2, \n",
    "    obs_builder_object=TreeObsForRailEnv(max_depth=3),\n",
    "    malfunction_generator=ParamMalfunctionGen(\n",
    "        MalfunctionParameters(\n",
    "            malfunction_rate=0.,  # Rate of malfunction\n",
    "            min_duration=3,  # Minimal duration\n",
    "            max_duration=20,  # Max duration\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "_,_ = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful functions for displaying tables\n",
    "\n",
    "# Display the Q-table as a set of heatmaps, one for each action\n",
    "def qtable_display(\n",
    "    q_array: np.ndarray, \n",
    "    title: Optional[str] = None, \n",
    "    figsize: Tuple[int, int] = (4, 4), \n",
    "    annot: bool = True, \n",
    "    fmt: str = \"0.1f\", \n",
    "    linewidths: float = .5, \n",
    "    square: bool = True, \n",
    "    cbar: bool = False, \n",
    "    cmap: str = \"Reds\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Display a Q-table as a set of heatmaps, one for each action.\n",
    "\n",
    "    For the frozen lake environment, there are 16 states and 4 actions thus this function will display 4 heatmaps, one for each action.\n",
    "    Each heatmap will display the Q-values for each state when performing the action indexed by the heatmap.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    q_array : np.ndarray\n",
    "        The Q-table to display. Each row corresponds to a state, and each column corresponds to an action.\n",
    "        In the frozen lake environment, there are 16 states and 4 actions thus the Q-table has a shape of (16, 4).\n",
    "        For instance, q_array[0, 3] is the Q-value (estimation of the expected reward) for performing action 3 (\"move up\") in state 0 (the top left square).\n",
    "    title : str, optional\n",
    "        The title of the plot, by default None\n",
    "    figsize : tuple, optional\n",
    "        The size of the figure (in inches), by default (4, 4)\n",
    "    annot : bool, optional\n",
    "        If True, write the data value in each cell, by default True\n",
    "    fmt : str, optional\n",
    "        The string formatting code to use when adding annotations, by default \"0.1f\" that will display a single decimal\n",
    "    linewidths : float, optional\n",
    "        The width of the lines that will divide each cell, by default .5\n",
    "    square : bool, optional\n",
    "        Whether to set the Axes aspect to \"equal\" so each cell is square-shaped, by default True\n",
    "    cbar : bool, optional\n",
    "        Whether to draw a colorbar, by default False\n",
    "    cmap : str, optional\n",
    "        The mapping from data values to color space, by default \"Reds\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Get the number of actions from the shape of the Q-table\n",
    "    num_actions = q_array.shape[1]\n",
    "\n",
    "    # Adjust the figure size (in inches) based on the number of actions\n",
    "    global_figsize = list(figsize)\n",
    "    global_figsize[0] *= num_actions\n",
    "\n",
    "    # Create a subplot for each action\n",
    "    fig, ax_list = plt.subplots(ncols=num_actions, figsize=global_figsize)\n",
    "\n",
    "    # For each action, display the Q-values for all states as a heatmap\n",
    "    for action_index in range(num_actions):\n",
    "        ax = ax_list[action_index]\n",
    "\n",
    "        # Retrieve the Q-values for each state when performing the action indexed by \"action_index\".\n",
    "        # This forms a 1D array, state_vec, where state_vec[i] = Q(i, action_index).\n",
    "        state_vec = q_array[:,action_index]\n",
    "\n",
    "        # Display the Q-values for each state when performing the action indexed by \"action_index\"\n",
    "        # i.e. display Q(., action_index)\n",
    "        states_display(\n",
    "            state_vec,\n",
    "            title=r\"$Q(\\cdot,a_{})$\".format(action_index),\n",
    "            #title=r\"$Q(\\cdot,a_{})$ {}\".format(action_index, action_labels[action_index]),\n",
    "            figsize=figsize, \n",
    "            annot=annot, \n",
    "            fmt=fmt, \n",
    "            linewidths=linewidths, \n",
    "            square=square, \n",
    "            cbar=cbar, \n",
    "            cmap=cmap, \n",
    "            ax=ax\n",
    "        )\n",
    "\n",
    "    # Set the title for the entire figure\n",
    "    plt.suptitle(title)\n",
    "    # Display the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def states_display(\n",
    "    state_seq: Sequence[float], \n",
    "    title: Optional[str] = None, \n",
    "    figsize: Tuple[int, int] = (5, 5), \n",
    "    annot: bool = True, \n",
    "    fmt: str = \"0.1f\", \n",
    "    linewidths: float = .5, \n",
    "    square: bool = True, \n",
    "    cbar: bool = False, \n",
    "    cmap: str = \"Reds\", \n",
    "    ax: Optional[matplotlib.axes.Axes] = None\n",
    ") -> Optional[matplotlib.axes.Axes]:\n",
    "    \"\"\"\n",
    "    Display the expected values of all states as a heatmap.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state_seq : Sequence[float]\n",
    "        The sequence of expected values to display. This can be a list, a 1D array, etc.\n",
    "        Each element is the estimation of the expected value of the corresponding state.\n",
    "        For example, state_seq[0] is the estimation of the expected value of the first state.\n",
    "        There are 16 elements in this sequence for the frozenlake environment, i.e., one per state of the environment.\n",
    "    title : str, optional\n",
    "        The title of the plot, by default None\n",
    "    figsize : tuple, optional\n",
    "        The size of the figure (in inches), by default (5, 5)\n",
    "    annot : bool, optional\n",
    "        If True, write the data value in each cell, by default True\n",
    "    fmt : str, optional\n",
    "        The string formatting code to use when adding annotations, by default \"0.1f\"\n",
    "    linewidths : float, optional\n",
    "        The width of the lines that will divide each cell, by default .5\n",
    "    square : bool, optional\n",
    "        Whether to set the Axes aspect to \"equal\" so each cell is square-shaped, by default True\n",
    "    cbar : bool, optional\n",
    "        Whether to draw a colorbar, by default False\n",
    "    cmap : str, optional\n",
    "        The mapping from data values to color space, by default \"Reds\"\n",
    "    ax : matplotlib.axes.Axes, optional\n",
    "        The axes object to draw the heatmap on, by default None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.axes.Axes, optional\n",
    "        The axes object with the heatmap if one was provided, otherwise None.\n",
    "    \"\"\"\n",
    "    # Calculate the size of the state array\n",
    "    size = int(math.sqrt(len(state_seq)))\n",
    "    # Convert the state sequence to a numpy array (if it isn't already one)\n",
    "    state_array = np.array(state_seq)\n",
    "    # Reshape the state array into a square matrix\n",
    "    # (we assume here that the environment has a state space that can be visualized as a square grid)\n",
    "    state_array = state_array.reshape(size, size)\n",
    "\n",
    "    # If no axes object is provided, create a new figure and axes\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    # Create a heatmap of the state array on the axes\n",
    "    ax = sns.heatmap(\n",
    "        state_array, \n",
    "        annot=annot, \n",
    "        fmt=fmt, \n",
    "        linewidths=linewidths, \n",
    "        square=square, \n",
    "        cbar=cbar, \n",
    "        cmap=cmap,\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    # If a title is provided, set the title of the plot\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "\n",
    "    # If no axes object was provided, display the plot\n",
    "    # Otherwise, return the axes object with the heatmap\n",
    "    if ax is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define epsilon-greedy policy\n",
    "\n",
    "def greedy_policy(state: int, q_array: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Determine the action that maximizes the Q-value for a given state.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : int\n",
    "        The current state.\n",
    "    q_array : np.ndarray\n",
    "        The Q-table.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The action that maximizes the Q-value for the given state.\n",
    "    \"\"\"\n",
    "    action = np.argmax(q_array[state, :])\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state: int, q_array: np.ndarray, epsilon: float) -> int:\n",
    "    \"\"\"\n",
    "    Determine the action to take based on an epsilon-greedy policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : int\n",
    "        The current state.\n",
    "    q_array : np.ndarray\n",
    "        The Q-table.\n",
    "    epsilon : float\n",
    "        The probability of choosing a random action.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The action to take.\n",
    "    \"\"\"\n",
    "    if np.random.rand() < epsilon:\n",
    "        # With probability epsilon, choose a random action\n",
    "        action = np.random.choice(len(q_array[state, :]))\n",
    "    else:\n",
    "        # With probability 1 - epsilon, choose the action with the highest Q-value\n",
    "        action = np.argmax(q_array[state, :])\n",
    "        \n",
    "    return action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY_EVERY_N_EPISODES = 50\n",
    "\n",
    "# Initialize the history of the Q-table and learning rate\n",
    "q_array_history = []\n",
    "alpha_history = []\n",
    "\n",
    "def sarsa(\n",
    "    environment: RailEnv, \n",
    "    alpha: float = 0.1, \n",
    "    alpha_factor: float = 0.9995, \n",
    "    gamma: float = 0.99, \n",
    "    epsilon: float = 0.5, \n",
    "    num_episodes: int = 1500, \n",
    "    display: bool = False\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform SARSA learning on a given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    environment : RailEnv\n",
    "        The environment to learn in.\n",
    "    alpha : float, optional\n",
    "        The learning rate, between 0 and 1. By default 0.1\n",
    "    alpha_factor : float, optional\n",
    "        The factor to decrease alpha by each episode, by default 0.9995\n",
    "    gamma : float, optional\n",
    "        The discount factor, between 0 and 1. By default 0.99\n",
    "    epsilon : float, optional\n",
    "        The probability of choosing a random action, by default 0.5\n",
    "    num_episodes : int, optional\n",
    "        The number of episodes to run, by default 1500\n",
    "    display : bool, optional\n",
    "        Whether to display the Q-table (every DISPLAY_EVERY_N_EPISODES episodes), by default False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The learned Q-table.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the number of states and actions in the environment\n",
    "    num_states = env.height * env.width\n",
    "    num_actions = 5    # MOVE_LEFT, MOVE_FORWARD, MOVE_RIGHT, STOP_MOVING, DO_NOTHING\n",
    "\n",
    "    # Initialize the Q-table to zeros\n",
    "    q_array = np.zeros([num_states, num_actions])\n",
    "\n",
    "    # Loop over the episodes\n",
    "    for episode_index in range(num_episodes):\n",
    "        # Display the Q-table every DISPLAY_EVERY_N_EPISODES episodes if display is True\n",
    "        if display and episode_index % DISPLAY_EVERY_N_EPISODES == 0:\n",
    "            qtable_display(q_array, title=\"Q table\")\n",
    "        else:\n",
    "            print('.', end=\"\")\n",
    "\n",
    "        # Save the current Q-table and learning rate\n",
    "        q_array_history.append(q_array.copy())\n",
    "        alpha_history.append(alpha)\n",
    "\n",
    "        # Decrease the learning rate if alpha_factor is not None\n",
    "        if alpha_factor is not None:\n",
    "            alpha = alpha * alpha_factor\n",
    "\n",
    "        s = environment.reset()[0]\n",
    "        done = False \n",
    "        a = epsilon_greedy_policy(s, q_array, epsilon)\n",
    "        while done is False:\n",
    "            s_, r, done, _ , _= environment.step(a)\n",
    "            a_ = epsilon_greedy_policy(s_, q_array, epsilon)\n",
    "            q_array[s, a] += alpha * (r + gamma * q_array[s_, a_] - q_array[s, a])\n",
    "            s = s_\n",
    "            a = a_        \n",
    "\n",
    "    # Return the learned Q-table\n",
    "    return q_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed strategy : bitmaps and DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning from bitmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structure:** (from Devid Farinelli and Giulia Cantini)\n",
    "\n",
    "A \"rail occupancy bitmap\" indicated on which rail and in which direction the agent is traveling at every timestep:\n",
    "1. A directed graph representation of the railway network is generated through BFS (breadth-firth search), each node is a switch and each edge is a rail between two switches.\n",
    "2. The shortest path for each agent is computed\n",
    "3. The path is transformed into a bitmap with the timesteps as columns and the rails as rows. The direction is 1 if the agent is traveling the edge from the source node to the destination node or -1 otherwise.\n",
    "\n",
    "Heatmaps are used to provide information about how the traffic is distributed across the rails over time.\n",
    "Each agent computes 2 heatmaps, one positive and one negative, both are generated summing the bitmaps of all the other agents.\n",
    "\n",
    "The network architecture is a Dueling DQN, with a Conv2D layer as input processing the concatenated agent bitmap, positive and negative heatmaps. The data goes through two separate streams (value and advantage) to be recombined in the final output Q values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dueling DQN model\n",
    "\n",
    "def dim_output(input_dim, filter_dim, stride_dim):\n",
    "    return (input_dim - filter_dim) // stride_dim + 1\n",
    "\n",
    "class Dueling_DQN(nn.Module):\n",
    "    def __init__(self, width, height, action_space):\n",
    "        super(Dueling_DQN, self).__init__()\n",
    "        \n",
    "        self.action_space = action_space\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(1, width))\n",
    "        self.fc1_adv = nn.Linear(in_features=64 * height, out_features=512) \n",
    "        self.fc1_val = nn.Linear(in_features=64 * height, out_features=512)\n",
    "        self.fc2_adv = nn.Linear(in_features=512, out_features=action_space)\n",
    "        self.fc2_val = nn.Linear(in_features=512, out_features=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x): # x is the input\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        adv = self.relu(self.fc1_adv(x))\n",
    "        val = self.relu(self.fc1_val(x))\n",
    "\n",
    "        adv = self.fc2_adv(adv)\n",
    "        val = self.fc2_val(val).expand(x.size(0), self.action_space)\n",
    "\n",
    "        x = val + adv - adv.mean(1).unsqueeze(1).expand(x.size(0), self.action_space)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bitmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the heatmaps and bitmap\n",
    "\n",
    "class ObsPreprocessor:\n",
    "    def __init__(self, max_rails, reorder_rails):\n",
    "        self.max_rails = max_rails\n",
    "        self.reorder_rails = reorder_rails\n",
    "\n",
    "    def _fill_padding(self, obs, max_rails):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param obs: Agent state \n",
    "        :param max_rails: Maximum number of rails in environment \n",
    "        :return: Observation padded with 0s along first axis (until max_rails)\n",
    "        \n",
    "        \"\"\"\n",
    "        prediction_depth = obs.shape[1]\n",
    "        \n",
    "        pad_agent_obs = np.zeros((max_rails, prediction_depth))\n",
    "        pad_agent_obs[:obs.shape[0], :obs.shape[1]] = obs\n",
    "        \n",
    "        return pad_agent_obs\n",
    "\n",
    "    def _get_heatmap(self, handle, bitmaps, max_rails):\n",
    "        temp_bitmaps = np.copy(bitmaps)\n",
    "        temp_bitmaps[handle, :, :] = 0\n",
    "        pos_dir = np.sum(np.where(temp_bitmaps > 0, temp_bitmaps, 0), axis=0)\n",
    "        neg_dir = np.abs(np.sum(np.where(temp_bitmaps < 0, temp_bitmaps, 0), axis=0))\n",
    "    \n",
    "        return pos_dir, neg_dir\n",
    "\n",
    "    def _swap_rails(self, bitmap, swap):\n",
    "        bitmap[range(len(swap))] = bitmap[swap]\n",
    "        bitmap[len(swap):, :] = 0\n",
    "        return bitmap\n",
    "\n",
    "    def _reorder_rails(self, bitmap, pos_map, neg_map): \n",
    "        swap = np.array([], dtype=int) \n",
    "\n",
    "        ts = 0 \n",
    "        rail = np.argmax(np.absolute(bitmap[:, ts]))\n",
    "        # If agent not departed\n",
    "        if bitmap[rail, ts] == 0: \n",
    "            ts = 1\n",
    "            rail = np.argmax(np.absolute(bitmap[:, ts]))\n",
    "        \n",
    "        # While the bitmap is not empty\n",
    "        while bitmap[rail, ts] != 0: \n",
    "            swap = np.append(swap, rail)\n",
    "            ts += np.argmax(bitmap[rail, ts:] == 0) \n",
    "            rail = np.argmax(np.absolute(bitmap[:, ts]))\n",
    "\n",
    "        if len(swap) > 0: \n",
    "            bitmap = self._swap_rails(bitmap, swap)\n",
    "            pos_map = self._swap_rails(pos_map, swap)\n",
    "            neg_map = self._swap_rails(neg_map, swap)\n",
    "        \n",
    "        return bitmap, pos_map, neg_map\n",
    "\n",
    "    def get_obs(self, handle, bitmap, maps):\n",
    "        pos_map, neg_map = self._get_heatmap(handle, maps, self.max_rails)\n",
    "\n",
    "        if self.reorder_rails:\n",
    "            bitmap, pos_map, neg_map = self._reorder_rails(bitmap, pos_map, neg_map)\n",
    "\n",
    "        state = np.concatenate([\n",
    "            self._fill_padding(bitmap, self.max_rails),\n",
    "            self._fill_padding(pos_map, self.max_rails),\n",
    "            self._fill_padding(neg_map, self.max_rails)\n",
    "        ])\n",
    "        \n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Waypoint = NamedTuple(\n",
    "    'Waypoint', [('position', Tuple[int, int]), ('direction', int)])\n",
    "\n",
    "CardinalNode = \\\n",
    "\tNamedTuple('CardinalNode', [('id_node', int), ('cardinal_point', int)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we implement a class that returns the rails occupancy as a bitmap with rails on y-axis and timesteps on x-axis. Rails are edges and the 1/-1 in the bitmap indicate the direction of the agent on the rail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n        --- timesteps --->\\nrail 0: 1 1 1       -1-1\\nrail 1:      1 1\\nrail 2:         -1-1\\n.\\n.\\nrail n:\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "        --- timesteps --->\n",
    "rail 0: 1 1 1       -1-1\n",
    "rail 1:      1 1\n",
    "rail 2:         -1-1\n",
    ".\n",
    ".\n",
    "rail n:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:352: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:355: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:352: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:355: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "C:\\Users\\lelia\\AppData\\Local\\Temp\\ipykernel_11740\\3097314515.py:352: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(holes == 0, \"All the cells of the bitmap should be filled\")\n",
      "C:\\Users\\lelia\\AppData\\Local\\Temp\\ipykernel_11740\\3097314515.py:355: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(np.all(temp), \"Thee agent's bitmap shouldn't have holes \")\n"
     ]
    }
   ],
   "source": [
    "class RailObsForRailEnv(ObservationBuilder):\n",
    "\n",
    "\tdef __init__(self, predictor):\n",
    "\t\t\"\"\"\n",
    "\t\tpredictor: class that predicts the path.\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(RailObsForRailEnv, self).__init__()\n",
    "  \n",
    "\t\tself.predictor = predictor\n",
    "\t\tself.num_agents = None\n",
    "\t\tself.num_rails = None # computed in reset()\n",
    "\t\tself.max_time_steps = self.predictor.max_depth\n",
    "\n",
    "\t\t\n",
    "\t\tself.cell_to_id_node = {} # Map cell position : id_node\n",
    "\t\tself.id_node_to_cell = {} # Map id_node to cell position\n",
    "\t\tself.info = {} # Map id_edge : tuple (CardinalNode1, CardinalNode2, edge_length)\n",
    "\t\tself.id_edge_to_cells = {} # Map id_edge : list of tuples (cell pos, crossing dir) in rail\n",
    "\t\tself.nodes = set() # Set of node ids\n",
    "\t\tself.edges = set() # Set of edge ids\n",
    "\n",
    "\t\tself.recompute_bitmap = True\n",
    "\n",
    "\tdef set_env(self, env: Environment):\n",
    "\t\tsuper().set_env(env)\n",
    "\t\tif self.predictor:\n",
    "\t\t\tself.predictor.set_env(self.env)\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\tself.cell_to_id_node = {}\n",
    "\t\tself.id_node_to_cell = {}\n",
    "\t\tself.info = {}\n",
    "\t\tself.id_edge_to_cells = {}\n",
    "\t\tself.nodes = set()\n",
    "\t\tself.edges = set()\n",
    "\t\tself._map_to_graph()\n",
    "\t\tself.recompute_bitmap = True\n",
    "\t\tself.num_agents = len(self.env.agents)\n",
    "\n",
    "\t\t# Calculate agents timesteps per cell\n",
    "\t\tself.tpc = dict()\n",
    "\t\tfor a in range(self.num_agents):\n",
    "\t\t\tagent_speed = self.env.agents[a].speed_data['speed']\n",
    "\t\t\tself.tpc[a] = int(np.reciprocal(agent_speed))\n",
    "\t\t\n",
    "\tdef get_many(self, handles: Optional[List[int]] = None):\n",
    "\t\tmaps = None\n",
    "\t\t# Compute bitmaps from shortest paths\n",
    "\t\tif self.recompute_bitmap:\n",
    "\t\t\tself.recompute_bitmap = False\n",
    "\n",
    "\t\t\tprediction_dict = self.predictor.get()\n",
    "\t\t\tself.paths = self.predictor.shortest_paths\n",
    "\t\t\tcells_sequence = self.predictor.compute_cells_sequence(prediction_dict)\n",
    "\n",
    "\t\t\tmaps = np.zeros((self.num_agents, self.num_rails, self.max_time_steps + 1), dtype=int)\n",
    "\t\t\tfor a in range(self.num_agents):\n",
    "\t\t\t\tmaps[a, :, :] = self._bitmap_from_cells_seq(a, cells_sequence[a])\n",
    "\n",
    "\t\t\tmaps = np.roll(maps, 1)\n",
    "\t\t\tmaps[:, :, 0] = 0\n",
    "\n",
    "\t\treturn maps\n",
    "\n",
    "\tdef get_altmaps(self, handle):\n",
    "\t\tagent = self.env.agents[handle]\n",
    "\t\taltpaths, cells_seqs = self.predictor.get_altpaths(handle, self.cell_to_id_node)\n",
    "\t\tmaps = []\n",
    "\t\tfor i in range(len(cells_seqs)):\n",
    "\t\t\tbitmap = self._bitmap_from_cells_seq(handle, cells_seqs[i])\n",
    "\n",
    "\t\t\t# If agent not departed, add 0 at the beginning\n",
    "\t\t\tif agent.status == TrainState.READY_TO_DEPART:\n",
    "\t\t\t\tbitmap[:, -1] = 0\n",
    "\t\t\t\tbitmap = np.roll(bitmap, 1)\n",
    "\n",
    "\t\t\tmaps.append(bitmap)\n",
    "\n",
    "\t\treturn maps, altpaths\n",
    "\n",
    "\tdef get_agent_action(self, handle):\n",
    "\t\tagent = self.env.agents[handle]\n",
    "\t\taction = RailEnvActions.DO_NOTHING\n",
    "\t\t\n",
    "\t\tif agent.status == TrainState.READY_TO_DEPART:\n",
    "\t\t\taction = RailEnvActions.MOVE_FORWARD\n",
    "\n",
    "\t\telif agent.status == TrainState.MOVING:\n",
    "\t\t\tif self.paths[handle] is None or len(self.paths[handle]) == 0:  # Railway disrupted\n",
    "\t\t\t\tprint('[WARN] AGENT {} RAIL DISRUPTED'.format(handle))\n",
    "\t\t\t\taction = RailEnvActions.STOP_MOVING\n",
    "\t\t\telse:\n",
    "\t\t\t\t# Get action\n",
    "\t\t\t\tstep = self.paths[handle][0]\n",
    "\t\t\t\tnext_action_element = step.next_action_element.action  # Get next_action_element\n",
    "\n",
    "\t\t\t\tif next_action_element == 1:\n",
    "\t\t\t\t\taction = RailEnvActions.MOVE_LEFT\n",
    "\t\t\t\telif next_action_element == 2:\n",
    "\t\t\t\t\taction = RailEnvActions.MOVE_FORWARD\n",
    "\t\t\t\telif next_action_element == 3:\n",
    "\t\t\t\t\taction = RailEnvActions.MOVE_RIGHT\n",
    "\t\t\t\t\n",
    "\t\t\t\tself.paths[handle] = self.paths[handle][1:]\n",
    "\n",
    "\t\treturn action\n",
    "\n",
    "\tdef is_before_switch(self, a):\n",
    "\t\tagent = self.env.agents[a]\n",
    "\t\tbefore_switch = False\n",
    "\n",
    "\t\tif agent.state == TrainState.MOVING :\n",
    "\t\t\tif len(self.paths[a]) > 0:\n",
    "\t\t\t\tcurr_pos = agent.position\n",
    "\t\t\t\tnext_pos = self.paths[a][0].next_action_element.next_position\n",
    "\t\t\t\tcurr_rail, _ = self._get_edge_from_cell(curr_pos)\n",
    "\t\t\t\tnext_rail, _ = self._get_edge_from_cell(next_pos)\n",
    "\t\t\t\tbefore_switch = curr_rail != -1 and next_rail == -1\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint('[WARN] agent\\'s {} path run out'.format(a))\n",
    "\t\t\t\tbefore_switch = True\n",
    "\n",
    "\t\treturn before_switch\n",
    "\n",
    "\tdef _get_rail_dir(self, a, maps, ts=0):\n",
    "\t\trail = np.argmax(np.absolute(maps[a, :, ts]))\n",
    "\t\tdirection = maps[a, rail, ts]\n",
    "\t\treturn rail, direction\n",
    "\n",
    "\tdef _delay(self, a, maps, rail, direction, delay):\n",
    "\t\ttpc = self.tpc[a]\n",
    "\t\told_rail, old_dir = self._get_rail_dir(a, maps)\n",
    "\n",
    "\t\tmaps[a] = np.roll(maps[a], delay)\n",
    "\t\tmaps[a, :, 0:delay+tpc] = 0\t\t\t\t # Reset the first bits\n",
    "\t\tmaps[a, old_rail, 0:tpc] = old_dir       # Fill the first with the current rail info\n",
    "\t\tmaps[a, rail, tpc:tpc+delay] = direction # Add delay to the next rail\n",
    "\t\t\n",
    "\t\treturn maps\n",
    "\n",
    "\tdef _is_cell_occupied(self, a, cell):\n",
    "\t\toccupied = False\n",
    "\n",
    "\t\tfor other in range(self.env.get_num_agents()):\n",
    "\t\t\tif other != a and self.env.agents[other].position == cell:\n",
    "\t\t\t\toccupied = True\n",
    "\t\t\t\tbreak\n",
    "\t\t\n",
    "\t\treturn occupied\n",
    "\n",
    "\tdef update_bitmaps(self, a, maps, is_before_switch=False):\n",
    "\t\t# Calculate exit time when switching rail\n",
    "\t\tif is_before_switch:\n",
    "\t\t\ttpc = self.tpc[a]\n",
    "   \n",
    "\t\t\tnext_rail, next_dir = self._get_rail_dir(a, maps, ts=tpc)\n",
    "\t\t\t_, last_exit = self._last_train_on_rail(a, next_rail, maps)\n",
    "   \n",
    "\t\t\tif last_exit > 0: # Check if rail is already occupied\n",
    "\t\t\t\t# tpc: skips the first bits that are curr_rail\n",
    "\t\t\t\tcurr_exit = np.argmax(maps[a, next_rail, tpc:] == 0)\n",
    "\t\t\t\t# Also consider the last cell of curr_rail\n",
    "\t\t\t\tcurr_exit += tpc\n",
    "\t\t\t\n",
    "\t\t\t\tif curr_exit <= last_exit:\n",
    "\t\t\t\t\tdelay = last_exit + tpc - curr_exit\n",
    "\t\t\t\t\tmaps = self._delay(a, maps, next_rail, next_dir, delay)\n",
    "\n",
    "\t\tmaps[a, :, 0] = 0\n",
    "\t\tmaps[a] = np.roll(maps[a], -1)\n",
    "\t\treturn maps\n",
    "\t\n",
    "\tdef set_agent_path(self, a, path):\n",
    "\t\tself.paths[a] = path\n",
    "\n",
    "\tdef _last_train_on_rail(self, a, rail, maps):\n",
    "\t\t\"\"\"\n",
    "\t\tFind train preceding agent 'handle' on rail.\n",
    "\t\t:param maps: \n",
    "\t\t:param rail: \n",
    "\t\t:param handle: \n",
    "\t\t:return: \n",
    "\t\t\"\"\"\n",
    "\t\tlast, last_exit = 0, 0 # Final train, its expected exit time\n",
    "\n",
    "\t\tfor other in range(self.env.get_num_agents()):\n",
    "\t\t\tif other == a or self.env.agents[other].status == TrainState.READY_TO_DEPART:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\ttpc = self.tpc[other]\n",
    "\n",
    "\t\t\t\n",
    "\t\t\tif maps[other, rail, 0] != 0:  # If agent is already on this rail\n",
    "\t\t\t\tother_exit = np.argmax(maps[other, rail, :] == 0)\n",
    "\n",
    "\t\t\t\tif other_exit > last_exit:\n",
    "\t\t\t\t\tlast, last_exit = other, other_exit\n",
    "\n",
    "\t\t\t# We use tpc-1, to skip the first bits of trains that have decided to enter rail, but are still crossing the cell before\n",
    "\t\t\n",
    "\t\t\telif maps[other, rail, tpc - 1] != 0:\n",
    "\t\t\t\tother_rail, _ = self._get_rail_dir(other, maps)\n",
    "\t\t\t\tother_exit = 0\n",
    "\n",
    "\t\t\t\t# Consider the time to cross the current cell\n",
    "\t\t\t\tif other_rail != rail:\n",
    "\t\t\t\t\tother_exit = np.argmax(maps[other, other_rail, :] == 0)\n",
    "\n",
    "\t\t\t\t# Add the estimated exit time\n",
    "\t\t\t\tother_exit += np.argmax(maps[other, rail, other_exit:] == 0)\n",
    "\n",
    "\t\t\t\tif other_exit > last_exit:\n",
    "\t\t\t\t\tlast, last_exit = other, other_exit\n",
    "\n",
    "\t\treturn last, last_exit\n",
    "\n",
    "\tdef _get_trains_on_rails(self, maps, rail, handle):\n",
    "\t\ttrains = []\n",
    "\t\tfor a in range(self.env.get_num_agents()):\n",
    "\t\t\tif not (maps[a, rail, 0] == 0 or a == handle):\n",
    "\t\t\t\texpected_exit_time = np.argmax(maps[a, rail, :] == 0) \n",
    "\t\t\t\ttrains.append((a, expected_exit_time))\n",
    "\t\ttrains.sort()\n",
    "\t\t\n",
    "\t\treturn trains\n",
    "\n",
    "\tdef _get_edge_from_cell(self, cell):\n",
    "\t\t\"\"\"\n",
    "\t\t:param cell: Cell for which we want to find the associated rail id.\n",
    "\t\t:return: A tuple (id rail, dist) where dist is the distance as offset from the beginning of the rail.\n",
    "\t\t\"\"\"\n",
    "\t\tfor edge in self.id_edge_to_cells.keys():\n",
    "\t\t\tcells = [cell[0] for cell in self.id_edge_to_cells[edge]] \n",
    "\t\t\tif cell in cells:\n",
    "\t\t\t\treturn edge, cells.index(cell)\n",
    "\n",
    "\t\treturn -1, -1\n",
    "\n",
    "\tdef _bitmap_from_cells_seq(self, handle, path) -> np.ndarray:\n",
    "\t\t\"\"\"\n",
    "\t\tCompute bitmap for agent handle, given a selected path.\n",
    "\t\t:param handle: \n",
    "\t\t:return: \n",
    "\t\t\"\"\"\n",
    "\t\tbitmap = np.zeros((self.num_rails, self.max_time_steps + 1), dtype=int)  # Max steps in the future + current ts\n",
    "\t\tagent = self.env.agents[handle]\n",
    "\t\t# Truncate path in the future, after reaching target\n",
    "\t\ttarget_index = [i for i, pos in enumerate(path) if pos[0] == agent.target[0] and pos[1] == agent.target[1]]\n",
    "\t\tif len(target_index) != 0:\n",
    "\t\t\ttarget_index = target_index[0]\n",
    "\t\t\tpath = path[:target_index + 1]\n",
    "\n",
    "\t\t# Add 0 at first ts - for 'not departed yet'\n",
    "\t\trail, _ = self._get_edge_from_cell(path[0])\n",
    "\n",
    "\t\t# Agent's cardinal node, where it entered the last edge\n",
    "\t\tagent_entry_node = None\n",
    "\t\t# Calculate initial edge entry point\n",
    "\t\ti = 0\n",
    "\t\trail, _ = self._get_edge_from_cell(path[i])\n",
    "\t\tif rail != -1: # If it's on an edge\n",
    "\t\t\tinitial_rail = rail\n",
    "\t\t\t# Search first switch\n",
    "\t\t\twhile rail != -1:\n",
    "\t\t\t\ti += 1\n",
    "\t\t\t\trail, _ = self._get_edge_from_cell(path[i])\n",
    "\n",
    "\t\t\tsrc, dst, _ = self.info[initial_rail]\n",
    "\t\t\tnode_id = self.cell_to_id_node[path[i]]\n",
    "\t\t\t# Reversed because we want the switch's cp\n",
    "\t\t\tentry_cp = self._reverse_dir(direction_to_point(path[i-1], path[i]))\n",
    "\t\t\t# If we reach the dst node\n",
    "\t\t\tif (node_id, entry_cp) == dst:\n",
    "\t\t\t\t# We entered from the src node (cross_dir = 1)\n",
    "\t\t\t\tagent_entry_node = src\n",
    "\t\t\t# Otherwise the opposite\n",
    "\t\t\telif (node_id, entry_cp) == src: \n",
    "\t\t\t\tagent_entry_node = dst\n",
    "\t\telse:\n",
    "\t\t\t#Handle the case you call this while on a switch before a rail\n",
    "\t\t\tnode_id = self.cell_to_id_node[path[i]]\n",
    "\t\t\t# Calculate exit direction (that's the entry cp for the next edge)\n",
    "\t\t\tcp = direction_to_point(path[0], path[1]) # it's ok\n",
    "\t\t\t# Not reversed because it's already relative to a switch\n",
    "\t\t\tagent_entry_node = CardinalNode(node_id, cp)\n",
    "\n",
    "\t\tholes = 0\n",
    "\t\t# Fill rail occupancy according to predicted position at ts\n",
    "\t\tfor ts in range(0, len(path)):\n",
    "\t\t\tcell = path[ts]\n",
    "\t\t\t# Find rail associated to cell\n",
    "\t\t\trail, _ = self._get_edge_from_cell(cell)\n",
    "\t\t\t# Find crossing direction\n",
    "\t\t\tif rail == -1: # Agent is on a switch\n",
    "\t\t\t\tholes += 1\n",
    "\t\t\t\t# Skip duplicated cells (for agents with fractional speed)\n",
    "\t\t\t\tif ts+1 < len(path) and cell != path[ts+1]:\n",
    "\t\t\t\t\tnode_id = self.cell_to_id_node[cell]\n",
    "\t\t\t\t\t# Calculate exit direction (that's the entry cp for the next edge)\n",
    "\t\t\t\t\tcp = direction_to_point(cell, path[ts+1])\n",
    "\t\t\t\t\t# Not reversed because it's already relative to a switch\n",
    "\t\t\t\t\tagent_entry_node = CardinalNode(node_id, cp)\n",
    "\t\t\telse: # Agent is on a rail\n",
    "\t\t\t\tcrossing_dir = None\n",
    "\t\t\t\tsrc, dst, _ = self.info[rail]\n",
    "\t\t\t\tif agent_entry_node == dst:\n",
    "\t\t\t\t\tcrossing_dir = 1\n",
    "\t\t\t\telif agent_entry_node == src: \n",
    "\t\t\t\t\tcrossing_dir = -1\n",
    "\n",
    "\t\t\t\tassert crossing_dir != None\n",
    "\n",
    "\t\t\t\tbitmap[rail, ts] = crossing_dir\n",
    "\n",
    "\t\t\t\tif holes > 0:\n",
    "\t\t\t\t\tbitmap[rail, ts-holes:ts] = crossing_dir\n",
    "\t\t\t\t\tholes = 0\n",
    "\n",
    "\t\tassert(holes == 0, \"All the cells of the bitmap should be filled\")\n",
    "\n",
    "\t\ttemp = np.any(bitmap[:, 1:(len(path)-1)] != 0, axis=0)\n",
    "\t\tassert(np.all(temp), \"Thee agent's bitmap shouldn't have holes \")\n",
    "\t\treturn bitmap\n",
    "\n",
    "\tdef _map_to_graph(self):\n",
    "\t\t\"\"\"\n",
    "\t\tBuild the representation of the map as a graph.\n",
    "\t\t:return: \n",
    "\t\t\"\"\"\n",
    "\t\tid_node_counter = 0\n",
    "\t\tconnections = {}\n",
    "\t\t# targets = [agent.target for agent in self.env.agents]\n",
    "\n",
    "\t\t# Identify cells hat are nodes (switches or diamond crossings)\n",
    "\t\tfor i in range(self.env.height):\n",
    "\t\t\tfor j in range(self.env.width):\n",
    "\n",
    "\t\t\t\tis_switch = False\n",
    "\t\t\t\tis_crossing = False\n",
    "\t\t\t\t# is_target = False\n",
    "\t\t\t\tconnections_matrix = np.zeros((4, 4))  # Matrix NESW x NESW\n",
    "\n",
    "\t\t\t\t# Check if diamond crossing\n",
    "\t\t\t\ttransitions_bit = bin(self.env.rail.get_full_transitions(i, j))\n",
    "\t\t\t\tif int(transitions_bit, 2) == int('1000010000100001', 2):\n",
    "\t\t\t\t\tis_crossing = True\n",
    "\t\t\t\t\tconnections_matrix[0, 2] = connections_matrix[2, 0] = 1\n",
    "\t\t\t\t\tconnections_matrix[1, 3] = connections_matrix[3, 1] = 1\n",
    "\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t# Check if target\n",
    "\t\t\t\t\t# if (i, j) in targets:\n",
    "\t\t\t\t\t#\tis_target = True\n",
    "\t\t\t\t\t# Check if switch\n",
    "\t\t\t\t\tfor direction in (0, 1, 2, 3):  # 0:N, 1:E, 2:S, 3:W\n",
    "\t\t\t\t\t\tpossible_transitions = self.env.rail.get_transitions(i, j, direction)\n",
    "\t\t\t\t\t\tfor t in range(4):  # Check groups of bits\n",
    "\t\t\t\t\t\t\tif possible_transitions[t]:\n",
    "\t\t\t\t\t\t\t\tinv_direction = (direction + 2) % 4\n",
    "\t\t\t\t\t\t\t\tconnections_matrix[inv_direction, t] = connections_matrix[t, inv_direction] = 1\n",
    "\t\t\t\t\t\tnum_transitions = np.count_nonzero(possible_transitions)\n",
    "\t\t\t\t\t\tif num_transitions > 1:\n",
    "\t\t\t\t\t\t\tis_switch = True\n",
    "\n",
    "\t\t\t\tif is_switch or is_crossing: #or is_target:\n",
    "\t\t\t\t\t# Add node - keep info on cell position\n",
    "\t\t\t\t\t# Update only for nodes that are switches\n",
    "\t\t\t\t\tconnections.update({id_node_counter: connections_matrix})\n",
    "\t\t\t\t\tself.id_node_to_cell.update({id_node_counter: (i, j)})\n",
    "\t\t\t\t\tself.cell_to_id_node.update({(i, j): id_node_counter})\n",
    "\t\t\t\t\tid_node_counter += 1\n",
    "\n",
    "\t\t# Enumerate edges from these nodes\n",
    "\t\tid_edge_counter = 0\n",
    "\t\t# Start from connections of one node and follow path until next switch is found\n",
    "\t\tnodes = connections.keys()  # ids\n",
    "\t\tvisited = set()  # Keeps set of CardinalNodes that were already visited\n",
    "\t\tfor n in nodes:\n",
    "\t\t\tfor cp in range(4):  # Check edges from the 4 cardinal points\n",
    "\t\t\t\tif np.count_nonzero(connections[n][cp, :]) > 0:\n",
    "\t\t\t\t\tvisited.add(CardinalNode(n, cp))  # Add to visited\n",
    "\t\t\t\t\tcells_sequence = []\n",
    "\t\t\t\t\tnode_found = False\n",
    "\t\t\t\t\tedge_length = 0\n",
    "\t\t\t\t\t# Keep going until another node is found\n",
    "\t\t\t\t\tdirection = cp\n",
    "\t\t\t\t\tpos = self.id_node_to_cell[n]\n",
    "\t\t\t\t\twhile not node_found:\n",
    "\t\t\t\t\t\tneighbour_pos = get_new_position(pos, direction)\n",
    "\t\t\t\t\t\tcells_sequence.append((neighbour_pos, direction))\n",
    "\t\t\t\t\t\tif neighbour_pos in self.cell_to_id_node:  # If neighbour is a node\n",
    "\t\t\t\t\t\t\t# node_found = True\n",
    "\t\t\t\t\t\t\t# Build edge, mark visited\n",
    "\t\t\t\t\t\t\tid_node1 = n\n",
    "\t\t\t\t\t\t\tcp1 = cp\n",
    "\t\t\t\t\t\t\tid_node2 = self.cell_to_id_node[neighbour_pos]\n",
    "\t\t\t\t\t\t\tcp2 = self._reverse_dir(direction)\n",
    "\t\t\t\t\t\t\tif CardinalNode(id_node2, cp2) not in visited:\n",
    "\t\t\t\t\t\t\t\tself.info.update({id_edge_counter:\n",
    "\t\t\t\t\t\t\t\t\t                  (CardinalNode(id_node1, cp1),\n",
    "\t\t\t\t\t\t\t\t\t                   CardinalNode(id_node2, cp2),\n",
    "\t\t\t\t\t\t\t\t\t                   edge_length)})\n",
    "\t\t\t\t\t\t\t\tcells_sequence.pop()  # Don't include this node in the edge\n",
    "\t\t\t\t\t\t\t\tself.id_edge_to_cells.update({id_edge_counter: cells_sequence})\n",
    "\t\t\t\t\t\t\t\tid_edge_counter += 1\n",
    "\t\t\t\t\t\t\t\tvisited.add(CardinalNode(id_node2, cp2))\n",
    "\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\t\tedge_length += 1  # Not considering switches in the count\n",
    "\t\t\t\t\t\t# Update pos and dir\n",
    "\t\t\t\t\t\tpos = neighbour_pos\n",
    "\t\t\t\t\t\texit_dir = self._reverse_dir(direction)\n",
    "\t\t\t\t\t\tpossible_transitions = np.array(self.env.rail.get_transitions(pos[0], pos[1], direction))\n",
    "\t\t\t\t\t\tpossible_transitions[exit_dir] = 0  # Don't consider direction from which I entered\n",
    "\t\t\t\t\t\t# t = 2\n",
    "\t\t\t\t\t\tt = np.argmax(possible_transitions)  # There's only one possible transition except the one that I took to get in\n",
    "\t\t\t\t\t\ttemp_pos = get_new_position(pos, t)\n",
    "\t\t\t\t\t\tif 0 <= temp_pos[0] < self.env.height and 0 <= temp_pos[1] < self.env.width:  # Patch - check if this cell is a rail\n",
    "\t\t\t\t\t\t\t# Entrance dir is always opposite to exit dir\n",
    "\t\t\t\t\t\t\tdirection = t\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tbreak\n",
    "\n",
    "\t\tself.nodes = nodes # Set of nodes\n",
    "\t\tself.edges = self.info.keys() # Set of edges\n",
    "\t\tself.num_rails = len(self.edges)\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _reverse_dir(direction):\n",
    "\t\t\"\"\"\n",
    "\t\tInvert direction (int) of one agent.\n",
    "\t\t:param direction: \n",
    "\t\t:return: \n",
    "\t\t\"\"\"\n",
    "\t\treturn int((direction + 2) % 4)\n",
    "\t\tpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shortest path prediction builder (Romain's one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "def get_shortest_paths(env, vis=False):\n",
    "    distance_map = DistanceMap(env.agents, env.width, env.height)\n",
    "    distance_map.reset(env.agents, env.rail)\n",
    "    distance_map.get()\n",
    "\n",
    "    # Visualize the distance map\n",
    "    if vis:\n",
    "        sp.visualize_distance_map(distance_map, 0)\n",
    "        sp.visualize_distance_map(distance_map, 1)\n",
    "\n",
    "    shortest_paths = sp.get_shortest_paths(distance_map)\n",
    "    for handle in shortest_paths.keys():\n",
    "        if len(shortest_paths) <= 1:\n",
    "            shortest_paths[handle] = 2\n",
    "        elif env.agents[handle].position is None:\n",
    "            shortest_paths[handle] = 2 # Forward = start moving in the map\n",
    "        else:\n",
    "            next_cell = shortest_paths[handle][1] # Next cell to visit\n",
    "            shortest_paths[handle] = sp.get_action_for_move(env.agents[handle].position, \n",
    "                                                        env.agents[handle].direction,\n",
    "                                                        next_cell.position,\n",
    "                                                        next_cell.direction,\n",
    "                                                        env.rail)\n",
    "    return shortest_paths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAgentShortest(TreeObsForRailEnv):\n",
    "    '''Implements shortest path observation for the agents.'''\n",
    "    def __init__(self):\n",
    "        super().__init__(max_depth=0)\n",
    "\n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "\n",
    "    def get(self, handle):\n",
    "        return get_shortest_paths(self.env)[handle]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "\n",
    "env = RailEnv(\n",
    "    width=20,\n",
    "    height=15,\n",
    "    rail_generator=SparseRailGen(\n",
    "        max_num_cities=2, \n",
    "        grid_mode=True,\n",
    "        max_rails_between_cities=2,\n",
    "        max_rail_pairs_in_city=1,\n",
    "    ),\n",
    "    line_generator=SparseLineGen(speed_ratio_map={1.: 1.}\n",
    "        ),\n",
    "    number_of_agents=2, \n",
    "    obs_builder_object=TreeObsForRailEnv(max_depth=3),\n",
    "    malfunction_generator=ParamMalfunctionGen(\n",
    "        MalfunctionParameters(\n",
    "            malfunction_rate=0.,  # Rate of malfunction\n",
    "            min_duration=3,  # Minimal duration\n",
    "            max_duration=20,  # Max duration\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "_,_ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m prediction_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m40\u001b[39m\n\u001b[1;32m----> 2\u001b[0m observation_builder \u001b[38;5;241m=\u001b[39m GlobalObsForRailEnv(bfs_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, predictor\u001b[38;5;241m=\u001b[39m\u001b[43msp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprediction_depth\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      4\u001b[0m state_size \u001b[38;5;241m=\u001b[39m prediction_depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m      5\u001b[0m network_action_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;66;03m# we limit ourselves to 2 actions : stop or move forward\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "prediction_depth = 40\n",
    "observation_builder = GlobalObsForRailEnv(bfs_depth=4, predictor=sp(max_depth=prediction_depth))\n",
    "\n",
    "state_size = prediction_depth + 5\n",
    "network_action_size = 2 # we limit ourselves to 2 actions : stop or move forward\n",
    "controller = Dueling_DQN(20, 15, network_action_size)\n",
    "railenv_action_dict = dict()\n",
    "    \n",
    "evaluation_number = 0\n",
    "while True:\n",
    "    \n",
    "    evaluation_number += 1\n",
    "    obs, info = env.reset()\n",
    "    if not obs:\n",
    "        break\n",
    "    \n",
    "    print(\"Test Number : {}\".format(evaluation_number))\n",
    "\n",
    "    number_of_agents = len(env.agents)\n",
    "    steps = 0\n",
    "    for a in range(number_of_agents):\n",
    "        action = 2\n",
    "        railenv_action_dict.update({a:action})\n",
    "    obs, all_rewards, done, info = env.step(railenv_action_dict)\n",
    "    \n",
    "    while done['__all__'] == False:\n",
    "        for a in range(number_of_agents):\n",
    "            if info['action_required'][a]:\n",
    "                network_action = controller.act(obs[a])\n",
    "                railenv_action = RailObsForRailEnv.get_agent_action(a)\n",
    "            else:\n",
    "                railenv_action = 0\n",
    "            railenv_action_dict.update({a: railenv_action})\n",
    "            \n",
    "        obs, all_rewards, done, info = env.step(railenv_action_dict)\n",
    "        steps += 1\n",
    "        \n",
    "        if done['__all__']:\n",
    "            print(\"Reward : \", sum(list(all_rewards.values())))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the policy\n",
    "obs_params = {\n",
    "    \"observation_tree_depth\": 3,\n",
    "    \"observation_radius\": 10,\n",
    "}\n",
    "train_params = {\n",
    "    \"eps_start\": 1.0,\n",
    "    \"eps_end\": 0.01,\n",
    "    \"eps_decay\": 0.99,\n",
    "    \"n_episodes\": 40,\n",
    "    \"checkpoint_interval\": 50,\n",
    "    \"n_eval_episodes\": 10,\n",
    "    \"restore_replay_buffer\": False,\n",
    "    \"save_replay_buffer\": False,\n",
    "    \"render\": False,\n",
    "    \"buffer_size\": int(1e5),\n",
    "    \"LSTM\" : True\n",
    "}\n",
    "\n",
    "\n",
    "model = Dueling_DQN(env.width, env.height, 5)\n",
    "policy = \n",
    "\n",
    "train_agent(model, policy, train_params, obs_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the policy on the best seed\n",
    "env_renderer = test_utils.render_one_test(env, policy, obs_params, seed=5, real_time_render=False, force_gif=True)\n",
    "env_renderer.make_gif('test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
