{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map-related methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('src/')\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Base flatland environment\n",
    "from flatland.envs.line_generators import SparseLineGen\n",
    "from flatland.envs.malfunction_generators import (\n",
    "    MalfunctionParameters,\n",
    "    ParamMalfunctionGen,\n",
    ")\n",
    "from flatland.envs.rail_env import RailEnv\n",
    "from flatland.envs.rail_generators import SparseRailGen\n",
    "from flatland.envs.observations import GlobalObsForRailEnv\n",
    "\n",
    "from flatland.envs.observations import TreeObsForRailEnv\n",
    "from flatland.envs.distance_map import DistanceMap\n",
    "import flatland.envs.rail_env_shortest_paths as sp\n",
    "\n",
    "from src import test_utils, training, rewards\n",
    "from src.observation_utils import normalize_observation\n",
    "from src.models import *\n",
    "from src.deep_model_policy import DeepPolicy, PolicyParameters\n",
    "\n",
    "# Visualization\n",
    "from flatland.utils.rendertools import RenderTool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "\n",
    "env = RailEnv(\n",
    "    width=20,\n",
    "    height=15,\n",
    "    rail_generator=SparseRailGen(\n",
    "        max_num_cities=2,  # Number of cities\n",
    "        grid_mode=True,\n",
    "        max_rails_between_cities=2,\n",
    "        max_rail_pairs_in_city=1,\n",
    "    ),\n",
    "    line_generator=SparseLineGen(speed_ratio_map={1.: 1.}\n",
    "        ),\n",
    "    number_of_agents=2, \n",
    "    obs_builder_object=TreeObsForRailEnv(max_depth=3),\n",
    "    malfunction_generator=ParamMalfunctionGen(\n",
    "        MalfunctionParameters(\n",
    "            malfunction_rate=0.,  # Rate of malfunction\n",
    "            min_duration=3,  # Minimal duration\n",
    "            max_duration=20,  # Max duration\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "_,_ = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY_EVERY_N_EPISODES = 50\n",
    "\n",
    "# Initialize the history of the Q-table and learning rate\n",
    "q_array_history = []\n",
    "alpha_history = []\n",
    "\n",
    "def sarsa(\n",
    "    environment: gym.Env, \n",
    "    alpha: float = 0.1, \n",
    "    alpha_factor: float = 0.9995, \n",
    "    gamma: float = 0.99, \n",
    "    epsilon: float = 0.5, \n",
    "    num_episodes: int = 10000, \n",
    "    display: bool = False\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform SARSA learning on a given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    environment : gym.Env\n",
    "        The environment to learn in.\n",
    "    alpha : float, optional\n",
    "        The learning rate, between 0 and 1. By default 0.1\n",
    "    alpha_factor : float, optional\n",
    "        The factor to decrease alpha by each episode, by default 0.9995\n",
    "    gamma : float, optional\n",
    "        The discount factor, between 0 and 1. By default 0.99\n",
    "    epsilon : float, optional\n",
    "        The probability of choosing a random action, by default 0.5\n",
    "    num_episodes : int, optional\n",
    "        The number of episodes to run, by default 10000\n",
    "    display : bool, optional\n",
    "        Whether to display the Q-table (every DISPLAY_EVERY_N_EPISODES episodes), by default False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The learned Q-table.\n",
    "        Each row corresponds to a state, and each column corresponds to an action.\n",
    "        In the frozen lake environment, there are 16 states and 4 actions thus the Q-table has a shape of (16, 4).\n",
    "        For instance, q_array[0, 3] is the Q-value (estimation of the expected reward) for performing action 3 (\"move up\") in state 0 (the top left square).\n",
    "    \"\"\"\n",
    "    # Get the number of states and actions in the environment\n",
    "    num_states = \n",
    "    num_actions = \n",
    "\n",
    "    # Initialize the Q-table to zeros\n",
    "    q_array = np.zeros([num_states, num_actions])\n",
    "\n",
    "    # Loop over the episodes\n",
    "    for episode_index in range(num_episodes):\n",
    "        # Display the Q-table every DISPLAY_EVERY_N_EPISODES episodes if display is True\n",
    "        if display and episode_index % DISPLAY_EVERY_N_EPISODES == 0:\n",
    "            qtable_display(q_array, title=\"Q table\")\n",
    "        else:\n",
    "            print('.', end=\"\")\n",
    "\n",
    "        # Save the current Q-table and learning rate\n",
    "        q_array_history_ex4.append(q_array.copy())\n",
    "        alpha_history_ex4.append(alpha)\n",
    "\n",
    "        # Decrease the learning rate if alpha_factor is not None\n",
    "        if alpha_factor is not None:\n",
    "            alpha = alpha * alpha_factor\n",
    "\n",
    "        # TODO...\n",
    "        s = environment.reset()[0]\n",
    "        done = False\n",
    "        a = epsilon_greedy_policy(s, q_array, epsilon)\n",
    "        while done is False:\n",
    "            s_, r, done, _ , _= environment.step(a)\n",
    "            a_ = epsilon_greedy_policy(s_, q_array, epsilon)\n",
    "            q_array[s, a] += alpha * (r + gamma * q_array[s_, a_] - q_array[s, a])\n",
    "            s = s_\n",
    "            a = a_        \n",
    "\n",
    "    # Return the learned Q-table\n",
    "    return q_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed strategy : bitmaps and DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 84\u001b[0m\n\u001b[0;32m     81\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;66;03m# Get action from model\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[43mDuelingDQN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     action \u001b[38;5;241m=\u001b[39m q_values\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# Step the environment\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'x'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from flatland.envs.rail_env import RailEnv\n",
    "from flatland.envs.distance_map import DistanceMap\n",
    "from flatland.envs.rail_env_shortest_paths import get_shortest_paths\n",
    "from flatland.envs.rail_generators import sparse_rail_generator\n",
    "from flatland.envs.observations import TreeObsForRailEnv\n",
    "from flatland.envs.line_generators import sparse_line_generator\n",
    "\n",
    "# Assuming that these functions are already defined as per the images provided:\n",
    "# - create_rail_env_bitmap()\n",
    "# - create_heatmaps()\n",
    "# - get_agent_action_from_shortest_path()\n",
    "\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        # Define network layers here based on the provided network architecture image\n",
    "        self.conv = nn.Conv2d(in_channels=input_shape[0], out_channels=64, kernel_size=(1, input_shape[2]))\n",
    "        self.fc_adv = nn.Linear(in_features=self.feature_size(), out_features=512)\n",
    "        self.fc_val = nn.Linear(in_features=self.feature_size(), out_features=512)\n",
    "        self.fc_adv2 = nn.Linear(in_features=512, out_features=num_actions)\n",
    "        self.fc_val2 = nn.Linear(in_features=512, out_features=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        adv = F.relu(self.fc_adv(x))\n",
    "        val = F.relu(self.fc_val(x))\n",
    "        \n",
    "        adv = self.fc_adv2(adv)\n",
    "        val = self.fc_val2(val).expand(x.size(0), self.num_actions)\n",
    "        \n",
    "        q_values = val + adv - adv.mean(1).unsqueeze(1).expand(x.size(0), self.num_actions)\n",
    "        return q_values\n",
    "\n",
    "    def feature_size(self):\n",
    "        return self.conv(torch.zeros(1, *self.input_shape)).view(1, -1).size(1)\n",
    "\n",
    "# Define the environment\n",
    "env = RailEnv(\n",
    "    width=20,\n",
    "    height=15,\n",
    "    rail_generator=SparseRailGen(\n",
    "        max_num_cities=2,  # Number of cities\n",
    "        grid_mode=True,\n",
    "        max_rails_between_cities=2,\n",
    "        max_rail_pairs_in_city=1,\n",
    "    ),\n",
    "    line_generator=SparseLineGen(speed_ratio_map={1.: 1.}\n",
    "        ),\n",
    "    number_of_agents=2, \n",
    "    obs_builder_object=TreeObsForRailEnv(max_depth=3),\n",
    "    malfunction_generator=ParamMalfunctionGen(\n",
    "        MalfunctionParameters(\n",
    "            malfunction_rate=0.,  # Rate of malfunction\n",
    "            min_duration=3,  # Minimal duration\n",
    "            max_duration=20,  # Max duration\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Reset environment to get the initial observation\n",
    "obs, _ = env.reset()\n",
    "input_shape = (3, 20, 15)  # Assuming the input shape of the network is (3, 20, 15)\n",
    "num_actions = env.action_space[0]  # Assuming action space is defined and accessible\n",
    "\n",
    "# Initialize network and optimizer\n",
    "model = DuelingDQN(input_shape, num_actions)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_episodes = 1000\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Get action from model\n",
    "        q_values = DuelingDQN.forward(state)\n",
    "        action = q_values.max(1)[1].view(1, 1)\n",
    "\n",
    "        # Step the environment\n",
    "        next_obs, reward, done, _ = env.step(action.item())\n",
    "        next_state = get_state(next_obs)  # Convert observations to model input state\n",
    "\n",
    "        # Compute the transition loss\n",
    "        loss = compute_loss(state, action, reward, next_state, done)\n",
    "\n",
    "        # Update model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning from bitmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structure:**\n",
    "\n",
    "A \"rail occupancy bitmap\" shows on which rail and in which direction the agent is traveling at every timestep and is obtained :\n",
    "1. A directed graph representation of the railway network is generated through BFS, each node is a switch and each edge is a rail between two switches.\n",
    "2. The shortest path for each agent is computed\n",
    "3. The path is transformed into a bitmap with the timesteps as columns and the rails as rows. The direction is 1 if the agent is traveling the edge from the source node to the destination node or -1 otherwise.\n",
    "\n",
    "Heatmaps are used to provide information about how the traffic is distributed across the rails over time.\n",
    "Each agent computes 2 heatmaps, one positive and one negative, both are generated summing the bitmaps of all the other agents.\n",
    "\n",
    "The general architecture is a Dueling DQN, where the input is a Conv2D layer that processes a concatenation of the agent bitmap, the positive and the negative heatmaps. Then data goes through two separate streams, the value and the advantage to be recombined in the final output Q values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "'''\n",
    "Dueling DQN model\n",
    "'''\n",
    "def dim_output(input_dim, filter_dim, stride_dim):\n",
    "    return (input_dim - filter_dim) // stride_dim + 1\n",
    "\n",
    "class Dueling_DQN(nn.Module):\n",
    "    def __init__(self, width, height, action_space):\n",
    "        super(Dueling_DQN, self).__init__()\n",
    "        self.action_space = action_space\n",
    "        # input shape (batch_size, in_channels = height/num_rails, width/prediction_depth + 1) \n",
    "        # self.conv1 = nn.Conv1d(in_channels=height, out_channels=64, kernel_size=1)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(1, width))\n",
    "\n",
    "        # output shape (batch_size, out_channels, conv_width)\n",
    "        # conv_width = dim_output(input_dim=width, filter_dim=20, stride_dim=1)\n",
    "\n",
    "        # in_features = conv_width * out_channels (feature maps/number of kernels, arbitrary)\n",
    "        # after last Conv1d\n",
    "        self.fc1_adv = nn.Linear(in_features=64 * height, out_features=512) \n",
    "        self.fc1_val = nn.Linear(in_features=64 * height, out_features=512)\n",
    "        self.fc2_adv = nn.Linear(in_features=512, out_features=action_space)\n",
    "        self.fc2_val = nn.Linear(in_features=512, out_features=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x): # \n",
    "        # batch_size = x.size(0)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        adv = self.relu(self.fc1_adv(x))\n",
    "        val = self.relu(self.fc1_val(x))\n",
    "\n",
    "        adv = self.fc2_adv(adv)\n",
    "        val = self.fc2_val(val).expand(x.size(0), self.action_space)\n",
    "\n",
    "        x = val + adv - adv.mean(1).unsqueeze(1).expand(x.size(0), self.action_space)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bitmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "class ObsPreprocessor:\n",
    "    def __init__(self, max_rails, reorder_rails):\n",
    "        self.max_rails = max_rails\n",
    "        self.reorder_rails = reorder_rails\n",
    "\n",
    "    def _fill_padding(self, obs, max_rails):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param obs: Agent state \n",
    "        :param max_rails: Maximum number of rails in environment \n",
    "        :return: Observation padded with 0s along first axis (until max_rails)\n",
    "        \n",
    "        \"\"\"\n",
    "        prediction_depth = obs.shape[1]\n",
    "        \n",
    "        pad_agent_obs = np.zeros((max_rails, prediction_depth))\n",
    "        pad_agent_obs[:obs.shape[0], :obs.shape[1]] = obs\n",
    "        \n",
    "        return pad_agent_obs\n",
    "\n",
    "    # (agents x rails x depth)\n",
    "    def _get_heatmap(self, handle, bitmaps, max_rails):\n",
    "        temp_bitmaps = np.copy(bitmaps)\n",
    "        temp_bitmaps[handle, :, :] = 0\n",
    "        pos_dir = np.sum(np.where(temp_bitmaps > 0, temp_bitmaps, 0), axis=0)\n",
    "        neg_dir = np.abs(np.sum(np.where(temp_bitmaps < 0, temp_bitmaps, 0), axis=0))\n",
    "        \n",
    "        return pos_dir, neg_dir\n",
    "\n",
    "    def _swap_rails(self, bitmap, swap):\n",
    "        bitmap[range(len(swap))] = bitmap[swap]\n",
    "        bitmap[len(swap):, :] = 0\n",
    "        return bitmap\n",
    "\n",
    "\n",
    "    def _reorder_rails(self, bitmap, pos_map, neg_map):\n",
    "        swap = np.array([], dtype=int)\n",
    "\n",
    "        ts = 0 \n",
    "        rail = np.argmax(np.absolute(bitmap[:, ts]))\n",
    "        # If agent not departed\n",
    "        if bitmap[rail, ts] == 0:\n",
    "            ts = 1\n",
    "            rail = np.argmax(np.absolute(bitmap[:, ts]))\n",
    "        \n",
    "        # While the bitmap is not empty\n",
    "        while bitmap[rail, ts] != 0:\n",
    "            swap = np.append(swap, rail)\n",
    "            ts += np.argmax(bitmap[rail, ts:] == 0)\n",
    "            rail = np.argmax(np.absolute(bitmap[:, ts]))\n",
    "\n",
    "        if len(swap) > 0:\n",
    "            bitmap = self._swap_rails(bitmap, swap)\n",
    "            pos_map = self._swap_rails(pos_map, swap)\n",
    "            neg_map = self._swap_rails(neg_map, swap)\n",
    "        \n",
    "        return bitmap, pos_map, neg_map\n",
    "\n",
    "    def get_obs(self, handle, bitmap, maps):\n",
    "        # Select subset of conflicting paths in bitmap\n",
    "        pos_map, neg_map = self._get_heatmap(handle, maps, self.max_rails)\n",
    "\n",
    "        if self.reorder_rails:\n",
    "            bitmap, pos_map, neg_map = self._reorder_rails(bitmap, pos_map, neg_map)\n",
    "\n",
    "        state = np.concatenate([\n",
    "            self._fill_padding(bitmap, self.max_rails),\n",
    "            self._fill_padding(pos_map, self.max_rails),\n",
    "            self._fill_padding(neg_map, self.max_rails)\n",
    "        ])\n",
    "        \n",
    "        return state # (prediction_depth + 1, max_cas * max_rails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from typing import Optional, List, Dict, Tuple, NamedTuple\n",
    "\n",
    "Waypoint = NamedTuple(\n",
    "    'Waypoint', [('position', Tuple[int, int]), ('direction', int)])\n",
    "\n",
    "CardinalNode = \\\n",
    "\tNamedTuple('CardinalNode', [('id_node', int), ('cardinal_point', int)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we implement a class that returns the rails occupancy as a bitmap with rails on y-axis and timesteps on x-axis. Rails are edges and the 1/-1 in the bitmap indicate the direction of the agent on the rail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n        --- timesteps --->\\nrail 0: 1 1 1       -1-1\\nrail 1:      1 1\\nrail 2:         -1-1\\n.\\n.\\nrail n:\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "        --- timesteps --->\n",
    "rail 0: 1 1 1       -1-1\n",
    "rail 1:      1 1\n",
    "rail 2:         -1-1\n",
    ".\n",
    ".\n",
    "rail n:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:379: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:382: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:379: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:382: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "C:\\Users\\lelia\\AppData\\Local\\Temp\\ipykernel_12940\\1420684031.py:379: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(holes == 0, \"All the cells of the bitmap should be filled\")\n",
      "C:\\Users\\lelia\\AppData\\Local\\Temp\\ipykernel_12940\\1420684031.py:382: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(np.all(temp), \"Thee agent's bitmap shouldn't have holes \")\n"
     ]
    }
   ],
   "source": [
    "from flatland.core.env import Environment\n",
    "from flatland.core.env_observation_builder import ObservationBuilder\n",
    "from flatland.core.grid.grid4_utils import get_new_position, direction_to_point\n",
    "from flatland.envs.rail_env import RailEnvActions\n",
    "from flatland.envs.step_utils.states import TrainState\n",
    "\n",
    "class RailObsForRailEnv(ObservationBuilder):\n",
    "\n",
    "\tdef __init__(self, predictor):\n",
    "\t\t\"\"\"\n",
    "\t\tpredictor: class that predicts the path.\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(RailObsForRailEnv, self).__init__()\n",
    "\t\tself.predictor = predictor\n",
    "\t\t\n",
    "\t\tself.num_agents = None\n",
    "\t\tself.num_rails = None # Depends on the map, must be computed in reset()\n",
    "\t\tself.max_time_steps = self.predictor.max_depth\n",
    "\n",
    "\t\t# Not all of them are necessary\n",
    "\t\tself.cell_to_id_node = {} # Map cell position : id_node\n",
    "\t\tself.id_node_to_cell = {} # Map id_node to cell position\n",
    "\t\tself.info = {} # Map id_edge : tuple (CardinalNode1, CardinalNode2, edge_length)\n",
    "\t\tself.id_edge_to_cells = {} # Map id_edge : list of tuples (cell pos, crossing dir) in rail (nodes are not counted)\n",
    "\t\tself.nodes = set() # Set of node ids\n",
    "\t\tself.edges = set() # Set of edge ids\n",
    "\n",
    "\t\tself.recompute_bitmap = True\n",
    "\n",
    "\tdef set_env(self, env: Environment):\n",
    "\t\tsuper().set_env(env)\n",
    "\t\tif self.predictor:\n",
    "\t\t\t# Use set_env available in PredictionBuilder (parent class)\n",
    "\t\t\tself.predictor.set_env(self.env)\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\tself.cell_to_id_node = {}\n",
    "\t\tself.id_node_to_cell = {}\n",
    "\t\tself.info = {}\n",
    "\t\tself.id_edge_to_cells = {}\n",
    "\t\tself.nodes = set()\n",
    "\t\tself.edges = set()\n",
    "\t\tself._map_to_graph()\n",
    "\t\tself.recompute_bitmap = True\n",
    "\n",
    "\t\tself.num_agents = len(self.env.agents)\n",
    "\n",
    "\t\t# Calculate agents timesteps per cell\n",
    "\t\tself.tpc = dict()\n",
    "\t\tfor a in range(self.num_agents):\n",
    "\t\t\tagent_speed = self.env.agents[a].speed_data['speed']\n",
    "\t\t\tself.tpc[a] = int(np.reciprocal(agent_speed))\n",
    "\t\t\n",
    "\tdef get_many(self, handles: Optional[List[int]] = None):\n",
    "\t\tmaps = None\n",
    "\t\t# Compute bitmaps from shortest paths\n",
    "\t\tif self.recompute_bitmap:\n",
    "\t\t\tself.recompute_bitmap = False\n",
    "\n",
    "\t\t\tprediction_dict = self.predictor.get()\n",
    "\t\t\tself.paths = self.predictor.shortest_paths\n",
    "\t\t\tcells_sequence = self.predictor.compute_cells_sequence(prediction_dict)\n",
    "\n",
    "\t\t\tmaps = np.zeros((self.num_agents, self.num_rails, self.max_time_steps + 1), dtype=int)\n",
    "\t\t\tfor a in range(self.num_agents):\n",
    "\t\t\t\tmaps[a, :, :] = self._bitmap_from_cells_seq(a, cells_sequence[a])\n",
    "\n",
    "\t\t\tmaps = np.roll(maps, 1)\n",
    "\t\t\tmaps[:, :, 0] = 0\n",
    "\n",
    "\t\treturn maps\n",
    "\n",
    "\tdef get_altmaps(self, handle):\n",
    "\t\tagent = self.env.agents[handle]\n",
    "\t\taltpaths, cells_seqs = self.predictor.get_altpaths(handle, self.cell_to_id_node)\n",
    "\t\tmaps = []\n",
    "\t\tfor i in range(len(cells_seqs)):\n",
    "\t\t\tbitmap = self._bitmap_from_cells_seq(handle, cells_seqs[i])\n",
    "\n",
    "\t\t\t# If agent not departed, add 0 at the beginning\n",
    "\t\t\tif agent.status == TrainState.READY_TO_DEPART:\n",
    "\t\t\t\tbitmap[:, -1] = 0\n",
    "\t\t\t\tbitmap = np.roll(bitmap, 1)\n",
    "\n",
    "\t\t\tmaps.append(bitmap)\n",
    "\n",
    "\t\treturn maps, altpaths\n",
    "\n",
    "\tdef get_agent_action(self, handle):\n",
    "\t\tagent = self.env.agents[handle]\n",
    "\t\taction = RailEnvActions.DO_NOTHING\n",
    "\t\t\n",
    "\t\tif agent.status == TrainState.READY_TO_DEPART:\n",
    "\t\t\taction = RailEnvActions.MOVE_FORWARD\n",
    "\n",
    "\t\telif agent.status == TrainState.MOVING:\n",
    "\t\t\t# This can return None when rails are disconnected or there was an error in the DistanceMap\n",
    "\t\t\tif self.paths[handle] is None or len(self.paths[handle]) == 0:  # Railway disrupted\n",
    "\t\t\t\tprint('[WARN] AGENT {} RAIL DISRUPTED'.format(handle))\n",
    "\t\t\t\taction = RailEnvActions.STOP_MOVING\n",
    "\t\t\telse:\n",
    "\t\t\t\t# Get action\n",
    "\t\t\t\tstep = self.paths[handle][0]\n",
    "\t\t\t\tnext_action_element = step.next_action_element.action  # Get next_action_element\n",
    "\n",
    "\t\t\t\t# Just to use the correct form/name\n",
    "\t\t\t\tif next_action_element == 1:\n",
    "\t\t\t\t\taction = RailEnvActions.MOVE_LEFT\n",
    "\t\t\t\telif next_action_element == 2:\n",
    "\t\t\t\t\taction = RailEnvActions.MOVE_FORWARD\n",
    "\t\t\t\telif next_action_element == 3:\n",
    "\t\t\t\t\taction = RailEnvActions.MOVE_RIGHT\n",
    "\t\t\t\t\n",
    "\t\t\t\tself.paths[handle] = self.paths[handle][1:]\n",
    "\n",
    "\t\treturn action\n",
    "\n",
    "\tdef is_before_switch(self, a):\n",
    "\t\tagent = self.env.agents[a]\n",
    "\t\tbefore_switch = False\n",
    "\n",
    "\t\tif agent.state == TrainState.MOVING :\n",
    "\t\t\tif len(self.paths[a]) > 0:\n",
    "\t\t\t\tcurr_pos = agent.position\n",
    "\t\t\t\tnext_pos = self.paths[a][0].next_action_element.next_position\n",
    "\t\t\t\tcurr_rail, _ = self._get_edge_from_cell(curr_pos)\n",
    "\t\t\t\tnext_rail, _ = self._get_edge_from_cell(next_pos)\n",
    "\t\t\t\tbefore_switch = curr_rail != -1 and next_rail == -1\n",
    "\t\t\telse:\n",
    "\t\t\t\t# This shouldn't happen, but it may happen\n",
    "\t\t\t\tprint('[WARN] agent\\'s {} path run out'.format(a))\n",
    "\t\t\t\t# Force path recalc\n",
    "\t\t\t\tbefore_switch = True\n",
    "\n",
    "\t\treturn before_switch\n",
    "\n",
    "\tdef _get_rail_dir(self, a, maps, ts=0):\n",
    "\t\trail = np.argmax(np.absolute(maps[a, :, ts]))\n",
    "\t\tdirection = maps[a, rail, ts]\n",
    "\t\treturn rail, direction\n",
    "\n",
    "\t# This should only be used by a train to delay itself\n",
    "\tdef _delay(self, a, maps, rail, direction, delay):\n",
    "\t\ttpc = self.tpc[a]\n",
    "\n",
    "\t\told_rail, old_dir = self._get_rail_dir(a, maps)\n",
    "\n",
    "\t\tmaps[a] = np.roll(maps[a], delay)\n",
    "\t\t# Reset the first bits\n",
    "\t\tmaps[a, :, 0:delay+tpc] = 0\n",
    "\t\t# Fill the first with the current rail info\n",
    "\t\tmaps[a, old_rail, 0:tpc] = old_dir\n",
    "\t\t# Add delay to the next rail\n",
    "\t\tmaps[a, rail, tpc:tpc+delay] = direction\n",
    "\t\t\n",
    "\t\treturn maps\n",
    "\n",
    "\tdef _is_cell_occupied(self, a, cell):\n",
    "\t\toccupied = False\n",
    "\n",
    "\t\tfor other in range(self.env.get_num_agents()):\n",
    "\t\t\tif other != a and self.env.agents[other].position == cell:\n",
    "\t\t\t\toccupied = True\n",
    "\t\t\t\tbreak\n",
    "\t\t\n",
    "\t\treturn occupied\n",
    "\n",
    "\tdef _check_headon_crash(self, a, rail, direction, maps):\n",
    "\t\tcrash = False\n",
    "\n",
    "\t\t# Check if rail is already occupied to compute new exit time\n",
    "\t\tlast, last_exit = self._last_train_on_rail(a, rail, maps)\n",
    "\n",
    "\t\tif last_exit > 0:\n",
    "\t\t\t# last_exit-1 instead of 0, because in 0 it may be crossing the last\n",
    "\t\t\t# cell before the switch\n",
    "\t\t\tlast_dir = maps[last, rail, last_exit - 1]\n",
    "\t\t\tcrash = last_dir != direction\n",
    "\n",
    "\t\treturn crash\n",
    "\n",
    "\tdef check_crash(self, a, maps, is_before_switch=False):\n",
    "\t\tcrash = False\n",
    "\t\tagent = self.env.agents[a]\n",
    "\n",
    "\t\tif agent.status == TrainState.READY_TO_DEPART:\n",
    "\t\t\t# init_pos not occupied\n",
    "\t\t\tnext_pos = agent.initial_position\n",
    "\t\t\tcrash = self._is_cell_occupied(a, next_pos)\n",
    "\n",
    "\t\t\tif not crash:\n",
    "\t\t\t\t# We should skip the first bit that is 0\n",
    "\t\t\t\trail, direction = self._get_rail_dir(a, maps, ts=1)\n",
    "\t\t\t\tcrash = self._check_headon_crash(a, rail, direction, maps)\n",
    "\n",
    "\t\telif is_before_switch:\n",
    "\t\t\ttpc = self.tpc[a]\n",
    "\t\t\tnext_rail, next_dir = self._get_rail_dir(a, maps, ts=tpc)\n",
    "\t\t\tcrash = self._check_headon_crash(a, next_rail, next_dir, maps)\n",
    "\n",
    "\t\telse: # action_required\n",
    "\t\t\tif len(self.paths[a]) > 0:\n",
    "\t\t\t\tnext_pos = self.paths[a][0].next_action_element.next_position\n",
    "\t\t\t\tcrash = self._is_cell_occupied(a, next_pos)\n",
    "\n",
    "\t\treturn crash\n",
    "\n",
    "\tdef update_bitmaps(self, a, maps, is_before_switch=False):\n",
    "\t\t# Calculate exit time when switching rail\n",
    "\t\tif is_before_switch:\n",
    "\t\t\ttpc = self.tpc[a]\n",
    "\t\t\tnext_rail, next_dir = self._get_rail_dir(a, maps, ts=tpc)\n",
    "\n",
    "\t\t\t# Check if rail is already occupied to compute new exit time\n",
    "\t\t\t_, last_exit = self._last_train_on_rail(a, next_rail, maps)\n",
    "\t\t\tif last_exit > 0:\n",
    "\t\t\t\t# tpc: skips the first bits that are curr_rail\n",
    "\t\t\t\tcurr_exit = np.argmax(maps[a, next_rail, tpc:] == 0)\n",
    "\t\t\t\t# Also consider the last cell of curr_rail\n",
    "\t\t\t\tcurr_exit += tpc\n",
    "\t\t\t\t# TODO? something changes if the last id is > or <  ?\n",
    "\t\t\t\tif curr_exit <= last_exit:\n",
    "\t\t\t\t\tdelay = last_exit + tpc - curr_exit\n",
    "\t\t\t\t\tmaps = self._delay(a, maps, next_rail, next_dir, delay)\n",
    "\n",
    "\t\tmaps[a, :, 0] = 0\n",
    "\t\tmaps[a] = np.roll(maps[a], -1)\n",
    "\t\treturn maps\n",
    "\t\n",
    "\tdef set_agent_path(self, a, path):\n",
    "\t\tself.paths[a] = path\n",
    "\n",
    "\tdef _last_train_on_rail(self, a, rail, maps):\n",
    "\t\t\"\"\"\n",
    "\t\tFind train preceding agent 'handle' on rail.\n",
    "\t\t:param maps: \n",
    "\t\t:param rail: \n",
    "\t\t:param handle: \n",
    "\t\t:return: \n",
    "\t\t\"\"\"\n",
    "\t\tlast, last_exit = 0, 0 # Final train, its expected exit time\n",
    "\n",
    "\t\tfor other in range(self.env.get_num_agents()):\n",
    "\t\t\tif other == a or self.env.agents[other].status == TrainState.READY_TO_DEPART:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\ttpc = self.tpc[other]\n",
    "\n",
    "\t\t\t# If agent is already on this rail\n",
    "\t\t\tif maps[other, rail, 0] != 0:\n",
    "\t\t\t\t# Add the estimated exit time\n",
    "\t\t\t\tother_exit = np.argmax(maps[other, rail, :] == 0)\n",
    "\n",
    "\t\t\t\tif other_exit > last_exit:\n",
    "\t\t\t\t\tlast, last_exit = other, other_exit\n",
    "\n",
    "\t\t\t# We use tpc-1, to skip the first bits of trains that have decided\n",
    "\t\t\t# to enter rail, but are still crossing the cell before\n",
    "\t\t\t# If an agent has not yet decided in tpc-1 it will be in the old rail \n",
    "\t\t\telif maps[other, rail, tpc - 1] != 0:\n",
    "\t\t\t\tother_rail, _ = self._get_rail_dir(other, maps)\n",
    "\t\t\t\tother_exit = 0\n",
    "\n",
    "\t\t\t\t# Consider the time to cross the current cell\n",
    "\t\t\t\tif other_rail != rail:\n",
    "\t\t\t\t\tother_exit = np.argmax(maps[other, other_rail, :] == 0)\n",
    "\n",
    "\t\t\t\t# TODO! CHECK\n",
    "\t\t\t\t# Add the estimated exit time\n",
    "\t\t\t\tother_exit += np.argmax(maps[other, rail, other_exit:] == 0)\n",
    "\n",
    "\t\t\t\tif other_exit > last_exit:\n",
    "\t\t\t\t\tlast, last_exit = other, other_exit\n",
    "\n",
    "\t\treturn last, last_exit\n",
    "\n",
    "\tdef _get_trains_on_rails(self, maps, rail, handle):\n",
    "\t\ttrains = []\n",
    "\t\tfor a in range(self.env.get_num_agents()):\n",
    "\t\t\tif not (maps[a, rail, 0] == 0 or a == handle):\n",
    "\t\t\t\texpected_exit_time = np.argmax(maps[a, rail, :] == 0) # Takes index/ts of last bit in a row\n",
    "\t\t\t\ttrains.append((a, expected_exit_time))\n",
    "\t\ttrains.sort()\n",
    "\t\t\n",
    "\t\treturn trains\n",
    "\n",
    "\tdef _get_edge_from_cell(self, cell):\n",
    "\t\t\"\"\"\n",
    "\t\t:param cell: Cell for which we want to find the associated rail id.\n",
    "\t\t:return: A tuple (id rail, dist) where dist is the distance as offset from the beginning of the rail.\n",
    "\t\t\"\"\"\n",
    "\t\tfor edge in self.id_edge_to_cells.keys():\n",
    "\t\t\tcells = [cell[0] for cell in self.id_edge_to_cells[edge]] \n",
    "\t\t\tif cell in cells:\n",
    "\t\t\t\treturn edge, cells.index(cell)\n",
    "\n",
    "\t\treturn -1, -1  # Node\n",
    "\n",
    "\tdef _bitmap_from_cells_seq(self, handle, path) -> np.ndarray:\n",
    "\t\t\"\"\"\n",
    "\t\tCompute bitmap for agent handle, given a selected path.\n",
    "\t\t:param handle: \n",
    "\t\t:return: \n",
    "\t\t\"\"\"\n",
    "\t\tbitmap = np.zeros((self.num_rails, self.max_time_steps + 1), dtype=int)  # Max steps in the future + current ts\n",
    "\t\tagent = self.env.agents[handle]\n",
    "\t\t# Truncate path in the future, after reaching target\n",
    "\t\ttarget_index = [i for i, pos in enumerate(path) if pos[0] == agent.target[0] and pos[1] == agent.target[1]]\n",
    "\t\tif len(target_index) != 0:\n",
    "\t\t\ttarget_index = target_index[0]\n",
    "\t\t\tpath = path[:target_index + 1]\n",
    "\n",
    "\t\t# Add 0 at first ts - for 'not departed yet'\n",
    "\t\trail, _ = self._get_edge_from_cell(path[0])\n",
    "\n",
    "\t\t# Agent's cardinal node, where it entered the last edge\n",
    "\t\tagent_entry_node = None\n",
    "\t\t# Calculate initial edge entry point\n",
    "\t\ti = 0\n",
    "\t\trail, _ = self._get_edge_from_cell(path[i])\n",
    "\t\tif rail != -1: # If it's on an edge\n",
    "\t\t\tinitial_rail = rail\n",
    "\t\t\t# Search first switch\n",
    "\t\t\twhile rail != -1:\n",
    "\t\t\t\ti += 1\n",
    "\t\t\t\trail, _ = self._get_edge_from_cell(path[i])\n",
    "\n",
    "\t\t\tsrc, dst, _ = self.info[initial_rail]\n",
    "\t\t\tnode_id = self.cell_to_id_node[path[i]]\n",
    "\t\t\t# Reversed because we want the switch's cp\n",
    "\t\t\tentry_cp = self._reverse_dir(direction_to_point(path[i-1], path[i]))\n",
    "\t\t\t# If we reach the dst node\n",
    "\t\t\tif (node_id, entry_cp) == dst:\n",
    "\t\t\t\t# We entered from the src node (cross_dir = 1)\n",
    "\t\t\t\tagent_entry_node = src\n",
    "\t\t\t# Otherwise the opposite\n",
    "\t\t\telif (node_id, entry_cp) == src: \n",
    "\t\t\t\tagent_entry_node = dst\n",
    "\t\telse:\n",
    "\t\t\t#Handle the case you call this while on a switch before a rail\n",
    "\t\t\tnode_id = self.cell_to_id_node[path[i]]\n",
    "\t\t\t# Calculate exit direction (that's the entry cp for the next edge)\n",
    "\t\t\tcp = direction_to_point(path[0], path[1]) # it's ok\n",
    "\t\t\t# Not reversed because it's already relative to a switch\n",
    "\t\t\tagent_entry_node = CardinalNode(node_id, cp)\n",
    "\n",
    "\t\tholes = 0\n",
    "\t\t# Fill rail occupancy according to predicted position at ts\n",
    "\t\tfor ts in range(0, len(path)):\n",
    "\t\t\tcell = path[ts]\n",
    "\t\t\t# Find rail associated to cell\n",
    "\t\t\trail, _ = self._get_edge_from_cell(cell)\n",
    "\t\t\t# Find crossing direction\n",
    "\t\t\tif rail == -1: # Agent is on a switch\n",
    "\t\t\t\tholes += 1\n",
    "\t\t\t\t# Skip duplicated cells (for agents with fractional speed)\n",
    "\t\t\t\tif ts+1 < len(path) and cell != path[ts+1]:\n",
    "\t\t\t\t\tnode_id = self.cell_to_id_node[cell]\n",
    "\t\t\t\t\t# Calculate exit direction (that's the entry cp for the next edge)\n",
    "\t\t\t\t\tcp = direction_to_point(cell, path[ts+1])\n",
    "\t\t\t\t\t# Not reversed because it's already relative to a switch\n",
    "\t\t\t\t\tagent_entry_node = CardinalNode(node_id, cp)\n",
    "\t\t\telse: # Agent is on a rail\n",
    "\t\t\t\tcrossing_dir = None\n",
    "\t\t\t\tsrc, dst, _ = self.info[rail]\n",
    "\t\t\t\tif agent_entry_node == dst:\n",
    "\t\t\t\t\tcrossing_dir = 1\n",
    "\t\t\t\telif agent_entry_node == src: \n",
    "\t\t\t\t\tcrossing_dir = -1\n",
    "\n",
    "\t\t\t\tassert crossing_dir != None\n",
    "\n",
    "\t\t\t\tbitmap[rail, ts] = crossing_dir\n",
    "\n",
    "\t\t\t\tif holes > 0:\n",
    "\t\t\t\t\tbitmap[rail, ts-holes:ts] = crossing_dir\n",
    "\t\t\t\t\tholes = 0\n",
    "\n",
    "\t\tassert(holes == 0, \"All the cells of the bitmap should be filled\")\n",
    "\n",
    "\t\ttemp = np.any(bitmap[:, 1:(len(path)-1)] != 0, axis=0)\n",
    "\t\tassert(np.all(temp), \"Thee agent's bitmap shouldn't have holes \")\n",
    "\t\treturn bitmap\n",
    "\n",
    "\t# Slightly modified wrt to the other\n",
    "\tdef _map_to_graph(self):\n",
    "\t\t\"\"\"\n",
    "\t\tBuild the representation of the map as a graph.\n",
    "\t\t:return: \n",
    "\t\t\"\"\"\n",
    "\t\tid_node_counter = 0\n",
    "\t\tconnections = {}\n",
    "\t\t# targets = [agent.target for agent in self.env.agents]\n",
    "\n",
    "\t\t# Identify cells hat are nodes (switches or diamond crossings)\n",
    "\t\tfor i in range(self.env.height):\n",
    "\t\t\tfor j in range(self.env.width):\n",
    "\n",
    "\t\t\t\tis_switch = False\n",
    "\t\t\t\tis_crossing = False\n",
    "\t\t\t\t# is_target = False\n",
    "\t\t\t\tconnections_matrix = np.zeros((4, 4))  # Matrix NESW x NESW\n",
    "\n",
    "\t\t\t\t# Check if diamond crossing\n",
    "\t\t\t\ttransitions_bit = bin(self.env.rail.get_full_transitions(i, j))\n",
    "\t\t\t\tif int(transitions_bit, 2) == int('1000010000100001', 2):\n",
    "\t\t\t\t\tis_crossing = True\n",
    "\t\t\t\t\tconnections_matrix[0, 2] = connections_matrix[2, 0] = 1\n",
    "\t\t\t\t\tconnections_matrix[1, 3] = connections_matrix[3, 1] = 1\n",
    "\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t# Check if target\n",
    "\t\t\t\t\t# if (i, j) in targets:\n",
    "\t\t\t\t\t#\tis_target = True\n",
    "\t\t\t\t\t# Check if switch\n",
    "\t\t\t\t\tfor direction in (0, 1, 2, 3):  # 0:N, 1:E, 2:S, 3:W\n",
    "\t\t\t\t\t\tpossible_transitions = self.env.rail.get_transitions(i, j, direction)\n",
    "\t\t\t\t\t\tfor t in range(4):  # Check groups of bits\n",
    "\t\t\t\t\t\t\tif possible_transitions[t]:\n",
    "\t\t\t\t\t\t\t\tinv_direction = (direction + 2) % 4\n",
    "\t\t\t\t\t\t\t\tconnections_matrix[inv_direction, t] = connections_matrix[t, inv_direction] = 1\n",
    "\t\t\t\t\t\tnum_transitions = np.count_nonzero(possible_transitions)\n",
    "\t\t\t\t\t\tif num_transitions > 1:\n",
    "\t\t\t\t\t\t\tis_switch = True\n",
    "\n",
    "\t\t\t\tif is_switch or is_crossing: #or is_target:\n",
    "\t\t\t\t\t# Add node - keep info on cell position\n",
    "\t\t\t\t\t# Update only for nodes that are switches\n",
    "\t\t\t\t\tconnections.update({id_node_counter: connections_matrix})\n",
    "\t\t\t\t\tself.id_node_to_cell.update({id_node_counter: (i, j)})\n",
    "\t\t\t\t\tself.cell_to_id_node.update({(i, j): id_node_counter})\n",
    "\t\t\t\t\tid_node_counter += 1\n",
    "\n",
    "\t\t# Enumerate edges from these nodes\n",
    "\t\tid_edge_counter = 0\n",
    "\t\t# Start from connections of one node and follow path until next switch is found\n",
    "\t\tnodes = connections.keys()  # ids\n",
    "\t\tvisited = set()  # Keeps set of CardinalNodes that were already visited\n",
    "\t\tfor n in nodes:\n",
    "\t\t\tfor cp in range(4):  # Check edges from the 4 cardinal points\n",
    "\t\t\t\tif np.count_nonzero(connections[n][cp, :]) > 0:\n",
    "\t\t\t\t\tvisited.add(CardinalNode(n, cp))  # Add to visited\n",
    "\t\t\t\t\tcells_sequence = []\n",
    "\t\t\t\t\tnode_found = False\n",
    "\t\t\t\t\tedge_length = 0\n",
    "\t\t\t\t\t# Keep going until another node is found\n",
    "\t\t\t\t\tdirection = cp\n",
    "\t\t\t\t\tpos = self.id_node_to_cell[n]\n",
    "\t\t\t\t\twhile not node_found:\n",
    "\t\t\t\t\t\tneighbour_pos = get_new_position(pos, direction)\n",
    "\t\t\t\t\t\tcells_sequence.append((neighbour_pos, direction))\n",
    "\t\t\t\t\t\tif neighbour_pos in self.cell_to_id_node:  # If neighbour is a node\n",
    "\t\t\t\t\t\t\t# node_found = True\n",
    "\t\t\t\t\t\t\t# Build edge, mark visited\n",
    "\t\t\t\t\t\t\tid_node1 = n\n",
    "\t\t\t\t\t\t\tcp1 = cp\n",
    "\t\t\t\t\t\t\tid_node2 = self.cell_to_id_node[neighbour_pos]\n",
    "\t\t\t\t\t\t\tcp2 = self._reverse_dir(direction)\n",
    "\t\t\t\t\t\t\tif CardinalNode(id_node2, cp2) not in visited:\n",
    "\t\t\t\t\t\t\t\tself.info.update({id_edge_counter:\n",
    "\t\t\t\t\t\t\t\t\t                  (CardinalNode(id_node1, cp1),\n",
    "\t\t\t\t\t\t\t\t\t                   CardinalNode(id_node2, cp2),\n",
    "\t\t\t\t\t\t\t\t\t                   edge_length)})\n",
    "\t\t\t\t\t\t\t\tcells_sequence.pop()  # Don't include this node in the edge\n",
    "\t\t\t\t\t\t\t\tself.id_edge_to_cells.update({id_edge_counter: cells_sequence})\n",
    "\t\t\t\t\t\t\t\tid_edge_counter += 1\n",
    "\t\t\t\t\t\t\t\tvisited.add(CardinalNode(id_node2, cp2))\n",
    "\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\t\tedge_length += 1  # Not considering switches in the count\n",
    "\t\t\t\t\t\t# Update pos and dir\n",
    "\t\t\t\t\t\tpos = neighbour_pos\n",
    "\t\t\t\t\t\texit_dir = self._reverse_dir(direction)\n",
    "\t\t\t\t\t\tpossible_transitions = np.array(self.env.rail.get_transitions(pos[0], pos[1], direction))\n",
    "\t\t\t\t\t\tpossible_transitions[exit_dir] = 0  # Don't consider direction from which I entered\n",
    "\t\t\t\t\t\t# t = 2\n",
    "\t\t\t\t\t\tt = np.argmax(possible_transitions)  # There's only one possible transition except the one that I took to get in\n",
    "\t\t\t\t\t\ttemp_pos = get_new_position(pos, t)\n",
    "\t\t\t\t\t\tif 0 <= temp_pos[0] < self.env.height and 0 <= temp_pos[1] < self.env.width:  # Patch - check if this cell is a rail\n",
    "\t\t\t\t\t\t\t# Entrance dir is always opposite to exit dir\n",
    "\t\t\t\t\t\t\tdirection = t\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tbreak\n",
    "\n",
    "\t\tself.nodes = nodes # Set of nodes\n",
    "\t\tself.edges = self.info.keys() # Set of edges\n",
    "\t\tself.num_rails = len(self.edges)\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _reverse_dir(direction):\n",
    "\t\t\"\"\"\n",
    "\t\tInvert direction (int) of one agent.\n",
    "\t\t:param direction: \n",
    "\t\t:return: \n",
    "\t\t\"\"\"\n",
    "\t\treturn int((direction + 2) % 4)\n",
    "\t\tpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shortest path prediction builder (Romain's one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "def get_shortest_paths(env, vis=False):\n",
    "    distance_map = DistanceMap(env.agents, env.width, env.height)\n",
    "    distance_map.reset(env.agents, env.rail)\n",
    "    distance_map.get()\n",
    "\n",
    "    # Visualize the distance map\n",
    "    if vis:\n",
    "        sp.visualize_distance_map(distance_map, 0)\n",
    "        sp.visualize_distance_map(distance_map, 1)\n",
    "\n",
    "    shortest_paths = sp.get_shortest_paths(distance_map)\n",
    "    for handle in shortest_paths.keys():\n",
    "        if len(shortest_paths) <= 1:\n",
    "            shortest_paths[handle] = 2\n",
    "        elif env.agents[handle].position is None:\n",
    "            shortest_paths[handle] = 2 # Forward = start moving in the map\n",
    "        else:\n",
    "            next_cell = shortest_paths[handle][1] # Next cell to visit\n",
    "            shortest_paths[handle] = sp.get_action_for_move(env.agents[handle].position, \n",
    "                                                        env.agents[handle].direction,\n",
    "                                                        next_cell.position,\n",
    "                                                        next_cell.direction,\n",
    "                                                        env.rail)\n",
    "    return shortest_paths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAgentShortest(TreeObsForRailEnv):\n",
    "    '''Implements shortest path observation for the agents.'''\n",
    "    def __init__(self):\n",
    "        super().__init__(max_depth=0)\n",
    "\n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "\n",
    "    def get(self, handle):\n",
    "        return get_shortest_paths(self.env)[handle]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "\n",
    "env = RailEnv(\n",
    "    width=20,\n",
    "    height=15,\n",
    "    rail_generator=SparseRailGen(\n",
    "        max_num_cities=2,  # Number of cities\n",
    "        grid_mode=True,\n",
    "        max_rails_between_cities=2,\n",
    "        max_rail_pairs_in_city=1,\n",
    "    ),\n",
    "    line_generator=SparseLineGen(speed_ratio_map={1.: 1.}\n",
    "        ),\n",
    "    number_of_agents=2, \n",
    "    obs_builder_object=TreeObsForRailEnv(max_depth=3),\n",
    "    malfunction_generator=ParamMalfunctionGen(\n",
    "        MalfunctionParameters(\n",
    "            malfunction_rate=0.,  # Rate of malfunction\n",
    "            min_duration=3,  # Minimal duration\n",
    "            max_duration=20,  # Max duration\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "_,_ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flatland.envs.observation_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimportlib_resources\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m path\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflatland\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mobservation_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GraphObsForRailEnv\n\u001b[0;32m      4\u001b[0m prediction_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m40\u001b[39m\n\u001b[0;32m      5\u001b[0m observation_builder \u001b[38;5;241m=\u001b[39m GraphObsForRailEnv(bfs_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, predictor\u001b[38;5;241m=\u001b[39mShortestPathPredictorForRailEnv(max_depth\u001b[38;5;241m=\u001b[39mprediction_depth))\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'flatland.envs.observation_utils'"
     ]
    }
   ],
   "source": [
    "from importlib_resources import path\n",
    "from flatland.envs.observation_utils import GraphObsForRailEnv\n",
    "\n",
    "prediction_depth = 40\n",
    "observation_builder = GraphObsForRailEnv(bfs_depth=4, predictor=ShortestPathPredictorForRailEnv(max_depth=prediction_depth))\n",
    "\n",
    "state_size = prediction_depth + 5\n",
    "network_action_size = 2\n",
    "controller = Agent('fc', state_size, network_action_size)\n",
    "railenv_action_dict = dict()\n",
    "\n",
    "    \n",
    "evaluation_number = 0\n",
    "while True:\n",
    "    \n",
    "    evaluation_number += 1\n",
    "    time_start = time.time()\n",
    "    \n",
    "    obs, info = env.reset()\n",
    "    if not obs:\n",
    "        break\n",
    "    \n",
    "    print(\"Test Number : {}\".format(evaluation_number))\n",
    "\n",
    "    local_env = env\n",
    "    number_of_agents = len(env.agents)\n",
    "    \n",
    "    time_taken_by_controller = []\n",
    "    time_taken_per_step = []\n",
    "    steps = 0\n",
    "    # First random action\n",
    "    for a in range(number_of_agents):\n",
    "        action = 2\n",
    "        railenv_action_dict.update({a:action})\n",
    "    obs, all_rewards, done, info = env.step(railenv_action_dict)\n",
    "\n",
    "    while True:\n",
    "        # Evaluation of a single episode\n",
    "    \n",
    "        time_start = time.time()\n",
    "        # Pick actions\n",
    "        for a in range(number_of_agents):\n",
    "            if info['action_required'][a]:\n",
    "                network_action = controller.act(obs[a])\n",
    "                railenv_action = observation_builder.choose_railenv_action(a, network_action)\n",
    "            else:\n",
    "                railenv_action = 0\n",
    "            railenv_action_dict.update({a: railenv_action})\n",
    "                \n",
    "        time_taken = time.time() - time_start\n",
    "        time_taken_by_controller.append(time_taken)\n",
    "\n",
    "        time_start = time.time()\n",
    "        # Perform env step\n",
    "        obs, all_rewards, done, info = env.step(railenv_action_dict)\n",
    "        steps += 1\n",
    "        time_taken = time.time() - time_start\n",
    "        time_taken_per_step.append(time_taken)\n",
    "        \n",
    "        if done['__all__']:\n",
    "            print(\"Reward : \", sum(list(all_rewards.values())))\n",
    "\n",
    "            break\n",
    "\n",
    "    np_time_taken_by_controller = np.array(time_taken_by_controller)\n",
    "    np_time_taken_per_step = np.array(time_taken_per_step)\n",
    "    print(\"=\" * 100)\n",
    "    print(\"=\" * 100)\n",
    "    print(\"Evaluation Number : \", evaluation_number)\n",
    "    print(\"Current Env Path : \", env.path)\n",
    "    print(\"Env Creation Time : \", env_creation_time)\n",
    "    print(\"Number of Steps : \", steps)\n",
    "    print(\"Mean/Std of Time taken by Controller : \", np_time_taken_by_controller.mean(),\n",
    "          np_time_taken_by_controller.std())\n",
    "    print(\"Mean/Std of Time per Step : \", np_time_taken_per_step.mean(), np_time_taken_per_step.std())\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "print(\"Evaluation of all environments complete...\")\n",
    "\n",
    "print(remote_client.submit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from flatland.envs.rail_generators import sparse_rail_generator\n",
    "from flatland.envs.line_generators import SparseLineGen\n",
    "from flatland.envs.malfunction_generators import malfunction_from_params\n",
    "from flatland.utils.rendertools import RenderTool, AgentRenderVariant\n",
    "from flatland.envs.rail_env import RailEnvActions\n",
    "\n",
    "\n",
    "\n",
    "def main(args):\n",
    "\t\n",
    "\t# Show options and values\n",
    "\tprint(' ' * 26 + 'Options')\n",
    "\tfor k, v in vars(args).items():\n",
    "\t\tprint(' ' * 26 + k + ': ' + str(v))\n",
    "\t# Where to save models\n",
    "\tresults_dir = os.path.join('results', args.model_id)\n",
    "\tif not os.path.exists(results_dir):\n",
    "\t\tos.makedirs(results_dir)\n",
    "\t\n",
    "\trail_generator = sparse_rail_generator(max_num_cities=args.max_num_cities,\n",
    "\t                                       seed=args.seed,\n",
    "\t                                       grid_mode=args.grid_mode,\n",
    "\t                                       max_rails_between_cities=args.max_rails_between_cities,\n",
    "\t                                       max_rails_in_city=args.max_rails_in_city,\n",
    "\t                                       )\n",
    "\n",
    "\t# Maps speeds to % of appearance in the env\n",
    "\tspeed_ration_map = {1.: 1}  # Fast passenger train\n",
    "\t\n",
    "\tif args.multi_speed:\n",
    "\t\tspeed_ration_map = {1.: 0.25,  # Fast passenger train\n",
    "\t\t\t\t\t\t\t1. / 2.: 0.25,  # Fast freight train\n",
    "\t\t\t\t\t\t\t1. / 3.: 0.25,  # Slow commuter train\n",
    "\t\t\t\t\t\t\t1. / 4.: 0.25}  # Slow freight train\n",
    "\n",
    "\tschedule_generator = SparseLineGen(speed_ration_map)\n",
    "\t\n",
    "\tprediction_builder = ShortestPathPredictorForRailEnv(max_depth=args.prediction_depth)\n",
    "\tobs_builder = RailObsForRailEnv(predictor=prediction_builder)\n",
    "\n",
    "\tenv = RailEnv(width=args.width,\n",
    "\t              height=args.height,\n",
    "\t              rail_generator=rail_generator,\n",
    "\t              random_seed=0,\n",
    "\t              schedule_generator=schedule_generator,\n",
    "\t              number_of_agents=args.num_agents,\n",
    "\t              obs_builder_object=obs_builder,\n",
    "\t              malfunction_generator_and_process_data=malfunction_from_params(\n",
    "\t\t              parameters={\n",
    "\t\t\t              'malfunction_rate': args.malfunction_rate,\n",
    "\t\t\t              'min_duration': args.min_duration,\n",
    "\t\t\t              'max_duration': args.max_duration\n",
    "\t\t              })\n",
    "\t              )\n",
    "\n",
    "\tif args.render:\n",
    "\t\tenv_renderer = RenderTool(\n",
    "\t\t\tenv,\n",
    "\t\t\tagent_render_variant=AgentRenderVariant.AGENT_SHOWS_OPTIONS_AND_BOX,\n",
    "\t\t\tshow_debug=True,\n",
    "\t\t\tscreen_height=800,\n",
    "\t\t\tscreen_width=800)\n",
    "\n",
    "\tif args.plot:\n",
    "\t\twriter = SummaryWriter(log_dir='runs/' + args.model_id)\n",
    "\n",
    "\tmax_rails = 100 # TODO Must be a parameter of the env (estimated)\n",
    "\t# max_steps = env.compute_max_episode_steps(env.width, env.height)\n",
    "\tmax_steps = 200\n",
    "\n",
    "\tpreprocessor = ObsPreprocessor(max_rails, args.reorder_rails)\n",
    "\n",
    "\tdqn = DQNAgent(args, bitmap_height=max_rails * 3, action_space=2)\n",
    "\n",
    "\tif args.load_path:\n",
    "\t\tfile = os.path.isfile(args.load_path)\n",
    "\t\tif file:\n",
    "\t\t\tdqn.qnetwork_local.load_state_dict(torch.load(args.load_path))\n",
    "\t\t\tprint('WEIGHTS LOADED from: ', args.load_path)\n",
    "\n",
    "\teps = args.start_eps\n",
    "\trailenv_action_dict = {}\n",
    "\tnetwork_action_dict = {}\n",
    "\t# Metrics\n",
    "\tdone_window = deque(maxlen=args.window_size) # Env dones over last window_size episodes\n",
    "\tdone_agents_window = deque(maxlen=args.window_size) # Fraction of done agents over last ...\n",
    "\treward_window = deque(maxlen=args.window_size) # Cumulative rewards over last window_size episodes\n",
    "\tnorm_reward_window = deque(maxlen=args.window_size) # Normalized cum. rewards over last window_size episodes\n",
    "\t# Track means over windows of window_size episodes\n",
    "\tmean_dones = [] \n",
    "\tmean_agent_dones = []\n",
    "\tmean_rewards = []\n",
    "\tmean_norm_rewards = []\n",
    "\t# Episode rewards/dones/norm rewards since beginning of training TODO\n",
    "\t#env_dones = []\n",
    "\t\n",
    "\tcrash = [False] * args.num_agents\n",
    "\tupdate_values = [False] * args.num_agents\n",
    "\tbuffer_obs = [[]] * args.num_agents\n",
    "\n",
    "\t############ Main loop\n",
    "\tfor ep in range(args.num_episodes):\n",
    "\t\tcumulative_reward = 0\n",
    "\t\tenv_done = 0\n",
    "\t\taltmaps = [None] * args.num_agents\n",
    "\t\taltpaths = [[]] * args.num_agents\n",
    "\t\tbuffer_rew = [0] * args.num_agents\n",
    "\t\tbuffer_done = [False] * args.num_agents\n",
    "\t\tcurr_obs = [None] * args.num_agents\n",
    "\n",
    "\t\tmaps, info = env.reset()\n",
    "\t\tif args.print:\n",
    "\t\t\tdebug.print_bitmaps(maps)\n",
    "\n",
    "\t\tif args.render:\n",
    "\t\t\tenv_renderer.reset()\n",
    "\n",
    "\t\tfor step in range(max_steps - 1):\n",
    "\t\t\t# Save a copy of maps at the beginning\n",
    "\t\t\tbuffer_maps = maps.copy()\n",
    "\t\t\t# rem first bit is 0 for agent not departed\n",
    "\t\t\tfor a in range(env.get_num_agents()):\n",
    "\t\t\t\tagent = env.agents[a]\n",
    "\t\t\t\tcrash[a] = False\n",
    "\t\t\t\tupdate_values[a] = False\n",
    "\t\t\t\tnetwork_action = None\n",
    "\t\t\t\taction = None\n",
    "\n",
    "\t\t\t\t# If agent is arrived\n",
    "\t\t\t\tif agent.status == RailAgentStatus.DONE or agent.status == RailAgentStatus.DONE_REMOVED:\n",
    "\t\t\t\t\t# TODO if agent !removed you should leave a bit in the bitmap\n",
    "\t\t\t\t\t# TODO? set bitmap only the first time\n",
    "\t\t\t\t\tmaps[a, :, :] = 0\n",
    "\t\t\t\t\tnetwork_action = 0\n",
    "\t\t\t\t\taction = RailEnvActions.DO_NOTHING\n",
    "\n",
    "\t\t\t\t# If agent is not departed\n",
    "\t\t\t\telif agent.status == RailAgentStatus.READY_TO_DEPART:\n",
    "\t\t\t\t\tupdate_values[a] = True\n",
    "\t\t\t\t\tobs = preprocessor.get_obs(a, maps[a], buffer_maps)\n",
    "\t\t\t\t\tcurr_obs[a] = obs.copy()\n",
    "\n",
    "\t\t\t\t\t# Network chooses action\n",
    "\t\t\t\t\tq_values = dqn.act(obs).cpu().data.numpy()\n",
    "\t\t\t\t\tif np.random.random() > eps:\n",
    "\t\t\t\t\t\tnetwork_action = np.argmax(q_values)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tnetwork_action = np.random.choice([0, 1])\n",
    "\n",
    "\t\t\t\t\tif network_action == 0:\n",
    "\t\t\t\t\t\taction = RailEnvActions.DO_NOTHING\n",
    "\t\t\t\t\telse: # Go\n",
    "\t\t\t\t\t\tcrash[a] = obs_builder.check_crash(a, maps)\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tif crash[a]:\n",
    "\t\t\t\t\t\t\tnetwork_action = 0\n",
    "\t\t\t\t\t\t\taction = RailEnvActions.STOP_MOVING\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tmaps = obs_builder.update_bitmaps(a, maps)\n",
    "\t\t\t\t\t\t\taction = obs_builder.get_agent_action(a)\n",
    "\t\t\n",
    "\t\t\t\t# If the agent is entering a switch\n",
    "\t\t\t\telif obs_builder.is_before_switch(a) and info['action_required'][a]:\n",
    "\t\t\t\t\t# If the altpaths cache is empty or already contains\n",
    "\t\t\t\t\t# the altpaths from the current agent's position\n",
    "\t\t\t\t\tif len(altpaths[a]) == 0 or agent.position != altpaths[a][0][0].position:\n",
    "\t\t\t\t\t\taltmaps[a], altpaths[a] = obs_builder.get_altmaps(a)\n",
    "\n",
    "\t\t\t\t\tif len(altmaps[a]) > 0:\n",
    "\t\t\t\t\t\tupdate_values[a] = True\n",
    "\t\t\t\t\t\taltobs = [None] * len(altmaps[a])\n",
    "\t\t\t\t\t\tq_values = np.array([])\n",
    "\t\t\t\t\t\tfor i in range(len(altmaps[a])):\n",
    "\t\t\t\t\t\t\taltobs[i] = preprocessor.get_obs(a, altmaps[a][i], buffer_maps)\n",
    "\t\t\t\t\t\t\tq_values = np.concatenate([q_values, dqn.act(altobs[i]).cpu().data.numpy()])\n",
    "\n",
    "\t\t\t\t\t\t# Epsilon-greedy action selection\n",
    "\t\t\t\t\t\tif np.random.random() > eps:\n",
    "\t\t\t\t\t\t\targmax = np.argmax(q_values)\n",
    "\t\t\t\t\t\t\tnetwork_action = argmax % 2\n",
    "\t\t\t\t\t\t\tbest_i = argmax // 2\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tnetwork_action = np.random.choice([0, 1])\n",
    "\t\t\t\t\t\t\tbest_i = np.random.choice(np.arange(len(altmaps[a])))\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t# Use new bitmaps and paths\n",
    "\t\t\t\t\t\tmaps[a, :, :] = altmaps[a][best_i]\n",
    "\t\t\t\t\t\tobs_builder.set_agent_path(a, altpaths[a][best_i])\n",
    "\t\t\t\t\t\tcurr_obs[a] = altobs[best_i].copy()\n",
    "\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tprint('[ERROR] NO ALTHPATHS EP: {} STEP: {} AGENT: {}'.format(ep, step, a))\n",
    "\t\t\t\t\t\tnetwork_action = 0\n",
    "\n",
    "\t\t\t\t\tif network_action == 0:\n",
    "\t\t\t\t\t\taction = RailEnvActions.STOP_MOVING\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tcrash[a] = obs_builder.check_crash(a, maps, is_before_switch=True)\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tif crash[a]:\n",
    "\t\t\t\t\t\t\tnetwork_action = 0\n",
    "\t\t\t\t\t\t\taction = RailEnvActions.STOP_MOVING\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\taction = obs_builder.get_agent_action(a)\n",
    "\t\t\t\t\t\t\tmaps = obs_builder.update_bitmaps(a, maps, is_before_switch=True)\n",
    "\t\t\n",
    "\t\t\t\t# If the agent is following a rail\n",
    "\t\t\t\telif info['action_required'][a]:\n",
    "\t\t\t\t\tcrash[a] = obs_builder.check_crash(a, maps)\n",
    "\n",
    "\t\t\t\t\tif crash[a]:\n",
    "\t\t\t\t\t\tnetwork_action = 0\n",
    "\t\t\t\t\t\taction = RailEnvActions.STOP_MOVING\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tnetwork_action = 1\n",
    "\t\t\t\t\t\taction = obs_builder.get_agent_action(a)\n",
    "\t\t\t\t\t\tmaps = obs_builder.update_bitmaps(a, maps)\n",
    "\n",
    "\t\t\t\telse: # not action_required\n",
    "\t\t\t\t\tnetwork_action = 1\n",
    "\t\t\t\t\taction = RailEnvActions.DO_NOTHING\n",
    "\t\t\t\t\tmaps = obs_builder.update_bitmaps(a, maps)\n",
    "\t\t\t\t\n",
    "\t\t\t\tnetwork_action_dict.update({a: network_action})\n",
    "\t\t\t\trailenv_action_dict.update({a: action})\n",
    "\n",
    "\t\t\t# Obs is computed from bitmaps while state is computed from env step\n",
    "\t\t\t_, reward, done, info = env.step(railenv_action_dict)\n",
    "\t\t\t\n",
    "\t\t\t# Update replay buffer and train agent\n",
    "\t\t\tif args.train:\n",
    "\t\t\t\tfor a in range(env.get_num_agents()):\n",
    "\t\t\t\t\tif args.crash_penalty and crash[a]:\n",
    "\t\t\t\t\t\t# Store bad experience\n",
    "\t\t\t\t\t\tdqn.step(curr_obs[a], 1, -100, curr_obs[a], True)\n",
    "\n",
    "\t\t\t\t\tif not args.switch2switch:\n",
    "\t\t\t\t\t\tif update_values[a] and not buffer_done[a]:\n",
    "\t\t\t\t\t\t\tnext_obs = preprocessor.get_obs(a, maps[a], maps)\n",
    "\t\t\t\t\t\t\tdqn.step(curr_obs[a], network_action_dict[a], reward[a], next_obs, done[a])\n",
    "\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tif update_values[a] and not buffer_done[a]:\n",
    "\t\t\t\t\t\t\t# If I had an obs from a previous switch\n",
    "\t\t\t\t\t\t\tif len(buffer_obs[a]) != 0:\n",
    "\t\t\t\t\t\t\t\tdqn.step(buffer_obs[a], 1, buffer_rew[a], curr_obs[a], done[a])\n",
    "\t\t\t\t\t\t\t\tbuffer_obs[a] = []\n",
    "\t\t\t\t\t\t\t\tbuffer_rew[a] = 0\n",
    "\n",
    "\t\t\t\t\t\t\tif network_action_dict[a] == 0:\n",
    "\t\t\t\t\t\t\t\tdqn.step(curr_obs[a], 1, reward[a], curr_obs[a], False)\n",
    "\t\t\t\t\t\t\telif network_action_dict[a] == 1:\n",
    "\t\t\t\t\t\t\t\t# I store the obs and update at the next switch\n",
    "\t\t\t\t\t\t\t\tbuffer_obs[a] = curr_obs[a].copy()\n",
    "\n",
    "\t\t\t\t\t\t# Cache reward only if we have an obs from a prev switch\n",
    "\t\t\t\t\t\tif len(buffer_obs[a]) != 0:\n",
    "\t\t\t\t\t\t\tbuffer_rew[a] += reward[a]\n",
    "\n",
    "\t\t\t\t\t# Now update the done cache to avoid adding experience many times\n",
    "\t\t\t\t\tbuffer_done[a] = done[a]\n",
    "\n",
    "\t\t\tfor a in range(env.get_num_agents()):\t\n",
    "\t\t\t\tcumulative_reward += reward[a] \n",
    "\t\t\t \n",
    "\t\t\tif done['__all__']:\n",
    "\t\t\t\tenv_done = 1\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t################### End of the episode\n",
    "\t\teps = max(args.end_eps, args.eps_decay * eps)  # Decrease epsilon\n",
    "\t\t# Metrics\n",
    "\t\tdone_window.append(env_done) # Save done in this episode\n",
    "\t\t\n",
    "\t\tnum_agents_done = 0  # Num of agents that reached their target in the last episode\n",
    "\t\tfor a in range(env.get_num_agents()): \n",
    "\t\t\tif done[a]:\n",
    "\t\t\t\tnum_agents_done += 1\n",
    "\t\tdone_agents_window.append(num_agents_done / env.get_num_agents())\n",
    "\t\treward_window.append(cumulative_reward)  # Save cumulative reward in this episode\n",
    "\t\tnormalized_reward = cumulative_reward / (env.compute_max_episode_steps(env.width, env.height) + env.get_num_agents())\n",
    "\t\tnorm_reward_window.append(normalized_reward)\n",
    "\t\t\n",
    "\t\tmean_dones.append((np.mean(done_window)))\n",
    "\t\tmean_agent_dones.append((np.mean(done_agents_window)))\n",
    "\t\tmean_rewards.append(np.mean(reward_window))\n",
    "\t\tmean_norm_rewards.append(np.mean(norm_reward_window))\n",
    "\n",
    "\t\t# Print training results info\n",
    "\t\tprint(\n",
    "\t\t\t'\\r{} Agents on ({},{}). Episode: {}\\t Mean done agents: {:.2f}\\t Mean reward: {:.2f}\\t Mean normalized reward: {:.2f}\\t Done agents in last episode: {:.2f}%\\t Epsilon: {:.2f}'.format(\n",
    "\t\t\t\tenv.get_num_agents(), args.width, args.height,\n",
    "\t\t\t\tep,\n",
    "\t\t\t\tmean_agent_dones[-1],  # Fraction of done agents\n",
    "\t\t\t\tmean_rewards[-1],\n",
    "\t\t\t\tmean_norm_rewards[-1],\n",
    "\t\t\t\t(num_agents_done / args.num_agents),\n",
    "\t\t\t\teps), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
